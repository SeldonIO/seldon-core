{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tritonclient Examples with Seldon Core V2\n",
    "\n",
    "- Note: for compatibility of Tritonclient check this issue https://github.com/SeldonIO/seldon-core-v2/issues/471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NAMESPACE\"] = \"seldon-mesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'172.19.255.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MESH_IP=!kubectl get svc seldon-mesh -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n",
    "MESH_IP=MESH_IP[0]\n",
    "import os\n",
    "os.environ['MESH_IP'] = MESH_IP\n",
    "MESH_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With MLServer\n",
    "\n",
    "- Note: binary data support in HTTP is blocked by https://github.com/SeldonIO/MLServer/issues/324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: mlops.seldon.io/v1alpha1\r\n",
      "kind: Model\r\n",
      "metadata:\r\n",
      "  name: iris\r\n",
      "spec:\r\n",
      "  storageUri: \"gs://seldon-models/scv2/samples/mlserver_1.3.5/iris-sklearn\"\r\n",
      "  requirements:\r\n",
      "  - sklearn\r\n",
      "  memory: 100Ki\r\n"
     ]
    }
   ],
   "source": [
    "!cat models/sklearn-iris-gs.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: mlops.seldon.io/v1alpha1\r\n",
      "kind: Pipeline\r\n",
      "metadata:\r\n",
      "  name: iris-pipeline\r\n",
      "spec:\r\n",
      "  steps:\r\n",
      "    - name: iris\r\n",
      "  output:\r\n",
      "    steps:\r\n",
      "    - iris\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipelines/iris.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/iris created\n",
      "pipeline.mlops.seldon.io/iris-pipeline created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n",
    "!kubectl apply -f pipelines/iris.yaml -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/iris condition met\n",
      "pipeline.mlops.seldon.io/iris-pipeline condition met\n"
     ]
    }
   ],
   "source": [
    "!kubectl wait --for condition=ready --timeout=300s model iris -n ${NAMESPACE}\n",
    "!kubectl wait --for condition=ready --timeout=300s pipelines iris-pipeline -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Transport Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: True\n",
      "model metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "http_triton_client = httpclient.InferenceServerClient(\n",
    "    url=f\"{MESH_IP}:80\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\n",
    "print(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against model\n",
    "\n",
    "binary_data = False\n",
    "\n",
    "inputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\n",
    "inputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n",
    "\n",
    "result = http_triton_client.infer(\"iris\", inputs, outputs=outputs)\n",
    "result.as_numpy(\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against pipeline\n",
    "\n",
    "binary_data = False\n",
    "\n",
    "inputs = [httpclient.InferInput(\"predict\", (1, 4), \"FP64\")]\n",
    "inputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"), binary_data=binary_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"predict\", binary_data=binary_data)]\n",
    "\n",
    "result = http_triton_client.infer(\"iris-pipeline.pipeline\", inputs, outputs=outputs)\n",
    "result.as_numpy(\"predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPC Transport Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grpc_triton_client = grpcclient.InferenceServerClient(\n",
    "    url=f\"{MESH_IP}:80\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: True\n",
      "name: \"iris_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"iris\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "print(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\n",
    "print(grpc_triton_client.get_model_metadata(model_name, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Against Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"iris\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "inputs = [\n",
    "    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n",
    "\n",
    "outputs = [grpcclient.InferRequestedOutput(\"predict\")]\n",
    "\n",
    "\n",
    "result = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\n",
    "result.as_numpy(\"predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Against Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"iris-pipeline.pipeline\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "inputs = [\n",
    "    grpcclient.InferInput(\"predict\", (1, 4), \"FP64\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.array([[1, 2, 3, 4]]).astype(\"float64\"))\n",
    "\n",
    "outputs = [grpcclient.InferRequestedOutput(\"predict\")]\n",
    "\n",
    "\n",
    "result = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\n",
    "result.as_numpy(\"predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Tritonserver\n",
    "\n",
    "- Note: binary data support in HTTP is blocked by https://github.com/SeldonIO/seldon-core-v2/issues/475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: mlops.seldon.io/v1alpha1\r\n",
      "kind: Model\r\n",
      "metadata:\r\n",
      "  name: tfsimple1\r\n",
      "spec:\r\n",
      "  storageUri: \"gs://seldon-models/triton/simple\"\r\n",
      "  requirements:\r\n",
      "  - tensorflow\r\n",
      "  memory: 100Ki\r\n"
     ]
    }
   ],
   "source": [
    "!cat models/tfsimple1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: mlops.seldon.io/v1alpha1\r\n",
      "kind: Pipeline\r\n",
      "metadata:\r\n",
      "  name: tfsimple\r\n",
      "spec:\r\n",
      "  steps:\r\n",
      "    - name: tfsimple1\r\n",
      "  output:\r\n",
      "    steps:\r\n",
      "    - tfsimple1\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipelines/tfsimple.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/tfsimple1 created\n",
      "pipeline.mlops.seldon.io/tfsimple created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f models/tfsimple1.yaml -n ${NAMESPACE}\n",
    "!kubectl apply -f pipelines/tfsimple.yaml -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io/tfsimple1 condition met\n",
      "pipeline.mlops.seldon.io/tfsimple condition met\n"
     ]
    }
   ],
   "source": [
    "!kubectl wait --for condition=ready --timeout=300s model tfsimple1 -n ${NAMESPACE}\n",
    "!kubectl wait --for condition=ready --timeout=300s pipelines tfsimple -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Transport Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: True\n",
      "model metadata: {'name': 'iris_1', 'versions': [], 'platform': '', 'inputs': [], 'outputs': [], 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "http_triton_client = httpclient.InferenceServerClient(\n",
    "    url=f\"{MESH_IP}:80\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"model ready:\", http_triton_client.is_model_ready(\"iris\"))\n",
    "print(\"model metadata:\", http_triton_client.get_model_metadata(\"iris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against model (no binary data)\n",
    "\n",
    "binary_data = False\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n",
    "\n",
    "\n",
    "result = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\n",
    "result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against model (with binary data)\n",
    "\n",
    "binary_data = True\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n",
    "\n",
    "\n",
    "result = http_triton_client.infer(\"tfsimple1\", inputs, outputs=outputs)\n",
    "result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against Pipeline (no binary data)\n",
    "\n",
    "binary_data = False\n",
    "\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "    httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n",
    "\n",
    "\n",
    "result = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\n",
    "result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binary data does not work with http behind pipeline\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# binary_data = True\n",
    "\n",
    "# inputs = [\n",
    "#     httpclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "#     httpclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "# ]\n",
    "# inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "# inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"), binary_data=binary_data)\n",
    "\n",
    "# outputs = [httpclient.InferRequestedOutput(\"OUTPUT0\", binary_data=binary_data)]\n",
    "\n",
    "\n",
    "# result = http_triton_client.infer(\"tfsimple.pipeline\", inputs, outputs=outputs)\n",
    "# result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPC Transport Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grpc_triton_client = grpcclient.InferenceServerClient(\n",
    "    url=f\"{MESH_IP}:80\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready: True\n",
      "name: \"tfsimple1_1\"\n",
      "versions: \"1\"\n",
      "platform: \"tensorflow_graphdef\"\n",
      "inputs {\n",
      "  name: \"INPUT0\"\n",
      "  datatype: \"INT32\"\n",
      "  shape: -1\n",
      "  shape: 16\n",
      "}\n",
      "inputs {\n",
      "  name: \"INPUT1\"\n",
      "  datatype: \"INT32\"\n",
      "  shape: -1\n",
      "  shape: 16\n",
      "}\n",
      "outputs {\n",
      "  name: \"OUTPUT0\"\n",
      "  datatype: \"INT32\"\n",
      "  shape: -1\n",
      "  shape: 16\n",
      "}\n",
      "outputs {\n",
      "  name: \"OUTPUT1\"\n",
      "  datatype: \"INT32\"\n",
      "  shape: -1\n",
      "  shape: 16\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tfsimple1\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "print(\"model ready:\", grpc_triton_client.is_model_ready(model_name, headers=headers))\n",
    "print(grpc_triton_client.get_model_metadata(model_name, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against Model\n",
    "\n",
    "model_name = \"tfsimple1\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "inputs = [\n",
    "    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n",
    "inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n",
    "\n",
    "outputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n",
    "\n",
    "\n",
    "result = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\n",
    "result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Against Pipeline\n",
    "\n",
    "model_name = \"tfsimple.pipeline\"\n",
    "headers = {\"seldon-model\": model_name}\n",
    "\n",
    "inputs = [\n",
    "    grpcclient.InferInput(\"INPUT0\", (1, 16), \"INT32\"),\n",
    "    grpcclient.InferInput(\"INPUT1\", (1, 16), \"INT32\"),\n",
    "]\n",
    "inputs[0].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n",
    "inputs[1].set_data_from_numpy(np.arange(1, 17).reshape(-1, 16).astype(\"int32\"))\n",
    "\n",
    "outputs = [grpcclient.InferRequestedOutput(\"OUTPUT0\")]\n",
    "\n",
    "\n",
    "result = grpc_triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\n",
    "result.as_numpy(\"OUTPUT0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io \"iris\" deleted\n",
      "pipeline.mlops.seldon.io \"iris-pipeline\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f models/sklearn-iris-gs.yaml -n ${NAMESPACE}\n",
    "!kubectl delete -f pipelines/iris.yaml -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mlops.seldon.io \"tfsimple1\" deleted\n",
      "pipeline.mlops.seldon.io \"tfsimple\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f models/tfsimple1.yaml -n ${NAMESPACE}\n",
    "!kubectl delete -f pipelines/tfsimple.yaml -n ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f90d0a8b54f81c3642b2ee3336e1de55986a3a982587eefc0621499dbdd4a3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
