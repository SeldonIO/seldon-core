{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder Outlier (VAE) Algorithm Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this document is to explain the Variational Auto-Encoder algorithm in Seldon's outlier detection framework.\n",
    "\n",
    "First, we provide a high level overview of the algorithm and the use case, then we will give a detailed explanation of the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection has many applications, ranging from preventing credit card fraud to detecting computer network intrusions. The available data is typically unlabeled and detection needs to be done in real-time. The outlier detector can be used as a standalone algorithm, or to detect anomalies in the input data of another predictive model.\n",
    "\n",
    "The VAE outlier detection algorithm predicts whether the input features are an outlier or not, dependent on a threshold level set by the user. The algorithm needs to be pretrained first on a batch of -preferably- inliers.\n",
    "\n",
    "As observations arrive, the algorithm will:\n",
    "- scale (standardize or minmax) the input features\n",
    "- first encode, and then decode the input data in an attempt to reconstruct the initial observations\n",
    "- compute a reconstruction error between the output of the decoder and the input data\n",
    "- predict that the observation is an outlier if the error is larger than the threshold level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Variational Auto-Encoders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Auto-Encoder is an algorithm that consists of 2 main building blocks: an encoder and a decoder. The encoder tries to find a compressed representation of the input data. The compressed data is then fed into the decoder, which aims to replicate the input data. Both the encoder and decoder are typically implemented with neural networks. The loss function to be minimized with stochastic gradient descent is a distance function between the input data and output of the decoder, and is called the reconstruction error.\n",
    "\n",
    "If we train the Auto-Encoder with inliers, it will be able to replicate new inlier data well with a low reconstruction error. However, if outliers are fed to the Auto-Encoder, the reconstruction error becomes large and we can classify the observation as an anomaly.\n",
    "\n",
    "A Variational Auto-Encoder adds constraints to the encoded representations of the input. The encodings are parameters of a probability distribution modeling the data. The decoder can then generate new data by sampling from the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building the VAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model definition in model.py takes 4 arguments that define the architecture:\n",
    "- the number of features in the input\n",
    "- the number of hidden layers used in the encoder and decoder\n",
    "- the dimension of the latent variable\n",
    "- the dimensions of each hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "def model(n_features, hidden_layers=1, latent_dim=2, hidden_dim=[], \n",
    "          output_activation='sigmoid', learning_rate=0.001):\n",
    "    \"\"\" Build VAE model. \n",
    "    \n",
    "    Arguments:\n",
    "        - n_features (int): number of features in the data\n",
    "        - hidden_layers (int): number of hidden layers used in encoder/decoder\n",
    "        - latent_dim (int): dimension of latent variable\n",
    "        - hidden_dim (list): list with dimension of each hidden layer\n",
    "        - output_activation (str): activation type for last dense layer in the decoder\n",
    "        - learning_rate (float): learning rate used during training\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the input data feeds in the encoder and is compressed by mapping it on the latent space which defines the probability distribution of the encodings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    # encoder\n",
    "    inputs = Input(shape=(n_features,), name='encoder_input')\n",
    "    # define hidden layers\n",
    "    enc_hidden = Dense(hidden_dim[0], activation='relu', name='encoder_hidden_0')(inputs)\n",
    "    i = 1\n",
    "    while i < hidden_layers:\n",
    "        enc_hidden = Dense(hidden_dim[i],activation='relu',name='encoder_hidden_'+str(i))(enc_hidden)\n",
    "        i+=1\n",
    "    \n",
    "    z_mean = Dense(latent_dim, name='z_mean')(enc_hidden)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(enc_hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then sample data from the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "def sampling(args):\n",
    "    \"\"\" Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    \n",
    "    Arguments:\n",
    "        - args (tensor): mean and log of variance of Q(z|X)\n",
    "        \n",
    "    Returns:\n",
    "        - z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim)) # by default, random_normal has mean=0 and std=1.0\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon # mean + stdev * eps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    # reparametrization trick to sample z\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled data passes through the decoder which aims to reconstruct the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    # decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    # define hidden layers\n",
    "    dec_hidden = Dense(hidden_dim[-1], activation='relu', name='decoder_hidden_0')(latent_inputs)\n",
    "\n",
    "    i = 2\n",
    "    while i < hidden_layers+1:\n",
    "        dec_hidden = Dense(hidden_dim[-i],activation='relu',name='decoder_hidden_'+str(i-1))(dec_hidden)\n",
    "        i+=1\n",
    "\n",
    "    outputs = Dense(n_features, activation=output_activation, name='decoder_output')(dec_hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is the sum of the reconstruction error and the KL-divergence. While the reconstruction error quantifies how well we can recreate the input data, the KL-divergence measures how close the latent representation is to the unit Gaussian distribution. This trade-off is important because we want our encodings to parameterize a probability distribution from which we can sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    # define VAE loss, optimizer and compile model\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "    reconstruction_loss *= n_features\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model can be trained on a batch of inliers by running the train.py script with the desired hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "!python train.py \\\n",
    "--dataset 'kddcup99' \\\n",
    "--samples 50000 \\\n",
    "--keep_cols \"$cols_str\" \\\n",
    "--hidden_layers 1 \\\n",
    "--latent_dim 2 \\\n",
    "--hidden_dim 9 \\\n",
    "--output_activation 'sigmoid' \\\n",
    "--clip 999999 \\\n",
    "--standardized \\\n",
    "--epochs 10 \\\n",
    "--batch_size 32 \\\n",
    "--learning_rate 0.001 \\\n",
    "--print_progress \\\n",
    "--model_name 'vae' \\\n",
    "--save \\\n",
    "--save_path './models/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights and hyperparameters are saved in the folder specified by \"save_path\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, which can then be served by Seldon Core, the pre-trained model weights and hyperparameters are loaded when defining an OutlierVAE object. The \"threshold\" argument defines above which reconstruction error a sample is classified as an outlier. The threshold is a key hyperparameter and needs to be picked carefully for each application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "class OutlierVAE(object):\n",
    "    \"\"\" Outlier detection using variational autoencoders (VAE).\n",
    "    \n",
    "    Arguments:\n",
    "        - threshold: (float): reconstruction error (mse) threshold used to classify outliers\n",
    "        - reservoir_size (int): number of observations kept in memory using reservoir sampling used for mean and stdev\n",
    "     \n",
    "    Functions:\n",
    "        - reservoir_sampling: applies reservoir sampling to incoming data\n",
    "        - predict: detect and return outliers\n",
    "        - send_feedback: add target labels as part of the feedback loop\n",
    "        - metrics: return custom metrics\n",
    "    \"\"\"\n",
    "    def __init__(self,threshold=10,reservoir_size=50000,model_name='vae',load_path='./models/'):\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.batch = []\n",
    "        self.N = 0 # total sample count up until now for reservoir sampling\n",
    "        \n",
    "        # load model architecture parameters\n",
    "        with open(load_path + model_name + '.pickle', 'rb') as f:\n",
    "            n_features, hidden_layers, latent_dim, hidden_dim, output_activation = pickle.load(f)\n",
    "            \n",
    "        # instantiate model\n",
    "        self.vae = model(n_features,hidden_layers=hidden_layers,latent_dim=latent_dim,\n",
    "                         hidden_dim=hidden_dim,output_activation=output_activation)\n",
    "        self.vae.load_weights(load_path + model_name + '_weights.h5') # load pretrained model weights\n",
    "        self.vae._make_predict_function()\n",
    "        \n",
    "        # load data preprocessing info\n",
    "        with open(load_path + 'preprocess_' + model_name + '.pickle', 'rb') as f:\n",
    "            preprocess = pickle.load(f)\n",
    "        self.preprocess, self.clip, self.axis = preprocess[:3]\n",
    "        if self.preprocess=='minmax':\n",
    "            self.xmin, self.xmax = preprocess[3:5]\n",
    "            self.min, self.max = preprocess[5:]\n",
    "        elif self.preprocess=='standardized':\n",
    "            self.mu, self.sigma = preprocess[3:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict method does the actual outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "    def predict(self,X,feature_names):\n",
    "        \"\"\" Detect outliers from mse using the threshold. \n",
    "        \n",
    "        Arguments:\n",
    "            - X: input data\n",
    "            - feature_names\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the observations are clipped. If the number of observations fed to the outlier detector up until now is at least equal to the defined reservoir size, the feature-wise scaling parameters are updated using the observations in the reservoir. The reservoir is updated each observation using reservoir sampling. The input data is then scaled using either standardization or minmax scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "        # clip data per feature\n",
    "        X = np.clip(X,[-c for c in self.clip],self.clip)\n",
    "    \n",
    "        if self.N < self.reservoir_size:\n",
    "            update_stand = False\n",
    "        else:\n",
    "            update_stand = True\n",
    "            \n",
    "        self.reservoir_sampling(X,update_stand=update_stand)\n",
    "        \n",
    "        # apply scaling\n",
    "        if self.preprocess=='minmax':\n",
    "            X_scaled = ((X - self.xmin) / (self.xmax - self.xmin)) * (self.max - self.min) + self.min\n",
    "        elif self.preprocess=='standardized':\n",
    "            X_scaled = (X - self.mu) / (self.sigma + 1e-10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make multiple predictions for an observation by sampling N times from the latent space. The mean squared error between the input data and output of the decoder is averaged across the N samples. If this value is above the threshold, an outlier is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "        # sample latent variables and calculate reconstruction errors\n",
    "        N = 10\n",
    "        mse = np.zeros([X.shape[0],N])\n",
    "        for i in range(N):\n",
    "            preds = self.vae.predict(X_scaled)\n",
    "            mse[:,i] = np.mean(np.power(X_scaled - preds, 2), axis=1)\n",
    "        self.mse = np.mean(mse, axis=1)\n",
    "        \n",
    "        # make prediction\n",
    "        self.prediction = np.array([1 if e > self.threshold else 0 for e in self.mse]).astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. ICLR 2014.\n",
    "- https://arxiv.org/pdf/1312.6114.pdf\n",
    "\n",
    "Francois Chollet. Building Autoencoders in Keras.\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
