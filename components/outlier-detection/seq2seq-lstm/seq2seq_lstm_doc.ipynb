{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence LSTM (seq2seq-LSTM) Outlier Algorithm Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this document is to explain the seq2seq-LSTM algorithm in Seldon's outlier detection framework.\n",
    "\n",
    "First, we provide a high level overview of the algorithm and the use case, then we will give a detailed explanation of the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection has many applications, ranging from preventing credit card fraud to detecting computer network intrusions. The available data is typically unlabeled and detection needs to be done in real-time. The outlier detector can be used as a standalone algorithm, or to detect anomalies in the input data of another predictive model.\n",
    "\n",
    "The seq2seq-LSTM outlier detection algorithm is suitable for time series data and predicts whether a sequence of input features is an outlier or not, dependent on a threshold level set by the user. The algorithm needs to be pretrained first on a batch of -preferably- inliers.\n",
    "\n",
    "As observations arrive, the algorithm will:\n",
    "- clip and scale the input features\n",
    "- first encode, and then sequentially decode the input time series data in an attempt to reconstruct the initial observations\n",
    "- compute a reconstruction error between the output of the decoder and the input data\n",
    "- predict that the observation is an outlier if the error is larger than the threshold level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Sequence-to-Sequence Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq models convert sequences from one domain into sequences in another domain. A typical example would be sentence translation between different languages. A seq2seq model consists of 2 main building blocks: an encoder and a decoder. The encoder processes the input sequence and initializes the decoder. The decoder then makes sequential predictions for the output sequence. In our case, the decoder aims to reconstruct the input sequence. Both the encoder and decoder are typically implemented with recurrent or 1D convolutional neural networks. Our implementation uses a type of recurrent neural network called LSTM networks. An excellent explanation of how LSTM units work is available [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). The loss function to be minimized with stochastic gradient descent is the mean squared error between the input and output sequence, and is called the reconstruction error.\n",
    "\n",
    "If we train the seq2seq model with inliers, it will be able to replicate new inlier data well with a low reconstruction error. However, if outliers are fed to the seq2seq model, the reconstruction error becomes large and we can classify the sequence as an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is inspired by [this blog post](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building the seq2seq-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq model definition in ```model.py``` takes 4 arguments that define the architecture:\n",
    "- the number of features in the input\n",
    "- a list with the number of units per [bidirectional](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks) LSTM layer in the encoder\n",
    "- a list with the number of units per LSTM layer in the decoder\n",
    "- the output activation type for the dense output layer on top of the last LSTM unit in the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "def model(n_features, encoder_dim = [20], decoder_dim = [20], dropout=0., learning_rate=.001, \n",
    "          loss='mean_squared_error', output_activation='sigmoid'):\n",
    "    \"\"\" Build seq2seq model.\n",
    "    \n",
    "    Arguments:\n",
    "        - n_features (int): number of features in the data\n",
    "        - encoder_dim (list): list with number of units per encoder layer\n",
    "        - decoder_dim (list): list with number of units per decoder layer\n",
    "        - dropout (float): dropout for LSTM units\n",
    "        - learning_rate (float): learning rate used during training\n",
    "        - loss (str): loss function used\n",
    "        - output_activation (str): activation type for the dense output layer in the decoder\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the bidirectional LSTM layers in the encoder and keep the state of the last LSTM unit to initialise the decoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# add encoder hidden layers\n",
    "encoder_lstm = []\n",
    "for i in range(enc_dim-1):\n",
    "    encoder_lstm.append(Bidirectional(LSTM(encoder_dim[i], dropout=dropout, \n",
    "                                           return_sequences=True,name='encoder_lstm_' + str(i))))\n",
    "    encoder_hidden = encoder_lstm[i](encoder_hidden)\n",
    "\n",
    "encoder_lstm.append(Bidirectional(LSTM(encoder_dim[-1], dropout=dropout, return_state=True, \n",
    "                                       name='encoder_lstm_' + str(enc_dim-1))))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm[-1](encoder_hidden)\n",
    "\n",
    "# only need to keep encoder states\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the LSTM units in the decoder, with the states initialised by the encoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# initialise decoder states with encoder states\n",
    "decoder_lstm = []\n",
    "for i in range(dec_dim):\n",
    "    decoder_lstm.append(LSTM(decoder_dim[i], dropout=dropout, return_sequences=True,\n",
    "                             return_state=True, name='decoder_lstm_' + str(i)))\n",
    "    decoder_hidden, _, _ = decoder_lstm[i](decoder_hidden, initial_state=encoder_states)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a dense layer with output activation of choice on top of the last LSTM layer in the decoder and compile the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# add linear layer on top of LSTM\n",
    "decoder_dense = Dense(n_features, activation=output_activation, name='dense_output')\n",
    "decoder_outputs = decoder_dense(decoder_hidden)\n",
    "\n",
    "# define seq2seq model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder predictions are sequential and we only need the encoder states to initialise the decoder for the first item in the sequence. From then on, the output and state of the decoder at each step in the sequence is used to predict the next item. As a result, we define separate encoder and decoder models for the prediction stage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# define encoder model returning encoder states\n",
    "encoder_model = Model(encoder_inputs, encoder_states * dec_dim)\n",
    "\n",
    "# define decoder model\n",
    "# need state inputs for each LSTM layer\n",
    "decoder_states_inputs = []\n",
    "for i in range(dec_dim):\n",
    "    decoder_state_input_h = Input(shape=(decoder_dim[i],), name='decoder_state_input_h_' + str(i))\n",
    "    decoder_state_input_c = Input(shape=(decoder_dim[i],), name='decoder_state_input_c_' + str(i))\n",
    "    decoder_states_inputs.append([decoder_state_input_h, decoder_state_input_c])\n",
    "decoder_states_inputs = [state for states in decoder_states_inputs for state in states]\n",
    "\n",
    "decoder_inference = decoder_inputs\n",
    "decoder_states = []\n",
    "for i in range(dec_dim):\n",
    "    decoder_inference, state_h, state_c = decoder_lstm[i](decoder_inference, \n",
    "                                                          initial_state=decoder_states_inputs[2*i:2*i+2])\n",
    "    decoder_states.append([state_h,state_c])\n",
    "decoder_states = [state for states in decoder_states for state in states]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_inference)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq-LSTM model can be trained on a batch of -ideally- inliers by running the ```train.py``` script with the desired hyperparameters. The example below trains the model on the first 2628 ECG's of the ECG5000 dataset. The input/output sequence has a length of 140, the encoder has 1 bidirectional LSTM layer with 20 units, and the decoder consists of 1 LSTM layer with 40 units. This has to be 2x the number of units of the bidirectional encoder because both the forward and backward encoder states are used to initialise the decoder. Feature-wise minmax scaling between 0 and 1 is applied to the input sequence so we can use a sigmoid activation in the decoder's output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!python train.py \\\n",
    "--dataset './data/ECG5000_TEST.arff' \\\n",
    "--data_range 0 2627 \\\n",
    "--minmax \\\n",
    "--timesteps 140 \\\n",
    "--encoder_dim 20 \\\n",
    "--decoder_dim 40 \\\n",
    "--output_activation 'sigmoid' \\\n",
    "--dropout 0 \\\n",
    "--learning_rate 0.005 \\\n",
    "--loss 'mean_squared_error' \\\n",
    "--epochs 100 \\\n",
    "--batch_size 32 \\\n",
    "--validation_split 0.2 \\\n",
    "--model_name 'seq2seq' \\\n",
    "--print_progress \\\n",
    "--save \\\n",
    "--save_path './models/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model weights and hyperparameters are saved in the folder specified by \"save_path\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, which can then be served by Seldon Core, the pre-trained model weights and hyperparameters are loaded when defining an OutlierSeq2SeqLSTM object. The \"threshold\" argument defines above which reconstruction error a sample is classified as an outlier. The threshold is a key hyperparameter and needs to be picked carefully for each application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OutlierSeq2SeqLSTM(object):\n",
    "    \"\"\" Outlier detection using a sequence-to-sequence (seq2seq) LSTM model.\n",
    "    \n",
    "    Arguments:\n",
    "        - threshold: (float): reconstruction error (mse) threshold used to classify outliers\n",
    "        - reservoir_size (int): number of observations kept in memory using reservoir sampling\n",
    "     \n",
    "    Functions:\n",
    "        - reservoir_sampling: applies reservoir sampling to incoming data\n",
    "        - predict: detect and return outliers\n",
    "        - send_feedback: add target labels as part of the feedback loop\n",
    "        - metrics: return custom metrics\n",
    "    \"\"\"\n",
    "    def __init__(self,threshold=0.003,reservoir_size=50000,model_name='model',load_path='./models/'):\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.batch = []\n",
    "        self.N = 0 # total sample count up until now for reservoir sampling\n",
    "        \n",
    "        # load model architecture parameters\n",
    "        with open(load_path + model_name + '.pickle', 'rb') as f:\n",
    "            self.timesteps, self.n_features, encoder_dim, decoder_dim, output_activation = pickle.load(f)\n",
    "            \n",
    "        # instantiate model\n",
    "        self.s2s, self.enc, self.dec = model(self.n_features,encoder_dim=encoder_dim,\n",
    "                                             decoder_dim=decoder_dim,output_activation=output_activation)\n",
    "        self.s2s.load_weights(load_path + model_name + '_weights.h5') # load pretrained model weights\n",
    "        self.s2s._make_predict_function()\n",
    "        self.enc._make_predict_function()\n",
    "        self.dec._make_predict_function()\n",
    "        \n",
    "        # load data preprocessing info\n",
    "        with open(load_path + 'preprocess_' + model_name + '.pickle', 'rb') as f:\n",
    "            preprocess = pickle.load(f)\n",
    "        self.preprocess, self.clip, self.axis = preprocess[:3]\n",
    "        if self.preprocess=='minmax':\n",
    "            self.xmin, self.xmax = preprocess[3:5]\n",
    "            self.min, self.max = preprocess[5:]\n",
    "        elif self.preprocess=='standardized':\n",
    "            self.mu, self.sigma = preprocess[3:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict method does the actual outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def predict(self,X,feature_names):\n",
    "    \"\"\" Detect outliers from mse using the threshold. \n",
    "\n",
    "    Arguments:\n",
    "        - X: input data\n",
    "        - feature_names\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data is (optionally) clipped. If the number of observations fed to the outlier detector up until now is at least equal to the defined reservoir size, the feature-wise scaling parameters are updated using the observations in the reservoir. The reservoir is updated each observation using reservoir sampling. We can then scale the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# clip data per feature\n",
    "for col,clip in enumerate(self.clip):\n",
    "    X[:,:,col] = np.clip(X[:,:,col],-clip,clip)\n",
    "\n",
    "# update reservoir\n",
    "if self.N < self.reservoir_size:\n",
    "    update_stand = False\n",
    "else:\n",
    "    update_stand = True\n",
    "\n",
    "self.reservoir_sampling(X,update_stand=update_stand)\n",
    "\n",
    "# apply scaling\n",
    "if self.preprocess=='minmax':\n",
    "    X = ((X - self.xmin) / (self.xmax - self.xmin)) * (self.max - self.min) + self.min\n",
    "elif self.preprocess=='standardized':\n",
    "    X = (X - self.mu) / (self.sigma + 1e-10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make predictions using the ```decode_sequence``` function and calculate the mean squared error between the input and output sequences. If this value is above the threshold, an outlier is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# make predictions\n",
    "n_obs = X.shape[0]\n",
    "self.mse = np.zeros(n_obs)\n",
    "for obs in range(n_obs):\n",
    "    input_seq = X[obs:obs+1,:,:]\n",
    "    decoded_seq = self.decode_sequence(input_seq)\n",
    "    self.mse[obs] = np.mean(np.power(input_seq[0,:,:] - decoded_seq[0,:,:], 2))\n",
    "self.prediction = np.array([1 if e > self.threshold else 0 for e in self.mse]).astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```decode_sequence``` function takes an input sequence and uses the encoder model to retrieve the state vectors of the last LSTM layer in the encoder so they can be used to initialise the LSTM layers in the decoder. The feature values of the first step in the input sequence are used to initialise the output sequence. We can then use the decoder model to make sequential predictions for the output sequence. At each step, we use the previous step's output value and state as decoder inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def decode_sequence(self,input_seq):\n",
    "    \"\"\" Feed output of encoder to decoder and make sequential predictions. \"\"\"\n",
    "\n",
    "    # use encoder the get state vectors\n",
    "    states_value = self.enc.predict(input_seq)\n",
    "\n",
    "    # generate initial target sequence\n",
    "    target_seq = input_seq[0,0,:].reshape((1,1,self.n_features))\n",
    "\n",
    "    # sequential prediction of time series\n",
    "    decoded_seq = np.zeros((1, self.timesteps, self.n_features))\n",
    "    decoded_seq[0,0,:] = target_seq[0,0,:]\n",
    "    i = 1\n",
    "    while i < self.timesteps:\n",
    "\n",
    "        decoder_output = self.dec.predict([target_seq] + states_value)\n",
    "\n",
    "        # update the target sequence\n",
    "        target_seq = np.zeros((1, 1, self.n_features))\n",
    "        target_seq[0, 0, :] = decoder_output[0]\n",
    "\n",
    "        # update output\n",
    "        decoded_seq[0, i, :] = decoder_output[0]\n",
    "\n",
    "        # update states\n",
    "        states_value = decoder_output[1:]\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    return decoded_seq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Francois Chollet. A ten-minute introduction to sequence-to-sequence learning in Keras\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "Christopher Olah. Understanding LSTM Networks\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Ilya Sutskever, Oriol Vinyals and Quoc V. Le. Sequence to Sequence Learning with Neural Networks. 2014\n",
    "- https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
