{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Wrapper Benchmarking\n",
    "\n",
    "## Prequisites\n",
    "\n",
    " * An authenticated K8S cluster with istio and Seldon Core installed\n",
    " * vegeta and ghz benchmarking tools\n",
    " \n",
    " Port forward to istio\n",
    " \n",
    " ```\n",
    " kubectl port-forward $(kubectl get pods -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') -n istio-system 8003:8080\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0-dev'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION=!cat ../../../version.txt\n",
    "VERSION=VERSION[0]\n",
    "VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): namespaces \"seldon\" already exists\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create namespace seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"seldon-core\" has been upgraded. Happy Helming!\r\n",
      "NAME: seldon-core\r\n",
      "LAST DEPLOYED: Mon Jun 28 10:20:06 2021\r\n",
      "NAMESPACE: seldon-system\r\n",
      "STATUS: deployed\r\n",
      "REVISION: 3\r\n",
      "TEST SUITE: None\r\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install seldon-core seldon-core-operator --repo https://storage.googleapis.com/seldon-charts --version 1.9.0 --namespace seldon-system --set istio.enabled=\"true\" --set istio.gateway=\"seldon-gateway.istio-system.svc.cluster.local\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Predict method\n",
    "\n",
    "The `seldontest_predict` has simply a `predict` method that does a loop with a configurable number of iterations (default 1) to simulate work. The iterations can be set as a Seldon parameter but in this case we are looking to benchmark the serialization/deserialization cost so want a minimal amount of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate model.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-model\n",
    "  namespace: seldon\n",
    "spec:\n",
    "  predictors:\n",
    "  - annotations:\n",
    "      seldon.io/no-engine: \"true\"\n",
    "    componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/seldontest_predict:{VERSION}\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: classifier\n",
    "          resources:\n",
    "            requests:\n",
    "              cpu: 1\n",
    "            limits:\n",
    "              cpu: 1\n",
    "          env:\n",
    "          - name: GUNICORN_WORKERS\n",
    "            value: \"1\"\n",
    "          - name: GUNICORN_THREADS\n",
    "            value: \"1\"\n",
    "        tolerations:\n",
    "        - key: model\n",
    "          operator: Exists\n",
    "          effect: NoSchedule\n",
    "    graph:\n",
    "      children: []\n",
    "      name: classifier\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/seldon-model created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/seldon-model-default-0-classifier-5445bd4ccf-rfj8n condition met\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl wait --for condition=ready --timeout=600s pods --all -n seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create payloads and associated vegeta configurations for\n",
    "\n",
    "  1. ndarray\n",
    "  1. tensor\n",
    "  1. tftensor\n",
    "  \n",
    "  We will create an array of 100,000 consecutive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 100000\n",
    "vals = list(range(sz))\n",
    "valStr = f\"{vals}\"\n",
    "payload = '{\"data\": {\"ndarray\": ['+valStr+']}}'\n",
    "with open(\"data_ndarray.json\",\"w\") as f:\n",
    "    f.write(payload)\n",
    "payload_tensor = '{\"data\":{\"tensor\":{\"shape\":[1,'+str(sz)+'],\"values\":'+valStr+'}}}'\n",
    "with open(\"data_tensor.json\",\"w\") as f:\n",
    "    f.write(payload_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from google.protobuf import json_format \n",
    "import numpy as np\n",
    "array = np.array(vals)\n",
    "tftensor = tf.make_tensor_proto(array)\n",
    "jStrTensor = json_format.MessageToJson(tftensor)\n",
    "jTensor = json.loads(jStrTensor)\n",
    "payload_tftensor = '{\"data\":{\"tftensor\":'+json.dumps(jTensor,separators=(',', ':'))+'}}'\n",
    "with open(\"data_tftensor.json\",\"w\") as f:\n",
    "    f.write(payload_tftensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "  \n",
    "sample_string_bytes = payload_tensor.encode(\"ascii\")\n",
    "base64_bytes = base64.b64encode(sample_string_bytes)\n",
    "base64_string = base64_bytes.decode(\"ascii\")\n",
    "jqPayload = {\"method\": \"POST\", \n",
    "             \"url\": \"http://localhost:8003/seldon/seldon/seldon-model/api/v1.0/predictions\", \n",
    "             \"body\": base64_string,\n",
    "             \"header\": {\"Content-Type\": [\"application/json\"]}}\n",
    "with open(\"vegeta_tensor.json\",\"w\") as f:\n",
    "    f.write(json.dumps(jqPayload,separators=(',', ':')))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "sample_string_bytes = payload.encode(\"ascii\")\n",
    "base64_bytes = base64.b64encode(sample_string_bytes)\n",
    "base64_string = base64_bytes.decode(\"ascii\")\n",
    "jqPayload = {\"method\": \"POST\", \n",
    "             \"url\": \"http://localhost:8003/seldon/seldon/seldon-model/api/v1.0/predictions\", \n",
    "             \"body\": base64_string,\n",
    "             \"header\": {\"Content-Type\": [\"application/json\"]}}\n",
    "with open(\"vegeta_ndarray.json\",\"w\") as f:\n",
    "    f.write(json.dumps(jqPayload,separators=(',', ':')))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "sample_string_bytes = payload_tftensor.encode(\"ascii\")\n",
    "base64_bytes = base64.b64encode(sample_string_bytes)\n",
    "base64_string = base64_bytes.decode(\"ascii\")\n",
    "jqPayload = {\"method\": \"POST\", \n",
    "             \"url\": \"http://localhost:8003/seldon/seldon/seldon-model/api/v1.0/predictions\", \n",
    "             \"body\": base64_string,\n",
    "             \"header\": {\"Content-Type\": [\"application/json\"]}}\n",
    "with open(\"vegeta_tftensor.json\",\"w\") as f:\n",
    "    f.write(json.dumps(jqPayload,separators=(',', ':')))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test port-forward to check everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[],\"tftensor\":{\"dtype\":\"DT_INT64\",\"int64Val\":[\"1\"],\"tensorShape\":{\"dim\":[{\"size\":\"1\"}]}}},\"meta\":{\"requestPath\":{\"classifier\":\"seldonio/seldontest_predict:1.10.0-dev\"}}}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "   -d '@./data_tftensor.json' \\\n",
    "    http://localhost:8003/seldon/seldon/seldon-model/api/v1.0/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test REST\n",
    "\n",
    " 1. ndarray\n",
    " 1. tensor\n",
    " 1. tftensor\n",
    " \n",
    " This can be done locally as the results should be indicative of the relative differences rather than very accurate timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         505, 50.49, 50.39\n",
      "Duration      [total, attack, wait]             10.022s, 10.002s, 19.945ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  17.513ms, 19.841ms, 19.551ms, 21.102ms, 22.914ms, 25.205ms, 31.239ms\n",
      "Bytes In      [total, mean]                     58075, 115.00\n",
      "Bytes Out     [total, mean]                     347902075, 688915.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:505  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_ndarray.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         509, 50.85, 50.75\n",
      "Duration      [total, attack, wait]             10.029s, 10.01s, 18.859ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  17.584ms, 19.7ms, 19.531ms, 20.669ms, 21.421ms, 24.327ms, 26.721ms\n",
      "Bytes In      [total, mean]                     69733, 137.00\n",
      "Bytes Out     [total, mean]                     350670460, 688940.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:509  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_tensor.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         620, 61.94, 61.84\n",
      "Duration      [total, attack, wait]             10.026s, 10.01s, 16.001ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  13.766ms, 16.168ms, 15.876ms, 17.819ms, 18.89ms, 21.744ms, 32.567ms\n",
      "Bytes In      [total, mean]                     115940, 187.00\n",
      "Bytes Out     [total, mean]                     661398020, 1066771.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:620  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_tftensor.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results\n",
    "\n",
    "| ndarray | tensor | tftensor |\n",
    "| ------- | ------ | -------- |\n",
    "| 19.8ms | 19.7ms | 16.2ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test gRPC\n",
    " \n",
    "  1. ndarray\n",
    "  1. tensor\n",
    "  1. tftensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t23\n",
      "  Total:\t10.06 s\n",
      "  Slowest:\t285.51 ms\n",
      "  Fastest:\t254.39 ms\n",
      "  Average:\t253.76 ms\n",
      "  Requests/sec:\t2.29\n",
      "\n",
      "Response time histogram:\n",
      "  254.391 [1]\t|∎∎∎∎∎∎∎\n",
      "  257.502 [1]\t|∎∎∎∎∎∎∎\n",
      "  260.614 [6]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  263.725 [3]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  266.837 [5]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  269.948 [0]\t|\n",
      "  273.060 [2]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  276.171 [1]\t|∎∎∎∎∎∎∎\n",
      "  279.283 [2]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  282.394 [0]\t|\n",
      "  285.506 [1]\t|∎∎∎∎∎∎∎\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 257.61 ms \n",
      "  25 % in 258.97 ms \n",
      "  50 % in 265.91 ms \n",
      "  75 % in 272.90 ms \n",
      "  90 % in 278.27 ms \n",
      "  95 % in 285.51 ms \n",
      "  0 % in 0 ns \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         22 responses   \n",
      "  [Canceled]   1 responses    \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_ndarray.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t90\n",
      "  Total:\t10.10 s\n",
      "  Slowest:\t22.38 ms\n",
      "  Fastest:\t4.89 ms\n",
      "  Average:\t8.02 ms\n",
      "  Requests/sec:\t8.91\n",
      "\n",
      "Response time histogram:\n",
      "  4.892 [1]\t|∎\n",
      "  6.641 [48]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  8.390 [16]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  10.139 [11]\t|∎∎∎∎∎∎∎∎∎\n",
      "  11.888 [3]\t|∎∎∎\n",
      "  13.637 [1]\t|∎\n",
      "  15.386 [0]\t|\n",
      "  17.135 [2]\t|∎∎\n",
      "  18.884 [2]\t|∎∎\n",
      "  20.633 [3]\t|∎∎∎\n",
      "  22.382 [2]\t|∎∎\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 5.56 ms \n",
      "  25 % in 5.97 ms \n",
      "  50 % in 6.55 ms \n",
      "  75 % in 8.83 ms \n",
      "  90 % in 16.66 ms \n",
      "  95 % in 19.07 ms \n",
      "  0 % in 0 ns \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         89 responses   \n",
      "  [Canceled]   1 responses    \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_tensor.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t420\n",
      "  Total:\t10.05 s\n",
      "  Slowest:\t13.37 ms\n",
      "  Fastest:\t3.89 ms\n",
      "  Average:\t5.24 ms\n",
      "  Requests/sec:\t41.80\n",
      "\n",
      "Response time histogram:\n",
      "  3.890 [1]\t|\n",
      "  4.837 [178]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  5.785 [159]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  6.733 [43]\t|∎∎∎∎∎∎∎∎∎∎\n",
      "  7.680 [22]\t|∎∎∎∎∎\n",
      "  8.628 [7]\t|∎∎\n",
      "  9.576 [4]\t|∎\n",
      "  10.524 [3]\t|∎\n",
      "  11.471 [1]\t|\n",
      "  12.419 [0]\t|\n",
      "  13.367 [1]\t|\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 4.36 ms \n",
      "  25 % in 4.61 ms \n",
      "  50 % in 4.96 ms \n",
      "  75 % in 5.48 ms \n",
      "  90 % in 6.59 ms \n",
      "  95 % in 7.34 ms \n",
      "  99 % in 10.42 ms \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         419 responses   \n",
      "  [Canceled]   1 responses     \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_tftensor.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results\n",
    "\n",
    "| ndarray | tensor | tftensor |\n",
    "| ------- | ------ | -------- |\n",
    "| 253ms | 8.4ms | 5.5ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    " * gRPC is generally faster than REST except for ndarray which is much worse and should not be used with gRPC\n",
    " * tftensor is fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"seldon-model\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f model.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predct Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate model.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-model\n",
    "  namespace: seldon\n",
    "spec:\n",
    "  predictors:\n",
    "  - annotations:\n",
    "      seldon.io/no-engine: \"true\"\n",
    "    componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/seldontest_predict_raw:{VERSION}\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: classifier\n",
    "          resources:\n",
    "            requests:\n",
    "              cpu: 1\n",
    "            limits:\n",
    "              cpu: 1\n",
    "          env:\n",
    "          - name: GUNICORN_WORKERS\n",
    "            value: \"1\"\n",
    "          - name: GUNICORN_THREADS\n",
    "            value: \"1\"\n",
    "        tolerations:\n",
    "        - key: model\n",
    "          operator: Exists\n",
    "          effect: NoSchedule\n",
    "    graph:\n",
    "      children: []\n",
    "      name: classifier\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/seldon-model created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/seldon-model-default-0-classifier-5dc8fbd597-kk7td condition met\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl wait --for condition=ready --timeout=600s pods --all -n seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test port-forward to check everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\r\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "   -d '@./data_tftensor.json' \\\n",
    "    http://localhost:8003/seldon/seldon/seldon-model/api/v1.0/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test REST\n",
    "\n",
    " 1. ndarray\n",
    " 1. tensor\n",
    " 1. tftensor\n",
    " \n",
    " This can be done locally as the results should be indicative of the relative differences rather than very accurate timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         724, 72.35, 72.25\n",
      "Duration      [total, attack, wait]             10.021s, 10.007s, 14.458ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  12.228ms, 13.838ms, 13.683ms, 14.641ms, 15.489ms, 17.888ms, 22.263ms\n",
      "Bytes In      [total, mean]                     2896, 4.00\n",
      "Bytes Out     [total, mean]                     498774460, 688915.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:724  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_ndarray.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         724, 72.32, 72.22\n",
      "Duration      [total, attack, wait]             10.025s, 10.011s, 14.307ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  12.362ms, 13.844ms, 13.701ms, 14.655ms, 15.493ms, 17.976ms, 18.802ms\n",
      "Bytes In      [total, mean]                     2896, 4.00\n",
      "Bytes Out     [total, mean]                     498792560, 688940.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:724  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_tensor.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests      [total, rate, throughput]         901, 90.04, 89.93\n",
      "Duration      [total, attack, wait]             10.018s, 10.007s, 11.64ms\n",
      "Latencies     [min, mean, 50, 90, 95, 99, max]  8.955ms, 11.116ms, 10.994ms, 12.099ms, 12.721ms, 15.208ms, 19.918ms\n",
      "Bytes In      [total, mean]                     3604, 4.00\n",
      "Bytes Out     [total, mean]                     961160671, 1066771.00\n",
      "Success       [ratio]                           100.00%\n",
      "Status Codes  [code:count]                      200:901  \n",
      "Error Set:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "vegeta attack -format=json -duration=10s -rate=0 -max-workers=1 -targets=vegeta_tftensor.json | \n",
    "  vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results\n",
    "\n",
    "| ndarray | tensor | tftensor |\n",
    "| ------- | ------ | -------- |\n",
    "| 13.3ms | 13.3ms | 11.1ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test gRPC\n",
    " \n",
    "  1. ndarray\n",
    "  1. tensor\n",
    "  1. tftensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t44\n",
      "  Total:\t10.04 s\n",
      "  Slowest:\t69.07 ms\n",
      "  Fastest:\t44.44 ms\n",
      "  Average:\t46.03 ms\n",
      "  Requests/sec:\t4.38\n",
      "\n",
      "Response time histogram:\n",
      "  44.440 [1]\t|∎\n",
      "  46.904 [31]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  49.367 [6]\t|∎∎∎∎∎∎∎∎\n",
      "  51.831 [2]\t|∎∎∎\n",
      "  54.294 [2]\t|∎∎∎\n",
      "  56.758 [0]\t|\n",
      "  59.221 [0]\t|\n",
      "  61.684 [0]\t|\n",
      "  64.148 [0]\t|\n",
      "  66.611 [0]\t|\n",
      "  69.075 [1]\t|∎\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 45.05 ms \n",
      "  25 % in 45.40 ms \n",
      "  50 % in 46.30 ms \n",
      "  75 % in 47.34 ms \n",
      "  90 % in 50.16 ms \n",
      "  95 % in 53.38 ms \n",
      "  0 % in 0 ns \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         43 responses   \n",
      "  [Canceled]   1 responses    \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_ndarray.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t92\n",
      "  Total:\t10.10 s\n",
      "  Slowest:\t19.81 ms\n",
      "  Fastest:\t4.93 ms\n",
      "  Average:\t7.91 ms\n",
      "  Requests/sec:\t9.11\n",
      "\n",
      "Response time histogram:\n",
      "  4.932 [1]\t|∎\n",
      "  6.419 [53]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  7.907 [12]\t|∎∎∎∎∎∎∎∎∎\n",
      "  9.395 [5]\t|∎∎∎∎\n",
      "  10.882 [4]\t|∎∎∎\n",
      "  12.370 [1]\t|∎\n",
      "  13.858 [3]\t|∎∎\n",
      "  15.346 [3]\t|∎∎\n",
      "  16.833 [2]\t|∎∎\n",
      "  18.321 [3]\t|∎∎\n",
      "  19.809 [4]\t|∎∎∎\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 5.21 ms \n",
      "  25 % in 5.68 ms \n",
      "  50 % in 6.04 ms \n",
      "  75 % in 8.27 ms \n",
      "  90 % in 15.77 ms \n",
      "  95 % in 19.04 ms \n",
      "  0 % in 0 ns \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         91 responses   \n",
      "  [Canceled]   1 responses    \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_tensor.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Count:\t426\n",
      "  Total:\t10.03 s\n",
      "  Slowest:\t11.74 ms\n",
      "  Fastest:\t3.67 ms\n",
      "  Average:\t5.02 ms\n",
      "  Requests/sec:\t42.48\n",
      "\n",
      "Response time histogram:\n",
      "  3.668 [1]\t|\n",
      "  4.475 [174]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  5.282 [141]\t|∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n",
      "  6.089 [43]\t|∎∎∎∎∎∎∎∎∎∎\n",
      "  6.897 [30]\t|∎∎∎∎∎∎∎\n",
      "  7.704 [16]\t|∎∎∎∎\n",
      "  8.511 [6]\t|∎\n",
      "  9.318 [8]\t|∎∎\n",
      "  10.126 [2]\t|\n",
      "  10.933 [1]\t|\n",
      "  11.740 [3]\t|∎\n",
      "\n",
      "Latency distribution:\n",
      "  10 % in 4.08 ms \n",
      "  25 % in 4.27 ms \n",
      "  50 % in 4.61 ms \n",
      "  75 % in 5.30 ms \n",
      "  90 % in 6.62 ms \n",
      "  95 % in 7.66 ms \n",
      "  99 % in 10.26 ms \n",
      "\n",
      "Status code distribution:\n",
      "  [OK]         425 responses   \n",
      "  [Canceled]   1 responses     \n",
      "\n",
      "Error distribution:\n",
      "  [1]   rpc error: code = Canceled desc = grpc: the client connection is closing   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ghz \\\n",
    "    --insecure \\\n",
    "    --proto ../../../proto/prediction.proto \\\n",
    "    --call seldon.protos.Seldon/Predict \\\n",
    "    --data-file=./data_tftensor.json \\\n",
    "    --qps=0 \\\n",
    "    --cpus=1 \\\n",
    "    --concurrency=1 \\\n",
    "    --duration=\"10s\" \\\n",
    "    --format summary \\\n",
    "    --metadata='{\"seldon\": \"seldon-model\", \"namespace\": \"seldon\"}' \\\n",
    "    localhost:8003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example results\n",
    "\n",
    "| ndarray | tensor | tftensor |\n",
    "| ------- | ------ | -------- |\n",
    "| 46ms | 7.9ms | 5.0ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    " * `predict_raw` is faster than `predict` but you will need to handle the serialization/deserializtion yourself which maybe will make them equivalent unless specific techniques can be applied for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
