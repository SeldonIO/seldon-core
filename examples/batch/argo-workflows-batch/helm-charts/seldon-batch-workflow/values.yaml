workflow:
  # Name of the overarching argo workflow
  name: seldon-batch-process
  # If true the randomly generated string will be apended as the name of the workflow
  useNameAsGenerateName: false
  # Namespace where to create the workflow and all resources in batch job
  namespace: default
pvc:
  # Name of the persistent volume claim to be created
  name: seldon-pvc
  # Size of the storage volume to be created for the batch job
  storage: 2Mi
# Seldon deployment to be created for batch processing
seldonDeployment:
  # Name to use for the seldon deployment which by default appends generated workflow ID
  name: seldon-{{workflow.uid}}
  # Image to use for the batch client
  image: seldonio/seldon-core-s2i-python37:1.5.0
  # Prepackaged model server to use [see https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html]
  server: SKLEARN_SERVER
  # The URL for the model that is to be used
  modelUri: gs://seldon-models/v1.11.0-dev/sklearn/iris
  # The number of seldon deployment replicas to launch
  replicas: 2
  # Waiting time before checks for deployment to ensure kubernetes cluster registers create
  waitTime: 5
  # The number of threads spawned by Python Gunicorn Flask server
  serverThreads: 10
  # The number of workers spawned by Python Gunicorn Flask server
  serverWorkers: 1
  # Whether to enable resources
  enableResources: false
  requests:
    # Requests for CPU (only added if enableResources is enabled)
    cpu: 50m
    # Requests for memory (only added if enableResources is enabled)
    memory: 100Mi
  limits:
    # Limits for CPU (only added if enableResources is enabled)
    cpu: 50m
    # Limits for memory (only added if enableResources is enabled)
    memory: 1000Mi
# The batch worker is the component that will send the requests from the files
batchWorker:
  # Endpoint of for the batch client to contact the seldon deployment
  host: istio-ingressgateway.istio-system.svc.cluster.local
  # Number of parallel batch client workers to process the data
  workers: 100
  # Number of times client will try to send a request if it fails
  retries: 3
  # The payload type to convert if choosing "data" as the data-type param
  payloadType: "ndarray"
  # The type of data that is expected in the input data (json, str, data)
  dataType: "data"
  # Whether to enable benchmarking on the batch processor worker
  enableBenchmark: true
rclone:
  # Rclone Image Name
  image: rclone/rclone:1.53
  # Name of secret containing rclone.conf file
  configSecretName: rclone-config-secret
  # The source:sourcepath of file for rclone to pull that will contain the batch data to process
  inputDataPath: cluster-minio:data/input-data.txt
  # The dest:destpath for rclone to know where to push results of batch processing
  outputDataPath: cluster-minio:data/output-data-{{workflow.uid}}.txt
