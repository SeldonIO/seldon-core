workflow:
  # Name of the overarching argo workflow
  name: seldon-batch-process
  # If true the randomly generated string will be apended as the name of the workflow
  useNameAsGenerateName: false
  # Namespace where to create the workflow and all resources in batch job
  namespace: default
pvc:
  # Name of the persistent volume claim to be created
  name: seldon-pvc
  # Size of the storage volume to be created for the batch job 
  storage: 2Mi
# Seldon deployment to be created for batch processing
seldonDeployment:
  # Name to use for the seldon deployment which by default appends generated workflow ID
  name: seldon-{{workflow.uid}}
  # Image to use for the batch client
  image: seldonio/seldon-core-s2i-python37:1.3.0-dev
  # Prepackaged model server to use [see https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html]
  server: SKLEARN_SERVER
  # The URL for the model that is to be used
  modelUri: gs://seldon-models/sklearn/iris
  # The number of seldon deployment replicas to launch
  replicas: 2
  # Waiting time before checks for deployment to ensure kubernetes cluster registers create
  waitTime: 5
  # The number of threads spawned by Python Gunicorn Flask server
  serverThreads: 10
  # The number of workers spawned by Python Gunicorn Flask server
  serverWorkers: 1
  # Whether to enable resources
  enableResources: false
  requests:
    # Requests for CPU (only added if enableResources is enabled)
    cpu: 50m
    # Requests for memory (only added if enableResources is enabled)
    memory: 100Mi
  limits:
    # Limits for CPU (only added if enableResources is enabled)
    cpu: 50m
    # Limits for memory (only added if enableResources is enabled)
    memory: 1000Mi
# The batch worker is the component that will send the requests from the files
batchWorker:
  # Endpoint of for the batch client to contact the seldon deployment
  host: istio-ingressgateway.istio-system.svc.cluster.local
  # Number of parallel batch client workers to process the data
  workers: 100
  # Number of times client will try to send a request if it fails
  retries: 3
  # The payload type to convert if choosing "data" as the data-type param
  payloadType: "ndarray"
  # The type of data that is expected in the input data (json, str, data)
  dataType: "data"
  # Whether to enable benchmarking on the batch processor worker
  enableBenchmark: true
minio:
  # The location of the minio endpoint 
  endpoint: http://minio.minio-system.svc.cluster.local:9000
  # This is the secret that should contain the values to access minio
  secret:
    # The name of the secret which by default is "minio" but you can create a different one
    name: minio
    keyName:
      # The key name inside that secret to find the access key to authenticate minio
      accesskey: accesskey
      # The key name inside that secret to find the secret key to authenticate minio
      secretkey: secretkey
  # The name of the file inside of minio that will contain the batch data to process
  inputDataPath: data/input-data.txt
  # The name of the file inside of minio that will contain the batch data to process
  outputDataPath: data/output-data-{{workflow.uid}}.txt
