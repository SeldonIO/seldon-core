{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing with Argo Worfklows and HDFS\n",
    "\n",
    "In this notebook we will dive into how you can run batch processing with Argo Workflows and Seldon Core.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "* Seldon core installed as per the docs with an ingress\n",
    "* HDFS namenode/datanode accessible from your cluster (here in-cluster installation for demo)\n",
    "* Argo Workfklows installed in cluster (and argo CLI for commands)\n",
    "* Python `hdfscli` for interacting with the installed `hdfs` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Seldon Core\n",
    "Use the notebook to [set-up Seldon Core with Ambassador or Istio Ingress](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html).\n",
    "\n",
    "Note: If running with KIND you need to make sure do follow [these steps](https://github.com/argoproj/argo-workflows/issues/2376#issuecomment-595593237) as workaround to the `/.../docker.sock` known issue:\n",
    "```bash\n",
    "kubectl patch -n argo configmap workflow-controller-configmap \\\n",
    "    --type merge -p '{\"data\": {\"config\": \"containerRuntimeExecutor: k8sapi\"}}'\n",
    "```    \n",
    "\n",
    "### Install HDFS\n",
    "For this example we will need a running `hdfs` storage. We can use these [helm charts](https://artifacthub.io/packages/helm/gradiant/hdfs) from Gradiant.\n",
    "\n",
    "```bash\n",
    "helm repo add gradiant https://gradiant.github.io/charts/\n",
    "kubectl create namespace hdfs-system || echo \"namespace hdfs-system already exists\"\n",
    "helm install hdfs gradiant/hdfs --namespace hdfs-system\n",
    "```\n",
    "\n",
    "Once installation is complete, run in separate terminal a `port-forward` command for us to be able to push/pull batch data.\n",
    "```bash\n",
    "kubectl port-forward -n hdfs-system svc/hdfs-httpfs 14000:14000\n",
    "```\n",
    "\n",
    "\n",
    "### Install and configure hdfscli \n",
    "In this example we will be using [hdfscli](https://pypi.org/project/hdfs/) Python library for interacting with HDFS.\n",
    "It supports both the WebHDFS (and HttpFS) API as well as Kerberos authentication (not covered by the example).\n",
    "\n",
    "You can install it with\n",
    "```bash\n",
    "pip install hdfs==2.5.8\n",
    "```\n",
    "\n",
    "To be able to put `input-data.txt` for our batch job into hdfs we need to configure the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hdfscli.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile hdfscli.cfg\n",
    "[global]\n",
    "default.alias = batch\n",
    "\n",
    "[batch.alias]\n",
    "url = http://localhost:14000\n",
    "user = hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Argo Workflows\n",
    "You can follow the instructions from the official [Argo Workflows Documentation](https://github.com/argoproj/argo#quickstart).\n",
    "\n",
    "You also need to make sure that argo has permissions to create seldon deployments - for this you can create a role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting role.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile role.yaml\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: Role\n",
    "metadata:\n",
    "  name: workflow\n",
    "rules:\n",
    "- apiGroups:\n",
    "  - \"\"\n",
    "  resources:\n",
    "  - pods\n",
    "  verbs:\n",
    "  - \"*\"\n",
    "- apiGroups:\n",
    "  - \"apps\"\n",
    "  resources:\n",
    "  - deployments\n",
    "  verbs:\n",
    "  - \"*\"\n",
    "- apiGroups:\n",
    "  - \"\"\n",
    "  resources:\n",
    "  - pods/log\n",
    "  verbs:\n",
    "  - \"*\"\n",
    "- apiGroups:\n",
    "  - machinelearning.seldon.io\n",
    "  resources:\n",
    "  - \"*\"\n",
    "  verbs:\n",
    "  - \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role.rbac.authorization.k8s.io/workflow unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f role.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A service account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/workflow created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create serviceaccount workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolebinding.rbac.authorization.k8s.io/workflow created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create rolebinding workflow --role=workflow --serviceaccount=seldon:workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Seldon Deployment\n",
    "\n",
    "For purpose of this batch example we will assume that Seldon Deployment is created independently from the workflow logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: sklearn\n",
    "  namespace: seldon\n",
    "spec:\n",
    "  name: iris\n",
    "  predictors:\n",
    "  - graph:\n",
    "      children: []\n",
    "      implementation: SKLEARN_SERVER\n",
    "      modelUri: gs://seldon-models/v1.14.0/sklearn/iris\n",
    "      name: classifier\n",
    "      logger:\n",
    "        mode: all\n",
    "    name: default\n",
    "    replicas: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/sklearn configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment \"sklearn-default-0-classifier\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n seldon rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=sklearn -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "with open(\"input-data.txt\", \"w\") as f:\n",
    "    for _ in range(10000):\n",
    "        data = [random.random() for _ in range(4)]\n",
    "        data = \"[[\" + \", \".join(str(x) for x in data) + \"]]\\n\"\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HDFSCLI_CONFIG=./hdfscli.cfg hdfscli upload input-data.txt /batch-data/input-data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare HDFS config / client image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For connecting to the `hdfs` from inside the cluster we will use the same `hdfscli` tool as we used above to put data in there.\n",
    "\n",
    "We will configure `hdfscli` using `hdfscli.cfg` file stored inside kubernetes secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hdfs-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hdfs-config.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: seldon-hdfscli-secret-file\n",
    "type: Opaque\n",
    "stringData:\n",
    "  hdfscli.cfg: |\n",
    "    [global]\n",
    "    default.alias = batch\n",
    "\n",
    "    [batch.alias]\n",
    "    url = http://hdfs-httpfs.hdfs-system.svc.cluster.local:14000\n",
    "    user = hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/seldon-hdfscli-secret-file configured\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f hdfs-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the client image we will use a following minimal Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.8\n",
    "RUN pip install hdfs==2.5.8\n",
    "ENV HDFSCLI_CONFIG /etc/hdfs/hdfscli.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is build and published as `seldonio/hdfscli:1.6.0-dev`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workflow\n",
    "\n",
    "This simple workflow will consist of three stages:\n",
    "- download-input-data\n",
    "- process-batch-inputs\n",
    "- upload-output-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting workflow.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile workflow.yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Workflow\n",
    "metadata:\n",
    "  name: sklearn-batch-job\n",
    "  namespace: seldon\n",
    "\n",
    "  labels:\n",
    "    deployment-name: sklearn\n",
    "    deployment-kind: SeldonDeployment\n",
    "\n",
    "spec:\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: seldon-job-pvc\n",
    "      namespace: seldon\n",
    "      ownerReferences:\n",
    "      - apiVersion: argoproj.io/v1alpha1\n",
    "        blockOwnerDeletion: true\n",
    "        kind: Workflow\n",
    "        name: '{{workflow.name}}'\n",
    "        uid: '{{workflow.uid}}'\n",
    "    spec:\n",
    "      accessModes:\n",
    "      - ReadWriteOnce\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 1Gi\n",
    "\n",
    "  volumes:\n",
    "  - name: config\n",
    "    secret:\n",
    "      secretName: seldon-hdfscli-secret-file\n",
    "\n",
    "  arguments:\n",
    "    parameters:\n",
    "    - name: batch_deployment_name\n",
    "      value: sklearn\n",
    "    - name: batch_namespace\n",
    "      value: seldon\n",
    "\n",
    "    - name: input_path\n",
    "      value: /batch-data/input-data.txt\n",
    "    - name: output_path\n",
    "      value: /batch-data/output-data-{{workflow.name}}.txt\n",
    "\n",
    "    - name: batch_gateway_type\n",
    "      value: istio\n",
    "    - name: batch_gateway_endpoint\n",
    "      value: istio-ingressgateway.istio-system.svc.cluster.local\n",
    "    - name: batch_transport_protocol\n",
    "      value: rest\n",
    "    - name: workers\n",
    "      value: \"10\"\n",
    "    - name: retries\n",
    "      value: \"3\"\n",
    "    - name: data_type\n",
    "      value: data\n",
    "    - name: payload_type\n",
    "      value: ndarray\n",
    "\n",
    "  entrypoint: seldon-batch-process\n",
    "\n",
    "  templates:\n",
    "  - name: seldon-batch-process\n",
    "    steps:\n",
    "    - - arguments: {}\n",
    "        name: download-input-data\n",
    "        template: download-input-data\n",
    "    - - arguments: {}\n",
    "        name: process-batch-inputs\n",
    "        template: process-batch-inputs\n",
    "    - - arguments: {}\n",
    "        name: upload-output-data\n",
    "        template: upload-output-data\n",
    "\n",
    "  - name: download-input-data\n",
    "    script:\n",
    "      image: seldonio/hdfscli:1.6.0-dev\n",
    "      volumeMounts:\n",
    "      - mountPath: /assets\n",
    "        name: seldon-job-pvc\n",
    "\n",
    "      - mountPath: /etc/hdfs\n",
    "        name: config\n",
    "        readOnly: true\n",
    "\n",
    "      env:\n",
    "      - name: INPUT_DATA_PATH\n",
    "        value: '{{workflow.parameters.input_path}}'\n",
    "\n",
    "      - name: HDFSCLI_CONFIG\n",
    "        value: /etc/hdfs/hdfscli.cfg\n",
    "\n",
    "      command: [sh]\n",
    "      source: |\n",
    "        hdfscli download ${INPUT_DATA_PATH} /assets/input-data.txt\n",
    "\n",
    "  - name: process-batch-inputs\n",
    "    container:\n",
    "      image: seldonio/seldon-core-s2i-python37:1.14.0\n",
    "\n",
    "      volumeMounts:\n",
    "      - mountPath: /assets\n",
    "        name: seldon-job-pvc\n",
    "\n",
    "      env:\n",
    "      - name: SELDON_BATCH_DEPLOYMENT_NAME\n",
    "        value: '{{workflow.parameters.batch_deployment_name}}'\n",
    "      - name: SELDON_BATCH_NAMESPACE\n",
    "        value: '{{workflow.parameters.batch_namespace}}'\n",
    "      - name: SELDON_BATCH_GATEWAY_TYPE\n",
    "        value: '{{workflow.parameters.batch_gateway_type}}'\n",
    "      - name: SELDON_BATCH_HOST\n",
    "        value: '{{workflow.parameters.batch_gateway_endpoint}}'\n",
    "      - name: SELDON_BATCH_TRANSPORT\n",
    "        value: '{{workflow.parameters.batch_transport_protocol}}'\n",
    "      - name: SELDON_BATCH_DATA_TYPE\n",
    "        value: '{{workflow.parameters.data_type}}'\n",
    "      - name: SELDON_BATCH_PAYLOAD_TYPE\n",
    "        value: '{{workflow.parameters.payload_type}}'\n",
    "      - name: SELDON_BATCH_WORKERS\n",
    "        value: '{{workflow.parameters.workers}}'\n",
    "      - name: SELDON_BATCH_RETRIES\n",
    "        value: '{{workflow.parameters.retries}}'\n",
    "      - name: SELDON_BATCH_INPUT_DATA_PATH\n",
    "        value: /assets/input-data.txt\n",
    "      - name: SELDON_BATCH_OUTPUT_DATA_PATH\n",
    "        value: /assets/output-data.txt\n",
    "\n",
    "      command: [seldon-batch-processor]\n",
    "      args: [--benchmark]\n",
    "\n",
    "\n",
    "  - name: upload-output-data\n",
    "    script:\n",
    "      image: seldonio/hdfscli:1.6.0-dev\n",
    "      volumeMounts:\n",
    "      - mountPath: /assets\n",
    "        name: seldon-job-pvc\n",
    "\n",
    "      - mountPath: /etc/hdfs\n",
    "        name: config\n",
    "        readOnly: true\n",
    "\n",
    "      env:\n",
    "      - name: OUTPUT_DATA_PATH\n",
    "        value: '{{workflow.parameters.output_path}}'\n",
    "\n",
    "      - name: HDFSCLI_CONFIG\n",
    "        value: /etc/hdfs/hdfscli.cfg\n",
    "\n",
    "      command: [sh]\n",
    "      source: |\n",
    "        hdfscli upload /assets/output-data.txt ${OUTPUT_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                sklearn-batch-job\n",
      "Namespace:           seldon\n",
      "ServiceAccount:      workflow\n",
      "Status:              Pending\n",
      "Created:             Thu Jan 14 18:36:52 +0000 (now)\n",
      "Progress:            \n",
      "Parameters:          \n",
      "  batch_deployment_name: sklearn\n",
      "  batch_namespace:   seldon\n",
      "  input_path:        /batch-data/input-data.txt\n",
      "  output_path:       /batch-data/output-data-{{workflow.name}}.txt\n",
      "  batch_gateway_type: istio\n",
      "  batch_gateway_endpoint: istio-ingressgateway.istio-system.svc.cluster.local\n",
      "  batch_transport_protocol: rest\n",
      "  workers:           10\n",
      "  retries:           3\n",
      "  data_type:         data\n",
      "  payload_type:      ndarray\n"
     ]
    }
   ],
   "source": [
    "!argo submit --serviceaccount workflow workflow.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                STATUS    AGE   DURATION   PRIORITY\n",
      "sklearn-batch-job   Running   1s    1s         0\n"
     ]
    }
   ],
   "source": [
    "!argo list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                sklearn-batch-job\n",
      "Namespace:           seldon\n",
      "ServiceAccount:      workflow\n",
      "Status:              Running\n",
      "Created:             Thu Jan 14 18:36:52 +0000 (39 seconds ago)\n",
      "Started:             Thu Jan 14 18:36:52 +0000 (39 seconds ago)\n",
      "Duration:            39 seconds\n",
      "Progress:            1/2\n",
      "ResourcesDuration:   1s*(100Mi memory),1s*(1 cpu)\n",
      "Parameters:          \n",
      "  batch_deployment_name: sklearn\n",
      "  batch_namespace:   seldon\n",
      "  input_path:        /batch-data/input-data.txt\n",
      "  output_path:       /batch-data/output-data-{{workflow.name}}.txt\n",
      "  batch_gateway_type: istio\n",
      "  batch_gateway_endpoint: istio-ingressgateway.istio-system.svc.cluster.local\n",
      "  batch_transport_protocol: rest\n",
      "  workers:           10\n",
      "  retries:           3\n",
      "  data_type:         data\n",
      "  payload_type:      ndarray\n",
      "\n",
      "\u001b[39mSTEP\u001b[0m                         TEMPLATE              PODNAME                       DURATION  MESSAGE\n",
      " \u001b[36m●\u001b[0m sklearn-batch-job         seldon-batch-process                                            \n",
      " ├───\u001b[32m✔\u001b[0m download-input-data   download-input-data   sklearn-batch-job-2227322232  6s          \n",
      " └───\u001b[36m●\u001b[0m process-batch-inputs  process-batch-inputs  sklearn-batch-job-2877616693  29s         \n"
     ]
    }
   ],
   "source": [
    "!argo get sklearn-batch-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:05,000 - batch_processor.py:167 - INFO:  Processed instances: 100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:05,417 - batch_processor.py:167 - INFO:  Processed instances: 200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:06,213 - batch_processor.py:167 - INFO:  Processed instances: 300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:06,642 - batch_processor.py:167 - INFO:  Processed instances: 400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:06,974 - batch_processor.py:167 - INFO:  Processed instances: 500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:07,278 - batch_processor.py:167 - INFO:  Processed instances: 600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:07,628 - batch_processor.py:167 - INFO:  Processed instances: 700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:08,378 - batch_processor.py:167 - INFO:  Processed instances: 800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:09,003 - batch_processor.py:167 - INFO:  Processed instances: 900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:09,337 - batch_processor.py:167 - INFO:  Processed instances: 1000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:09,697 - batch_processor.py:167 - INFO:  Processed instances: 1100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:10,014 - batch_processor.py:167 - INFO:  Processed instances: 1200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:10,349 - batch_processor.py:167 - INFO:  Processed instances: 1300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:10,843 - batch_processor.py:167 - INFO:  Processed instances: 1400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:11,207 - batch_processor.py:167 - INFO:  Processed instances: 1500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:11,562 - batch_processor.py:167 - INFO:  Processed instances: 1600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:11,975 - batch_processor.py:167 - INFO:  Processed instances: 1700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:12,350 - batch_processor.py:167 - INFO:  Processed instances: 1800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:12,783 - batch_processor.py:167 - INFO:  Processed instances: 1900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:13,139 - batch_processor.py:167 - INFO:  Processed instances: 2000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:13,563 - batch_processor.py:167 - INFO:  Processed instances: 2100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:13,928 - batch_processor.py:167 - INFO:  Processed instances: 2200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:14,352 - batch_processor.py:167 - INFO:  Processed instances: 2300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:14,699 - batch_processor.py:167 - INFO:  Processed instances: 2400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:15,042 - batch_processor.py:167 - INFO:  Processed instances: 2500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:15,701 - batch_processor.py:167 - INFO:  Processed instances: 2600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:16,124 - batch_processor.py:167 - INFO:  Processed instances: 2700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:16,748 - batch_processor.py:167 - INFO:  Processed instances: 2800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:17,300 - batch_processor.py:167 - INFO:  Processed instances: 2900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:17,904 - batch_processor.py:167 - INFO:  Processed instances: 3000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:18,454 - batch_processor.py:167 - INFO:  Processed instances: 3100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:18,823 - batch_processor.py:167 - INFO:  Processed instances: 3200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:19,236 - batch_processor.py:167 - INFO:  Processed instances: 3300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:19,586 - batch_processor.py:167 - INFO:  Processed instances: 3400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:20,317 - batch_processor.py:167 - INFO:  Processed instances: 3500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:20,948 - batch_processor.py:167 - INFO:  Processed instances: 3600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:21,356 - batch_processor.py:167 - INFO:  Processed instances: 3700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:21,851 - batch_processor.py:167 - INFO:  Processed instances: 3800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:22,205 - batch_processor.py:167 - INFO:  Processed instances: 3900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:22,553 - batch_processor.py:167 - INFO:  Processed instances: 4000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:23,051 - batch_processor.py:167 - INFO:  Processed instances: 4100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:23,557 - batch_processor.py:167 - INFO:  Processed instances: 4200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:24,016 - batch_processor.py:167 - INFO:  Processed instances: 4300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:24,350 - batch_processor.py:167 - INFO:  Processed instances: 4400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:24,883 - batch_processor.py:167 - INFO:  Processed instances: 4500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:25,295 - batch_processor.py:167 - INFO:  Processed instances: 4600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:25,669 - batch_processor.py:167 - INFO:  Processed instances: 4700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:26,055 - batch_processor.py:167 - INFO:  Processed instances: 4800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:26,795 - batch_processor.py:167 - INFO:  Processed instances: 4900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:27,462 - batch_processor.py:167 - INFO:  Processed instances: 5000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:27,887 - batch_processor.py:167 - INFO:  Processed instances: 5100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:28,332 - batch_processor.py:167 - INFO:  Processed instances: 5200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:28,742 - batch_processor.py:167 - INFO:  Processed instances: 5300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:29,069 - batch_processor.py:167 - INFO:  Processed instances: 5400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:29,443 - batch_processor.py:167 - INFO:  Processed instances: 5500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:29,840 - batch_processor.py:167 - INFO:  Processed instances: 5600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:30,235 - batch_processor.py:167 - INFO:  Processed instances: 5700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:30,578 - batch_processor.py:167 - INFO:  Processed instances: 5800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:31,024 - batch_processor.py:167 - INFO:  Processed instances: 5900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:31,381 - batch_processor.py:167 - INFO:  Processed instances: 6000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:31,847 - batch_processor.py:167 - INFO:  Processed instances: 6100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:32,239 - batch_processor.py:167 - INFO:  Processed instances: 6200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:32,603 - batch_processor.py:167 - INFO:  Processed instances: 6300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:33,080 - batch_processor.py:167 - INFO:  Processed instances: 6400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:33,567 - batch_processor.py:167 - INFO:  Processed instances: 6500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:34,043 - batch_processor.py:167 - INFO:  Processed instances: 6600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:34,444 - batch_processor.py:167 - INFO:  Processed instances: 6700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:34,812 - batch_processor.py:167 - INFO:  Processed instances: 6800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:35,148 - batch_processor.py:167 - INFO:  Processed instances: 6900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:35,519 - batch_processor.py:167 - INFO:  Processed instances: 7000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:35,873 - batch_processor.py:167 - INFO:  Processed instances: 7100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:36,278 - batch_processor.py:167 - INFO:  Processed instances: 7200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:36,694 - batch_processor.py:167 - INFO:  Processed instances: 7300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:37,061 - batch_processor.py:167 - INFO:  Processed instances: 7400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:37,509 - batch_processor.py:167 - INFO:  Processed instances: 7500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:37,865 - batch_processor.py:167 - INFO:  Processed instances: 7600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:38,211 - batch_processor.py:167 - INFO:  Processed instances: 7700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:38,590 - batch_processor.py:167 - INFO:  Processed instances: 7800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:39,028 - batch_processor.py:167 - INFO:  Processed instances: 7900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:39,419 - batch_processor.py:167 - INFO:  Processed instances: 8000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:39,910 - batch_processor.py:167 - INFO:  Processed instances: 8100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:40,532 - batch_processor.py:167 - INFO:  Processed instances: 8200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:41,022 - batch_processor.py:167 - INFO:  Processed instances: 8300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:41,436 - batch_processor.py:167 - INFO:  Processed instances: 8400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:41,800 - batch_processor.py:167 - INFO:  Processed instances: 8500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:42,238 - batch_processor.py:167 - INFO:  Processed instances: 8600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:42,704 - batch_processor.py:167 - INFO:  Processed instances: 8700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:43,079 - batch_processor.py:167 - INFO:  Processed instances: 8800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:43,712 - batch_processor.py:167 - INFO:  Processed instances: 8900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:44,075 - batch_processor.py:167 - INFO:  Processed instances: 9000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:44,459 - batch_processor.py:167 - INFO:  Processed instances: 9100\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:44,806 - batch_processor.py:167 - INFO:  Processed instances: 9200\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:45,344 - batch_processor.py:167 - INFO:  Processed instances: 9300\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:45,764 - batch_processor.py:167 - INFO:  Processed instances: 9400\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:46,110 - batch_processor.py:167 - INFO:  Processed instances: 9500\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:46,547 - batch_processor.py:167 - INFO:  Processed instances: 9600\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:46,987 - batch_processor.py:167 - INFO:  Processed instances: 9700\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:47,371 - batch_processor.py:167 - INFO:  Processed instances: 9800\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:47,905 - batch_processor.py:167 - INFO:  Processed instances: 9900\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:48,289 - batch_processor.py:167 - INFO:  Processed instances: 10000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:48,290 - batch_processor.py:168 - INFO:  Total processed instances: 10000\u001b[0m\n",
      "\u001b[35msklearn-batch-job-2877616693: 2021-01-14 18:37:48,290 - batch_processor.py:116 - INFO:  Elapsed time: 43.7087140083313\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!argo logs sklearn-batch-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull output-data from hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HDFSCLI_CONFIG=./hdfscli.cfg hdfscli download /batch-data/output-data-sklearn-batch-job.txt output-data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.49551509682202705, 0.4192462053867995, 0.08523869779117352]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 0.0, \"batch_instance_id\": \"409ad222-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.14889581912569078, 0.40048258722097885, 0.45062159365333043]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 10.0, \"batch_instance_id\": \"409d56e6-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.1859090109477526, 0.46433848375587844, 0.349752505296369]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 1.0, \"batch_instance_id\": \"409ad68c-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.35453094061556073, 0.3866773326679568, 0.2587917267164825]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 3.0, \"batch_instance_id\": \"409bb106-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.14218706541271167, 0.2726759160836421, 0.5851370185036463]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 13.0, \"batch_instance_id\": \"409dabc8-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.15720251854631545, 0.3840752321558323, 0.45872224929785227]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 2.0, \"batch_instance_id\": \"409b7362-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.1808891729172985, 0.32704139903027096, 0.49206942805243054]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 14.0, \"batch_instance_id\": \"409dac86-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.14218974549047703, 0.41059890080264444, 0.4472113537068785]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 15.0, \"batch_instance_id\": \"409de20a-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.2643002677975754, 0.44720843507174224, 0.28849129713068233]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 16.0, \"batch_instance_id\": \"409dfe98-5697-11eb-90de-06ee7f9820ec\"}}}}\n",
      "{\"data\": {\"names\": [\"t:0\", \"t:1\", \"t:2\"], \"ndarray\": [[0.2975075875929912, 0.25439317776178244, 0.44809923464522644]]}, \"meta\": {\"requestPath\": {\"classifier\": \"seldonio/sklearnserver:1.6.0-dev\"}, \"tags\": {\"tags\": {\"batch_id\": \"409a3f56-5697-11eb-be58-06ee7f9820ec\", \"batch_index\": 4.0, \"batch_instance_id\": \"409c3a2c-5697-11eb-90de-06ee7f9820ec\"}}}}\n"
     ]
    }
   ],
   "source": [
    "!head output-data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"sklearn\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow 'sklearn-batch-job' deleted\n"
     ]
    }
   ],
   "source": [
    "!argo delete sklearn-batch-job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
