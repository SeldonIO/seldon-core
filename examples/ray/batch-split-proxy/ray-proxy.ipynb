{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel mini-batch processing with Saldon & Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    " * Kind cluster with Seldon Installed\n",
    " * Docker to build image\n",
    " * kubectl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will explore how one can use Seldon with Ray to split incoming request into smaller mini-batches that can be computer in parallel.\n",
    "\n",
    "In addition to usual `Seldon Deployment` we will create a `Ray Cluster` to which we will connect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Component Connnections](figures/ray-proxy-1.svg \"Component Connnections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is quite simple. User sends a single request that contains main feature vectors for which they want to compute inference. \n",
    "\n",
    "Assumption is that user expects result of the inference computation to be computed as soon as possible and returned in the request's response.\n",
    "\n",
    "To achieve this we split incoming rquest into smaller mini-batches that which can be computed in parallel using pool of Ray Actors.\n",
    "We then combine computed requests and return them to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Component Connnections](figures/ray-proxy-2.svg \"Component Connnections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will perform sentiment analysis using Roberta model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Core\n",
    "\n",
    "Use the setup notebook to [Setup Cluster](../../seldon_core_setup.ipynb) to setup Seldon Core with an ingress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/distributed-roberta created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create namespace distributed-roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"kind-kind\" modified.\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context $(kubectl config current-context) --namespace=distributed-roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Docker Image\n",
    "\n",
    "We start by preparing our model. We assume that model binary will be present at `/microservice/pytorch_model.bin`. \n",
    "\n",
    "The `RobertaModel` class bellow simply loads the binary and serves predictions. This class could be directly use with `Seldon`.\n",
    "\n",
    "To offload computation to `Ray Actors` we prepare a second `ProxyModel` class that:\n",
    "- connects to running Ray Cluster\n",
    "- converts `RobertaModel` into `Ray Actors`\n",
    "- serves prediction by proxing computation to `Ray Actors`\n",
    "\n",
    "Number of actors and mini-batch size can be controlled via environmental variables at the runtime. We will also switch between `RobertaModel` and `ProxyModel` using environmental variables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Model.py\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from seldon_core.utils import getenv_as_bool\n",
    "\n",
    "\n",
    "RAY_PROXY = getenv_as_bool(\"RAY_PROXY\", default=False)\n",
    "MODEL_FILE = \"/microservice/pytorch_model.bin\"\n",
    "\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"100\"))\n",
    "NUM_ACTORS = int(os.environ.get(\"NUM_ACTORS\", \"10\"))\n",
    "\n",
    "\n",
    "class RobertaModel:\n",
    "    def __init__(self, load_on_init=False):\n",
    "        if load_on_init:\n",
    "            self.load()\n",
    "\n",
    "    def load(self):\n",
    "        import torch\n",
    "        from simpletransformers.model import TransformerModel\n",
    "\n",
    "        logging.info(\"starting RobertaModel...\")\n",
    "        model = TransformerModel(\n",
    "            \"roberta\",\n",
    "            \"roberta-base\",\n",
    "            args=({\"fp16\": False, \"use_multiprocessing\": False}),\n",
    "            use_cuda=False,\n",
    "        )\n",
    "        model.model.load_state_dict(torch.load(MODEL_FILE))\n",
    "        self.model = model\n",
    "        logging.info(\"... started RobertaModel\")\n",
    "\n",
    "    def predict(self, data, names=[], meta={}):\n",
    "        logging.info(f\"received inference request: {data}\")\n",
    "        data = data.astype(\"U\")\n",
    "        output = self.model.predict(data)[1].argmax(axis=1)\n",
    "        logging.info(\"finished calculating prediction\")\n",
    "        return output\n",
    "\n",
    "\n",
    "class ProxyModel:\n",
    "    def load(self):\n",
    "        ray.init(address=\"auto\")\n",
    "\n",
    "        self.actors = [\n",
    "            ray.remote(RobertaModel).remote(load_on_init=True)\n",
    "            for _ in range(NUM_ACTORS)\n",
    "        ]\n",
    "\n",
    "        self.pool = ray.util.ActorPool(self.actors)\n",
    "\n",
    "    def predict(self, data, names=[], meta=[]):\n",
    "        logging.info(f\"data received: {data}\")\n",
    "        batches = np.array_split(data, max(data.shape[0] // BATCH_SIZE, 1))\n",
    "        logging.info(f\"spllited into {len(batches)} batches\")\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        results = list(self.pool.map(lambda a, v: a.predict.remote(v), batches))\n",
    "        results = np.concatenate(results).tolist()\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        return {\"time-taken\": t2 - t1, \"results\": results}\n",
    "\n",
    "\n",
    "if not RAY_PROXY:\n",
    "    logging.info(\"Model = RobertaModel\")\n",
    "    Model = RobertaModel\n",
    "else:\n",
    "    logging.info(\"Model = ProxyModel\")\n",
    "    Model = ProxyModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Docker image\n",
    "\n",
    "We will use same image for `Seldon Deployment` and nodes in the `Ray Cluster`. \n",
    "\n",
    "This image is available as `seldonio/distributed-roberta:0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "ray==1.0.0\n",
    "\n",
    "simpletransformers==0.48.5\n",
    "tensorboardX==1.9\n",
    "\n",
    "# newer version seems to be missing wheels\n",
    "sentencepiece==0.1.91\n",
    "\n",
    "transformers==3.2.0\n",
    "torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM seldonio/seldon-core-s2i-python37-ubi8:1.7.0-dev\n",
    "\n",
    "# Add Model\n",
    "COPY training/outputs/pytorch_model.bin /microservice/pytorch_model.bin\n",
    "\n",
    "# Install requirements\n",
    "COPY requirements.txt /microservice\n",
    "RUN pip install -r requirements.txt && rm -r ~/.cache/pip\n",
    "\n",
    "RUN pip install ray[tune]\n",
    "\n",
    "# Add file that will download Roberta cache inside container\n",
    "COPY import_for_cache_download.py /tmp\n",
    "RUN python3 /tmp/import_for_cache_download.py\n",
    "\n",
    "# Add Seldon Model\n",
    "ENV MODEL_NAME Model\n",
    "ENV API_TYPE REST\n",
    "\n",
    "COPY Model.py /microservice/\n",
    "CMD seldon-core-microservice $MODEL_NAME $API_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ray Cluster\n",
    "\n",
    "The setup of Ray Cluster is based on [Ray documentation on advanced k8s usage](https://docs.ray.io/en/releases-1.0.0/cluster/kubernetes.html) (Ray 1.0.0).\n",
    "\n",
    "We will use our `distributed-roberta:0.1` image and set 2 replicas with 5 workers each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy-ray.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy-ray.yaml\n",
    "# Ray head node service, allowing worker pods to discover the head node.\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  namespace: distributed-roberta\n",
    "  name: ray-head\n",
    "spec:\n",
    "  ports:\n",
    "    # Redis ports.\n",
    "    - name: redis-primary\n",
    "      port: 6379\n",
    "      targetPort: 6379\n",
    "    - name: redis-shard-0\n",
    "      port: 6380\n",
    "      targetPort: 6380\n",
    "    - name: redis-shard-1\n",
    "      port: 6381\n",
    "      targetPort: 6381\n",
    "    # Ray internal communication ports.\n",
    "    - name: object-manager\n",
    "      port: 12345\n",
    "      targetPort: 12345\n",
    "    - name: node-manager\n",
    "      port: 12346\n",
    "      targetPort: 12346\n",
    "    - name: http\n",
    "      port: 8000\n",
    "      protocol: TCP\n",
    "      targetPort: 8000\n",
    "  selector:\n",
    "    component: ray-head\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  namespace: distributed-roberta\n",
    "  name: ray-head\n",
    "spec:\n",
    "  # Do not change this - Ray currently only supports one head node per cluster.\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      component: ray-head\n",
    "      type: ray\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        component: ray-head\n",
    "        type: ray\n",
    "    spec:\n",
    "      # If the head node goes down, the entire cluster (including all worker\n",
    "      # nodes) will go down as well. If you want Kubernetes to bring up a new\n",
    "      # head node in this case, set this to \"Always,\" else set it to \"Never.\"\n",
    "      restartPolicy: Always\n",
    "\n",
    "      # This volume allocates shared memory for Ray to use for its plasma\n",
    "      # object store. If you do not provide this, Ray will fall back to\n",
    "      # /tmp which cause slowdowns if is not a shared memory volume.\n",
    "      volumes:\n",
    "      - name: dshm\n",
    "        emptyDir:\n",
    "          medium: Memory\n",
    "      containers:\n",
    "        - name: ray-head\n",
    "          image: seldonio/distributed-roberta:0.1\n",
    "          imagePullPolicy: Always\n",
    "          command: [ \"/bin/bash\", \"-c\", \"--\" ]\n",
    "          args:\n",
    "            - \"ray start --head --node-ip-address=$MY_POD_IP --port=6379 --redis-shard-ports=6380,6381 --num-cpus=$MY_CPU_REQUEST --object-manager-port=12345 --node-manager-port=12346 --block\"\n",
    "          ports:\n",
    "            - containerPort: 6379 # Redis port.\n",
    "            - containerPort: 6380 # Redis port.\n",
    "            - containerPort: 6381 # Redis port.\n",
    "            - containerPort: 12345 # Ray internal communication.\n",
    "            - containerPort: 12346 # Ray internal communication.\n",
    "            - containerPort: 8000\n",
    "              protocol: TCP\n",
    "          # This volume allocates shared memory for Ray to use for its plasma\n",
    "          # object store. If you do not provide this, Ray will fall back to\n",
    "          # /tmp which cause slowdowns if is not a shared memory volume.\n",
    "          volumeMounts:\n",
    "            - mountPath: /dev/shm\n",
    "              name: dshm\n",
    "          env:\n",
    "            - name: MY_POD_IP\n",
    "              valueFrom:\n",
    "                fieldRef:\n",
    "                  fieldPath: status.podIP\n",
    "            # This is used in the ray start command so that Ray can spawn the\n",
    "            # correct number of processes. Omitting this may lead to degraded\n",
    "            # performance.\n",
    "            - name: MY_CPU_REQUEST\n",
    "              valueFrom:\n",
    "                resourceFieldRef:\n",
    "                  resource: requests.cpu\n",
    "          resources:\n",
    "            requests:\n",
    "              cpu: 1\n",
    "              memory: 1024Mi\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  namespace: distributed-roberta\n",
    "  name: ray-worker\n",
    "spec:\n",
    "  # Change this to scale the number of worker nodes started in the Ray cluster.\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      component: ray-worker\n",
    "      type: ray\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        component: ray-worker\n",
    "        type: ray\n",
    "    spec:\n",
    "      restartPolicy: Always\n",
    "      volumes:\n",
    "      - name: dshm\n",
    "        emptyDir:\n",
    "          medium: Memory\n",
    "      containers:\n",
    "      - name: ray-worker\n",
    "        image: seldonio/distributed-roberta:0.1\n",
    "        imagePullPolicy: Always\n",
    "        command: [\"/bin/bash\", \"-c\", \"--\"]\n",
    "        args:\n",
    "          - \"ray start --node-ip-address=$MY_POD_IP --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_SERVICE_HOST:$RAY_HEAD_SERVICE_PORT_REDIS_PRIMARY --object-manager-port=12345 --node-manager-port=12346 --block\"\n",
    "        ports:\n",
    "          - containerPort: 12345 # Ray internal communication.\n",
    "          - containerPort: 12346 # Ray internal communication.\n",
    "        volumeMounts:\n",
    "          - mountPath: /dev/shm\n",
    "            name: dshm\n",
    "        env:\n",
    "          - name: MY_POD_IP\n",
    "            valueFrom:\n",
    "              fieldRef:\n",
    "                fieldPath: status.podIP\n",
    "          # This is used in the ray start command so that Ray can spawn the\n",
    "          # correct number of processes. Omitting this may lead to degraded\n",
    "          # performance.\n",
    "          - name: MY_CPU_REQUEST\n",
    "            valueFrom:\n",
    "              resourceFieldRef:\n",
    "                resource: requests.cpu\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 5\n",
    "            memory: 512Mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Deployment\n",
    "\n",
    "We will create two Seldon Deployments in this example. One standard and one with proxy to Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy-seldon.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy-seldon.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-model-ray-proxy\n",
    "  namespace: distributed-roberta\n",
    "spec:\n",
    "  name: mock-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        volumes:\n",
    "        - name: dshm\n",
    "          emptyDir:\n",
    "            medium: Memory\n",
    "        containers:\n",
    "        - name: model\n",
    "          image: seldonio/distributed-roberta:0.1\n",
    "          imagePullPolicy: Always\n",
    "          securityContext:\n",
    "            runAsUser: 0\n",
    "          command: [ \"/bin/bash\", \"-c\", \"--\" ]\n",
    "          args:\n",
    "            - \"ray start --node-ip-address=$MY_POD_IP --num-cpus=0 --address=$RAY_HEAD_SERVICE_HOST:$RAY_HEAD_SERVICE_PORT_REDIS_PRIMARY --object-manager-port=12345 --node-manager-port=12346 &&\n",
    "              seldon-core-microservice $MODEL_NAME $API_TYPE\"\n",
    "          ports:\n",
    "            - containerPort: 12345 # Ray internal communication.\n",
    "            - containerPort: 12346 # Ray internal communication.\n",
    "          volumeMounts:\n",
    "            - mountPath: /dev/shm\n",
    "              name: dshm\n",
    "          env:\n",
    "          - name: MY_POD_IP\n",
    "            valueFrom:\n",
    "              fieldRef:\n",
    "                fieldPath: status.podIP\n",
    "          - name: SELDON_LOG_LEVEL\n",
    "            value: DEBUG\n",
    "          - name: GUNICORN_THREADS\n",
    "            value: \"1\"\n",
    "          - name: RAY_PROXY\n",
    "            value: \"true\"\n",
    "          - name: BATCH_SIZE\n",
    "            value: \"50\"\n",
    "          - name: NUM_ACTORS\n",
    "            value: \"10\"\n",
    "    graph:\n",
    "      name: model\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-model\n",
    "  namespace: distributed-roberta\n",
    "spec:\n",
    "  name: mock-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - name: model\n",
    "          image: seldonio/distributed-roberta:0.1\n",
    "          imagePullPolicy: Always\n",
    "          securityContext:\n",
    "            runAsUser: 0\n",
    "          env:\n",
    "          - name: SELDON_LOG_LEVEL\n",
    "            value: DEBUG\n",
    "          - name: GUNICORN_THREADS\n",
    "            value: \"1\"\n",
    "          - name: RAY_PROXY\n",
    "            value: \"false\"\n",
    "    graph:\n",
    "      name: model\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/ray-head created\n",
      "deployment.apps/ray-head created\n",
      "deployment.apps/ray-worker created\n",
      "seldondeployment.machinelearning.seldon.io/seldon-model-ray-proxy created\n",
      "seldondeployment.machinelearning.seldon.io/seldon-model created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl apply -f deploy-ray.yaml\n",
    "kubectl apply -f deploy-seldon.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment \"seldon-model-default-0-model\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-model -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment \"seldon-model-ray-proxy-default-0-model\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-model-ray-proxy -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.datasets import fetch_movie_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "data = movies.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests (only Seldon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.77 ms, sys: 97 µs, total: 8.87 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_namespace = \"distributed-roberta\"\n",
    "model_name = \"seldon-model\"\n",
    "\n",
    "endpoint = (\n",
    "    f\"http://localhost:8003/seldon/{model_namespace}/{model_name}/api/v1.0/predictions\"\n",
    ")\n",
    "\n",
    "r = requests.post(url=endpoint, json={\"data\": {\"ndarray\": payload}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = r.json()[\"data\"][\"ndarray\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests (proxy to Ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 ms, sys: 725 µs, total: 3.78 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_namespace = \"distributed-roberta\"\n",
    "model_name = \"seldon-model-ray-proxy\"\n",
    "\n",
    "endpoint = (\n",
    "    f\"http://localhost:8003/seldon/{model_namespace}/{model_name}/api/v1.0/predictions\"\n",
    ")\n",
    "\n",
    "r = requests.post(url=endpoint, json={\"data\": {\"ndarray\": payload}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = r.json()[\"jsonData\"][\"results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, both methods yields the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 == d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as expected, using 10 workers give, give of take, 10x speedup in computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service \"ray-head\" deleted\n",
      "deployment.apps \"ray-head\" deleted\n",
      "deployment.apps \"ray-worker\" deleted\n",
      "seldondeployment.machinelearning.seldon.io \"seldon-model-ray-proxy\" deleted\n",
      "seldondeployment.machinelearning.seldon.io \"seldon-model\" deleted\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl delete -f deploy-ray.yaml\n",
    "kubectl delete -f deploy-seldon.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
