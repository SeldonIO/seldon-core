{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's Triton pre-packed server. the example also covers converting the model to ONNX format.\n",
    "The implemented example below is of the Greedy approach for the next token prediction.\n",
    "more info: https://huggingface.co/transformers/model_doc/gpt2.html?highlight=gpt2\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance.\n",
    "\n",
    "\n",
    "## Steps:\n",
    "1. Download pretrained GPT2 model from hugging face\n",
    "2. Convert the model to ONNX\n",
    "3. Store it in MinIo bucket\n",
    "4. Setup Seldon-Core in your kubernetes cluster\n",
    "5. Deploy the ONNX model with Seldonâ€™s prepackaged Triton server.\n",
    "6. Interact with the model, run a greedy alg example (generate sentence completion)\n",
    "7. Run load test using vegeta\n",
    "8. Clean-up\n",
    "\n",
    "## Basic requirements\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.5.1\n",
    "torch==1.8.1\n",
    "tokenizers<0.11,>=0.10.1\n",
    "tensorflow==2.4.1\n",
    "tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-diesel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-evaluation",
   "metadata": {},
   "source": [
    "### Export HuggingFace TFGPT2LMHeadModel pre-trained model and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id)\n",
    "model.save_pretrained(\"./tfgpt2model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-tribute",
   "metadata": {},
   "source": [
    "### Convert the TensorFlow saved model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./tfgpt2model/saved_model/1 --opset 11  --output model.onnx"
   ]
  },
  {
   "source": [
    "## Azure Setup\n",
    "We  have provided [Azure Setup Notebook](AzureSetup.ipynb) that deploys AKS cluster, Azure storage account and installs Azure Blob CSI driver. If AKS cluster already exists skip to creation of Blob Storage and CSI driver installtion steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"seldon\"   # feel free to replace or use this default\n",
    "aks_name = \"modeltests\"    \n",
    "\n",
    "storage_account_name = \"modeltestsgpt\"        # fill in\n",
    "storage_container_name = \"gpt2onnx\"             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-pantyhose",
   "metadata": {},
   "source": [
    "### Copy your model to Azure Blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Copy model file\n",
    "!az extension add --name storage-preview\n",
    "!az storage azcopy blob upload --container {storage_container_name} \\\n",
    "                               --account-name {storage_account_name} \\\n",
    "                               --source  ./model.onnx \\\n",
    "                               --destination gpt2/1/model.onnx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mThis command has been deprecated and will be removed in future release. Use 'az storage fs file list' instead. For more information go to https://github.com/Azure/azure-cli/blob/dev/src/azure-cli/azure/cli/command_modules/storage/docs/ADLS%20Gen2.md\u001b[39m\n",
      "\u001b[33mThe behavior of this command has been altered by the following extension: storage-preview\u001b[0m\n",
      "Name               IsDirectory    Blob Type    Blob Tier    Length     Content Type              Last Modified              Snapshot\n",
      "-----------------  -------------  -----------  -----------  ---------  ------------------------  -------------------------  ----------\n",
      "gpt2/1/model.onnx                 BlockBlob    Hot          652535462  application/octet-stream  2021-05-28T04:37:11+00:00\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Verify Uploaded file\n",
    "!az storage blob list \\\n",
    "    --account-name {storage_account_name}\\\n",
    "    --container-name {storage_container_name} \\\n",
    "    --output table \n",
    "    "
   ]
  },
  {
   "source": [
    "##  Add Azure PersistentVolume and Claim \n",
    "For more details on creating PersistentVolume using CSI driver refer to https://github.com/kubernetes-sigs/blob-csi-driver/blob/master/deploy/example/e2e_usage.md\n",
    " - Create secret\n",
    " - Create PersistentVolume pointing to secret and Blob Container Name and `mountOptions` specifying user id for non-root containers \n",
    " - Creare PersistentVolumeClaim to bind to volume"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = !az storage account keys list --account-name {storage_account_name} -g {resource_group} --query '[0].value' -o tsv\n",
    "storage_account_key = key[0] "
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create secret to access storage account\n",
    "!kubectl create secret generic azure-blobsecret --from-literal azurestorageaccountname={storage_account_name} --from-literal azurestorageaccountkey=\"{storage_account_key}\" --type=Opaque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile azure-blobfuse-pv.yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv-gpt2blob\n",
    "  \n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Retain  # \"Delete\" is not supported in static provisioning\n",
    "  csi:\n",
    "    driver: blob.csi.azure.com\n",
    "    readOnly: false\n",
    "    volumeHandle: trainingdata  # make sure this volumeid is unique in the cluster\n",
    "    volumeAttributes:\n",
    "      containerName: gpt2onnx # Modify if changed in Notebook\n",
    "    nodeStageSecretRef:\n",
    "      name: azure-blobsecret\n",
    "      namespace: default\n",
    "  mountOptions:\n",
    "    - -o uid=8888  \n",
    "    - -o allow_other\n",
    "---\n",
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: pvc-gpt2blob\n",
    " \n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "  volumeName: pv-gpt2blob\n",
    "  storageClassName: \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "persistentvolume/pv-gptblob configured\n",
      "persistentvolumeclaim/pvc-gptblob unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f  azure-blobfuse-pv.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE\npersistentvolume/pv-gpt2blob   10Gi       RWX            Retain           Bound    default/pvc-gpt2blob                           4h54m\n\nNAME                                 STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/pvc-gpt2blob   Bound    pv-gpt2blob   10Gi       RWX                           4h54m\n"
     ]
    }
   ],
   "source": [
    "# Verify PVC is bound\n",
    "!kubectl get pv,pvc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-syracuse",
   "metadata": {},
   "source": [
    "### Run Seldon in your kubernetes cluster\n",
    "\n",
    "Follow the [Seldon-Core Setup notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html) to Setup a cluster with Ambassador Ingress or Istio and install Seldon Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-outreach",
   "metadata": {},
   "source": [
    "### Deploy your model with Seldon pre-packaged Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beneficial-anime",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting gpt2-deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpt2-deploy.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: gpt2\n",
    "spec:\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - name: gpt2\n",
    "          resources:\n",
    "            requests:\n",
    "              memory: 750Mi\n",
    "              cpu: 2\n",
    "              #nvidia.com/gpu: 1    \n",
    "            limits:\n",
    "              memory: 2Gi\n",
    "              cpu: 2\n",
    "              #nvidia.com/gpu: 1    \n",
    "    graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: pvc://pvc-gpt2blob/\n",
    "      name: gpt2\n",
    "      type: MODEL      \n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subjective-involvement",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "seldondeployment.machinelearning.seldon.io/gpt2 created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kubectl apply -f gpt2-deploy.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demanding-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Waiting for deployment \"gpt2-default-0-gpt2\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "error: deployment \"gpt2-default-0-gpt2\" exceeded its progress deadline\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=gpt2 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-supervisor",
   "metadata": {},
   "source": [
    "#### Interact with the model: get model metadata (a \"test\" request to make sure our model is available and loaded correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "married-roller",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*   Trying 20.186.162.33:80...\n",
      "* TCP_NODELAY set\n",
      "* Connected to 20.186.162.33 (20.186.162.33) port 80 (#0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* Mark bundle as not supporting multiuse\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* Connection #0 to host 20.186.162.33 left intact\n",
      "{\"name\":\"gpt2\",\"versions\":[\"1\"],\"platform\":\"onnxruntime_onnx\",\"inputs\":[{\"name\":\"input_ids:0\",\"datatype\":\"INT32\",\"shape\":[-1,-1]},{\"name\":\"attention_mask:0\",\"datatype\":\"INT32\",\"shape\":[-1,-1]}],\"outputs\":[{\"name\":\"past_key_values\",\"datatype\":\"FP32\",\"shape\":[12,2,-1,12,-1,64]},{\"name\":\"logits\",\"datatype\":\"FP32\",\"shape\":[-1,-1,50257]}]}"
     ]
    }
   ],
   "source": [
    "ingress_ip=!(kubectl get svc --namespace seldon-system ambassador -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
    "ingress_ip = ingress_ip[0]\n",
    "\n",
    "!curl -v http://{ingress_ip}:80/seldon/default/gpt2/v2/models/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-resource",
   "metadata": {},
   "source": [
    "### Run prediction test: generate a sentence completion using GPT2 model  - Greedy approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-termination",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence .\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence . I\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence . I love\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence . I love the\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence . I love the way\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n",
      "Sentence: I love Artificial Intelligence . I love the way it\n",
      "sending request to http://20.186.162.33/seldon/default/gpt2/v2/models/gpt2/infer\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 16: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# This includes IncompleteRead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connection broken: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3878af7ebc1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'sending request to {tfserving_url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfserving_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = 'I love Artificial Intelligence'\n",
    "count = 0\n",
    "max_gen_len = 20\n",
    "gen_sentence = input_text\n",
    "while count < max_gen_len:\n",
    "    input_ids = tokenizer.encode(gen_sentence, return_tensors='tf')\n",
    "    shape = input_ids.shape.as_list()\n",
    "    payload = {\n",
    "            \"inputs\": [\n",
    "                {\"name\": \"input_ids:0\",\n",
    "                 \"datatype\": \"INT32\",\n",
    "                 \"shape\": shape,\n",
    "                 \"data\": input_ids.numpy().tolist()\n",
    "                 },\n",
    "                {\"name\": \"attention_mask:0\",\n",
    "                 \"datatype\": \"INT32\",\n",
    "                 \"shape\": shape,\n",
    "                 \"data\": np.ones(shape, dtype=np.int32).tolist()\n",
    "                 }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    tfserving_url = \"http://\" + str(ingress_ip) + \"/seldon/default/gpt2/v2/models/gpt2/infer\"\n",
    "    print(f'sending request to {tfserving_url}')\n",
    "\n",
    "    ret = requests.post(tfserving_url, json=payload)\n",
    "\n",
    "    try:\n",
    "        res = ret.json()\n",
    "    except:\n",
    "       continue\n",
    "\n",
    "    # extract logits\n",
    "    logits = np.array(res[\"outputs\"][1][\"data\"])\n",
    "    logits = logits.reshape(res[\"outputs\"][1][\"shape\"])\n",
    "\n",
    "    # take the best next token probability of the last token of input ( greedy approach)\n",
    "    next_token = logits.argmax(axis=2)[0]\n",
    "    next_token_str = tokenizer.decode(next_token[-1:], skip_special_tokens=True,\n",
    "                                      clean_up_tokenization_spaces=True).strip()\n",
    "    gen_sentence += ' ' + next_token_str\n",
    "    print (f'Sentence: {gen_sentence}')\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print(f'Input: {input_text}\\nOutput: {gen_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-status",
   "metadata": {},
   "source": [
    "### Run Load Test / Performance Test using vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-discovery",
   "metadata": {},
   "source": [
    "#### Install vegeta, for more details take a look in [vegeta](https://github.com/tsenart/vegeta#install) official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/tsenart/vegeta/releases/download/v12.8.3/vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!tar -zxvf vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!chmod +x vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lying",
   "metadata": {},
   "source": [
    "#### Generate vegeta [target file](https://github.com/tsenart/vegeta#-targets) contains \"post\" cmd with payload in the requiered structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import run, Popen, PIPE\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import base64\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = 'I enjoy working in Seldon'\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "shape = input_ids.shape.as_list()\n",
    "payload = {\n",
    "\t\t\"inputs\": [\n",
    "\t\t\t{\"name\": \"input_ids:0\",\n",
    "\t\t\t \"datatype\": \"INT32\",\n",
    "\t\t\t \"shape\": shape,\n",
    "\t\t\t \"data\": input_ids.numpy().tolist()\n",
    "\t\t\t },\n",
    "\t\t\t{\"name\": \"attention_mask:0\",\n",
    "\t\t\t \"datatype\": \"INT32\",\n",
    "\t\t\t \"shape\": shape,\n",
    "\t\t\t \"data\": np.ones(shape, dtype=np.int32).tolist()\n",
    "\t\t\t }\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\n",
    "cmd= {\"method\": \"POST\",\n",
    "\t\t\"header\": {\"Content-Type\": [\"application/json\"] },\n",
    "\t\t\"url\": \"http://localhost:80/seldon/default/gpt2/v2/models/gpt2/infer\",\n",
    "\t\t\"body\": base64.b64encode(bytes(json.dumps(payload), \"utf-8\")).decode(\"utf-8\")}\n",
    "\n",
    "with open(\"vegeta_target.json\", mode=\"w\") as file:\n",
    "\tjson.dump(cmd, file)\n",
    "\tfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vegeta attack -targets=vegeta_target.json -rate=1 -duration=60s -format=json | vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-suite",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f gpt2-deploy.yaml -n default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}