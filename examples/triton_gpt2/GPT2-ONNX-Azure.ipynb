{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's Triton pre-packed server. the example also covers converting the model to ONNX format.\n",
    "The implemented example below is of the Greedy approach for the next token prediction.\n",
    "more info: https://huggingface.co/transformers/model_doc/gpt2.html?highlight=gpt2\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance.\n",
    "\n",
    "\n",
    "## Steps:\n",
    "1. Download pretrained GPT2 model from hugging face\n",
    "2. Convert the model to ONNX\n",
    "3. Store it in MinIo bucket\n",
    "4. Setup Seldon-Core in your kubernetes cluster\n",
    "5. Deploy the ONNX model with Seldonâ€™s prepackaged Triton server.\n",
    "6. Interact with the model, run a greedy alg example (generate sentence completion)\n",
    "7. Run load test using vegeta\n",
    "8. Clean-up\n",
    "\n",
    "## Basic requirements\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.5.1\n",
    "torch==1.8.1\n",
    "tokenizers<0.11,>=0.10.1\n",
    "tensorflow==2.4.1\n",
    "tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-diesel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-evaluation",
   "metadata": {},
   "source": [
    "### Export HuggingFace TFGPT2LMHeadModel pre-trained model and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id)\n",
    "model.save_pretrained(\"./tfgpt2model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-tribute",
   "metadata": {},
   "source": [
    "### Convert the TensorFlow saved model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./tfgpt2model/saved_model/1 --opset 11  --output model.onnx"
   ]
  },
  {
   "source": [
    "## Azure Setup\n",
    "We  have provided [Azure Setup Notebook](AzureSetup.ipynb) that deploys AKS cluster, Azure storage account and installs Azure Blob CSI driver. If AKS cluster already exists skip to creation of Blob Storage and CSI driver installtion steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"seldon\"   # feel free to replace or use this default\n",
    "aks_name = \"modeltests\"    \n",
    "\n",
    "storage_account_name = \"modeltestsgpt\"        # fill in\n",
    "storage_container_name = \"gpt2onnx\"             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-pantyhose",
   "metadata": {},
   "source": [
    "### Copy your model to Azure Blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Copy model file\n",
    "!az extension add --name storage-preview\n",
    "!az storage azcopy blob upload --container {storage_container_name} \\\n",
    "                               --account-name {storage_account_name} \\\n",
    "                               --source  ./model.onnx \\\n",
    "                               --destination gpt2/1/model.onnx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mThis command has been deprecated and will be removed in future release. Use 'az storage fs file list' instead. For more information go to https://github.com/Azure/azure-cli/blob/dev/src/azure-cli/azure/cli/command_modules/storage/docs/ADLS%20Gen2.md\u001b[39m\n",
      "\u001b[33mThe behavior of this command has been altered by the following extension: storage-preview\u001b[0m\n",
      "Name               IsDirectory    Blob Type    Blob Tier    Length     Content Type              Last Modified              Snapshot\n",
      "-----------------  -------------  -----------  -----------  ---------  ------------------------  -------------------------  ----------\n",
      "gpt2/1/model.onnx                 BlockBlob    Hot          652535462  application/octet-stream  2021-05-28T04:37:11+00:00\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Verify Uploaded file\n",
    "!az storage blob list \\\n",
    "    --account-name {storage_account_name}\\\n",
    "    --container-name {storage_container_name} \\\n",
    "    --output table \n",
    "    "
   ]
  },
  {
   "source": [
    "##  Add Azure PersistentVolume and Claim "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = !az storage account keys list --account-name {storage_account_name} -g {resource_group} --query '[0].value' -o tsv\n",
    "storage_account_key = key[0] "
   ]
  },
  {
   "source": [
    "For more details on creating PersistentVolume using CSI driver refer to https://github.com/kubernetes-sigs/blob-csi-driver/blob/master/deploy/example/e2e_usage.md"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create secret to access storage account\n",
    "!kubectl create secret generic azure-blobsecret --from-literal azurestorageaccountname={storage_account_name} --from-literal azurestorageaccountkey=\"{storage_account_key}\" --type=Opaque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile azure-blobfuse-pv.yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv-gptblob\n",
    "  \n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Retain  # \"Delete\" is not supported in static provisioning\n",
    "  csi:\n",
    "    driver: blob.csi.azure.com\n",
    "    readOnly: false\n",
    "    volumeHandle: trainingdata  # make sure this volumeid is unique in the cluster\n",
    "    volumeAttributes:\n",
    "      containerName: gpt2onnx # Modify if changed in Notebook\n",
    "    nodeStageSecretRef:\n",
    "      name: azure-blobsecret\n",
    "      namespace: default\n",
    "    \n",
    "---\n",
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: pvc-gptblob\n",
    " \n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "  volumeName: pv-gptblob\n",
    "  storageClassName: \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f  azure-blobfuse-pv.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                 STORAGECLASS   REASON   AGE\npersistentvolume/pv-blob      10Gi       RWX            Retain           Terminating   default/pvc-blob                              127m\npersistentvolume/pv-gptblob   10Gi       RWX            Retain           Bound         default/pvc-gptblob                           15m\n\nNAME                                STATUS        VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/pvc-blob      Terminating   pv-blob      10Gi       RWX                           127m\npersistentvolumeclaim/pvc-gptblob   Bound         pv-gptblob   10Gi       RWX                           15m\n"
     ]
    }
   ],
   "source": [
    "# Verify PVC is bound\n",
    "!kubectl get pv,pvc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-syracuse",
   "metadata": {},
   "source": [
    "### Run Seldon in your kubernetes cluster\n",
    "\n",
    "Follow the [Seldon-Core Setup notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html) to Setup a cluster with Ambassador Ingress or Istio and install Seldon Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-outreach",
   "metadata": {},
   "source": [
    "### Deploy your model with Seldon pre-packaged Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beneficial-anime",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting gpt2-deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpt2-deploy.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: gpt2\n",
    "spec:\n",
    "  predictors:\n",
    "  - graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: pvc://pvc-gptblob\n",
    "      name: gpt2\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subjective-involvement",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "seldondeployment.machinelearning.seldon.io/gpt2 created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kubectl apply -f gpt2-deploy.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demanding-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Waiting for deployment \"gpt2-default-0-gpt2\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "error: deployment \"gpt2-default-0-gpt2\" exceeded its progress deadline\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=gpt2 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-supervisor",
   "metadata": {},
   "source": [
    "#### Interact with the model: get model metadata (a \"test\" request to make sure our model is available and loaded correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingress_ip=!(kubectl get svc --namespace seldon-system ambassador -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
    "ingress_ip = ingress_ip[0]\n",
    "\n",
    "!curl -v http://{ingress_ip}:80/seldon/default/gpt2/v2/models/gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-resource",
   "metadata": {},
   "source": [
    "### Run prediction test: generate a sentence completion using GPT2 model  - Greedy approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = 'I enjoy working in Seldon'\n",
    "count = 0\n",
    "max_gen_len = 10\n",
    "gen_sentence = input_text\n",
    "while count < max_gen_len:\n",
    "    input_ids = tokenizer.encode(gen_sentence, return_tensors='tf')\n",
    "    shape = input_ids.shape.as_list()\n",
    "    payload = {\n",
    "            \"inputs\": [\n",
    "                {\"name\": \"input_ids:0\",\n",
    "                 \"datatype\": \"INT32\",\n",
    "                 \"shape\": shape,\n",
    "                 \"data\": input_ids.numpy().tolist()\n",
    "                 },\n",
    "                {\"name\": \"attention_mask:0\",\n",
    "                 \"datatype\": \"INT32\",\n",
    "                 \"shape\": shape,\n",
    "                 \"data\": np.ones(shape, dtype=np.int32).tolist()\n",
    "                 }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    ret = requests.post('http://localhost:80/seldon/default/gpt2/v2/models/gpt2/infer', json=payload)\n",
    "\n",
    "    try:\n",
    "        res = ret.json()\n",
    "    except:\n",
    "       continue\n",
    "\n",
    "    # extract logits\n",
    "    logits = np.array(res[\"outputs\"][1][\"data\"])\n",
    "    logits = logits.reshape(res[\"outputs\"][1][\"shape\"])\n",
    "\n",
    "    # take the best next token probability of the last token of input ( greedy approach)\n",
    "    next_token = logits.argmax(axis=2)[0]\n",
    "    next_token_str = tokenizer.decode(next_token[-1:], skip_special_tokens=True,\n",
    "                                      clean_up_tokenization_spaces=True).strip()\n",
    "    gen_sentence += ' ' + next_token_str\n",
    "    count += 1\n",
    "\n",
    "print(f'Input: {input_text}\\nOutput: {gen_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-status",
   "metadata": {},
   "source": [
    "### Run Load Test / Performance Test using vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-discovery",
   "metadata": {},
   "source": [
    "#### Install vegeta, for more details take a look in [vegeta](https://github.com/tsenart/vegeta#install) official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/tsenart/vegeta/releases/download/v12.8.3/vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!tar -zxvf vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!chmod +x vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lying",
   "metadata": {},
   "source": [
    "#### Generate vegeta [target file](https://github.com/tsenart/vegeta#-targets) contains \"post\" cmd with payload in the requiered structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import run, Popen, PIPE\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import base64\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = 'I enjoy working in Seldon'\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "shape = input_ids.shape.as_list()\n",
    "payload = {\n",
    "\t\t\"inputs\": [\n",
    "\t\t\t{\"name\": \"input_ids:0\",\n",
    "\t\t\t \"datatype\": \"INT32\",\n",
    "\t\t\t \"shape\": shape,\n",
    "\t\t\t \"data\": input_ids.numpy().tolist()\n",
    "\t\t\t },\n",
    "\t\t\t{\"name\": \"attention_mask:0\",\n",
    "\t\t\t \"datatype\": \"INT32\",\n",
    "\t\t\t \"shape\": shape,\n",
    "\t\t\t \"data\": np.ones(shape, dtype=np.int32).tolist()\n",
    "\t\t\t }\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\n",
    "cmd= {\"method\": \"POST\",\n",
    "\t\t\"header\": {\"Content-Type\": [\"application/json\"] },\n",
    "\t\t\"url\": \"http://localhost:80/seldon/default/gpt2/v2/models/gpt2/infer\",\n",
    "\t\t\"body\": base64.b64encode(bytes(json.dumps(payload), \"utf-8\")).decode(\"utf-8\")}\n",
    "\n",
    "with open(\"vegeta_target.json\", mode=\"w\") as file:\n",
    "\tjson.dump(cmd, file)\n",
    "\tfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vegeta attack -targets=vegeta_target.json -rate=1 -duration=60s -format=json | vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-suite",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f gpt2-deploy.yaml -n default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}