{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit classification\n",
    "\n",
    "We are wrapping a logistic regression model that receives the text output from a tfidf model that stores 1000 features.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "### Training\n",
    "* Spacy\n",
    "* Sklearn\n",
    "* Seldon-core\n",
    "\n",
    "### Seldon\n",
    "* Sklearn\n",
    "* Dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy>=0.13.3 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.20.3)\n",
      "Collecting spacy==2.0.18 (from -r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/70/65504a011d7b262e73cfe470c36ca245a1f8e45b3b6661081289ffd72009/spacy-2.0.18-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: dill==0.2.9 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.2.9)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from scikit-learn>=0.18->-r requirements.txt (line 2)) (1.16.3)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (1.35)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (2.0.1)\n",
      "Collecting regex==2018.01.10 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (0.9.6)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (6.12.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (2.21.0)\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (0.4.3.2)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (4.31.1)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (0.9.0)\n",
      "Installing collected packages: regex, spacy, msgpack\n",
      "  Found existing installation: regex 2019.4.14\n",
      "    Uninstalling regex-2019.4.14:\n",
      "      Successfully uninstalled regex-2019.4.14\n",
      "  Found existing installation: spacy 2.0.16\n",
      "    Uninstalling spacy-2.0.16:\n",
      "      Successfully uninstalled spacy-2.0.16\n",
      "  Found existing installation: msgpack 0.6.1\n",
      "    Uninstalling msgpack-0.6.1:\n",
      "      Successfully uninstalled msgpack-0.6.1\n",
      "Successfully installed msgpack-0.5.6 regex-2018.1.10 spacy-2.0.18\n"
     ]
    }
   ],
   "source": [
    "# Let's first install any dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install seldon-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/alejandro/miniconda3/envs/reddit-classification/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "import dill\n",
    "import sys, os\n",
    "\n",
    "# This import may take a while as it will download the Spacy ENGLISH model\n",
    "from ml_utils import CleanTextTransformer, SpacyTokenTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_idx</th>\n",
       "      <th>parent_idx</th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8756</td>\n",
       "      <td>8877</td>\n",
       "      <td>Always be wary of news articles that cite unpu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7330</td>\n",
       "      <td>7432</td>\n",
       "      <td>The problem I have with this is that the artic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15711</td>\n",
       "      <td>15944</td>\n",
       "      <td>This is indicative of a typical power law, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1604</td>\n",
       "      <td>1625</td>\n",
       "      <td>This doesn't make sense. Chess obviously trans...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13327</td>\n",
       "      <td>13520</td>\n",
       "      <td>1. I dispute that gene engineering is burdenso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prev_idx  parent_idx                                               body  \\\n",
       "0      8756        8877  Always be wary of news articles that cite unpu...   \n",
       "1      7330        7432  The problem I have with this is that the artic...   \n",
       "2     15711       15944  This is indicative of a typical power law, and...   \n",
       "3      1604        1625  This doesn't make sense. Chess obviously trans...   \n",
       "4     13327       13520  1. I dispute that gene engineering is burdenso...   \n",
       "\n",
       "   removed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = [\"prev_idx\", \"parent_idx\", \"body\", \"removed\"]\n",
    "\n",
    "TEXT_COLUMN = \"body\" \n",
    "CLEAN_COLUMN = \"clean_body\"\n",
    "TOKEN_COLUMN = \"token_body\"\n",
    "\n",
    "# Downloading the 50k reddit dataset of moderated comments\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv\", \n",
    "                         names=df_cols, skiprows=1, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce0fd58d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many examples we have of each class\n",
    "df[\"removed\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"body\"].values\n",
    "y = df[\"removed\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, \n",
    "    stratify=y, \n",
    "    random_state=42, \n",
    "    test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "clean_text_transformer = CleanTextTransformer()\n",
    "x_train_clean = clean_text_transformer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and get the lemmas\n",
    "spacy_tokenizer = SpacyTokenTransformer()\n",
    "x_train_tokenized = spacy_tokenizer.transform(x_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2',\n",
       "        preprocessor=<function <lambda> at 0x7fce08047378>,\n",
       "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern=None,\n",
       "        tokenizer=<function <lambda> at 0x7fce080472f0>, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    preprocessor=lambda x: x, \n",
    "    tokenizer=lambda x: x, \n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vectorizer.fit(x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our tokens to tfidf vectors\n",
    "x_train_tfidf = tfidf_vectorizer.transform(\n",
    "    x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier\n",
    "lr = LogisticRegression(C=0.1, solver='sag')\n",
    "lr.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the models we'll deploy\n",
    "with open('tfidf_vectorizer.model', 'wb') as model_file:\n",
    "    dill.dump(tfidf_vectorizer, model_file)\n",
    "with open('lr.model', 'wb') as model_file:\n",
    "    dill.dump(lr, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dill\r\n",
      "\r\n",
      "from ml_utils import CleanTextTransformer, SpacyTokenTransformer\r\n",
      "\r\n",
      "class RedditClassifier(object):\r\n",
      "    def __init__(self):\r\n",
      "        \r\n",
      "        self._clean_text_transformer = CleanTextTransformer()\r\n",
      "        self._spacy_tokenizer = SpacyTokenTransformer()\r\n",
      "        \r\n",
      "        with open('tfidf_vectorizer.model', 'rb') as model_file:\r\n",
      "            self._tfidf_vectorizer = dill.load(model_file)\r\n",
      "           \r\n",
      "        with open('lr.model', 'rb') as model_file:\r\n",
      "            self._lr_model = dill.load(model_file)\r\n",
      "\r\n",
      "    def predict(self, X, feature_names):\r\n",
      "        clean_text = self._clean_text_transformer.transform(X)\r\n",
      "        spacy_tokens = self._spacy_tokenizer.transform(clean_text)\r\n",
      "        tfidf_features = self._tfidf_vectorizer.transform(spacy_tokens)\r\n",
      "        predictions = self._lr_model.predict_proba(tfidf_features)\r\n",
      "        return predictions\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# This is the class we will use to deploy\n",
    "!cat RedditClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf']\n",
      "[[0.82767056 0.17232944]]\n"
     ]
    }
   ],
   "source": [
    "# test that our model works\n",
    "from RedditClassifier import RedditClassifier\n",
    "# With one sample\n",
    "sample = x_test[0:1]\n",
    "print(sample)\n",
    "print(RedditClassifier().predict(sample, [\"feature_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Docker Image\n",
    "Using the S2I command line interface we wrap our current model to seve it through the Seldon interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME=RedditClassifier\r\n",
      "API_TYPE=REST\r\n",
      "SERVICE_TYPE=MODEL\r\n",
      "PERSISTENCE=0\r\n"
     ]
    }
   ],
   "source": [
    "# To create a docker image we need to create the .s2i folder configuration as below:\n",
    "!cat .s2i/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy>= 0.13.3\r\n",
      "scikit-learn>=0.18\r\n",
      "spacy==2.0.18\r\n",
      "dill==0.2.9\r\n"
     ]
    }
   ],
   "source": [
    "# As well as a requirements.txt file with all the relevant dependencies\n",
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "Collecting scipy>=0.13.3 (from -r requirements.txt (line 1))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Collecting scikit-learn>=0.18 (from -r requirements.txt (line 2))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/90/c7/401c231c445fb6fad135e92197da9c3e77983de169ff1887cc18af94498d/scikit_learn-0.21.1-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
      "Collecting spacy==2.0.18 (from -r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/ae/6e/a89da6b5c83f8811e46e3a9270c1aed90e9b9ee6c60faf52b7239e5d3d69/spacy-2.0.18-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Collecting dill==0.2.9 (from -r requirements.txt (line 4))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from scipy>=0.13.3->-r requirements.txt (line 1)) (1.16.3)\n",
      "Collecting joblib>=0.11 (from scikit-learn>=0.18->-r requirements.txt (line 2))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
      "Collecting ujson>=1.35 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "Collecting regex==2018.01.10 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/76/f4/7146c3812f96fcaaf2d06ff6862582302626a59011ccb6f2833bb38d80f7/regex-2018.01.10.tar.gz (612kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/site-packages (from spacy==2.0.18->-r requirements.txt (line 3)) (2.21.0)\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting thinc<6.13.0,>=6.12.1 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/db/a7/46640a46fd707aeb204aa4257a70974b6a22a0204ba703164d803215776f/thinc-6.12.1-cp36-cp36m-manylinux1_x86_64.whl (1.9MB)\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/3d/61/9b0520c28eb199a4b1ca667d96dd625bba003c14c75230195f9691975f85/cymem-2.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->-r requirements.txt (line 3)) (2.8)\n",
      "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl (49kB)\n",
      "Collecting msgpack-numpy<0.4.4 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3)) (1.12.0)\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18->-r requirements.txt (line 3))\n",
      "  WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Downloading https://files.pythonhosted.org/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz (45kB)\n",
      "Building wheels for collected packages: dill, ujson, regex, wrapt, cytoolz, toolz\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "Building wheel for ujson (setup.py): started\n",
      "Building wheel for ujson (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
      "Building wheel for regex (setup.py): started\n",
      "Building wheel for regex (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/74/17/3f/c77bba99efd74ba1a19862c9dd97f4b6d735e2826721dc00ff\n",
      "Building wheel for wrapt (setup.py): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
      "Building wheel for cytoolz (setup.py): started\n",
      "Building wheel for cytoolz (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
      "Building wheel for toolz (setup.py): started\n",
      "Building wheel for toolz (setup.py): finished with status 'done'\n",
      "Stored in directory: /root/.cache/pip/wheels/f4/0c/f6/ce6b2d1aa459ee97cc3c0f82236302bd62d89c86c700219463\n",
      "Successfully built dill ujson regex wrapt cytoolz toolz\n",
      "Installing collected packages: scipy, joblib, scikit-learn, cymem, preshed, ujson, regex, dill, plac, wrapt, tqdm, msgpack, msgpack-numpy, murmurhash, toolz, cytoolz, thinc, spacy\n",
      "Successfully installed cymem-2.0.2 cytoolz-0.9.0.1 dill-0.2.9 joblib-0.13.2 msgpack-0.5.6 msgpack-numpy-0.4.3.2 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 regex-2018.1.10 scikit-learn-0.21.1 scipy-1.3.0 spacy-2.0.18 thinc-6.12.1 toolz-0.9.0 tqdm-4.32.1 ujson-1.35 wrapt-1.10.11\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "Build completed successfully\n"
     ]
    }
   ],
   "source": [
    "!s2i build . seldonio/seldon-core-s2i-python3:0.6 reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_predictor\r\n"
     ]
    }
   ],
   "source": [
    "# Remove previously deployed containers for this model\n",
    "!docker rm -f reddit_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812a02932fb6d61eb90727f99df051a67720a14de59504bbd808f1159fcda65e\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --name \"reddit_predictor\" -d --rm -p 5001:5000 reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21T05:18:10.089008100Z starting microservice\n",
      "2019-05-21T05:18:11.567254000Z 2019-05-21 05:18:11,566 - seldon_core.microservice:main:154 - INFO:  Starting microservice.py:main\n",
      "2019-05-21T05:18:11.568744400Z 2019-05-21 05:18:11,568 - seldon_core.microservice:main:185 - INFO:  Annotations: {}\n",
      "2019-05-21T05:18:12.629109700Z Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "2019-05-21T05:18:13.675815500Z   Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "2019-05-21T05:18:22.124982300Z Building wheels for collected packages: en-core-web-sm\n",
      "2019-05-21T05:18:22.125493500Z   Building wheel for en-core-web-sm (setup.py): started\n",
      "2019-05-21T05:18:24.199642200Z   Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "2019-05-21T05:18:24.199964600Z   Stored in directory: /tmp/pip-ephem-wheel-cache-d7p16zav/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
      "2019-05-21T05:18:24.513275700Z Successfully built en-core-web-sm\n",
      "2019-05-21T05:18:24.513295200Z Installing collected packages: en-core-web-sm\n",
      "2019-05-21T05:18:24.579380500Z Successfully installed en-core-web-sm-2.0.0\n",
      "2019-05-21T05:18:24.893582100Z WARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "2019-05-21T05:18:24.893610900Z You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "2019-05-21T05:18:26.083684000Z --- Logging error ---\n",
      "2019-05-21T05:18:26.083710200Z Traceback (most recent call last):\n",
      "2019-05-21T05:18:26.083714300Z   File \"/usr/local/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
      "2019-05-21T05:18:26.083718000Z     msg = self.format(record)\n",
      "2019-05-21T05:18:26.083721100Z   File \"/usr/local/lib/python3.6/logging/__init__.py\", line 840, in format\n",
      "2019-05-21T05:18:26.083724100Z     return fmt.format(record)\n",
      "2019-05-21T05:18:26.083726900Z   File \"/usr/local/lib/python3.6/logging/__init__.py\", line 577, in format\n",
      "2019-05-21T05:18:26.083729900Z     record.message = record.getMessage()\n",
      "2019-05-21T05:18:26.083732600Z   File \"/usr/local/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "2019-05-21T05:18:26.083735600Z     msg = msg % self.args\n",
      "2019-05-21T05:18:26.083738400Z TypeError: not all arguments converted during string formatting\n",
      "2019-05-21T05:18:26.083741300Z Call stack:\n",
      "2019-05-21T05:18:26.083744100Z   File \"/usr/local/bin/seldon-core-microservice\", line 10, in <module>\n",
      "2019-05-21T05:18:26.083747200Z     sys.exit(main())\n",
      "2019-05-21T05:18:26.083749900Z   File \"/usr/local/lib/python3.6/site-packages/seldon_core/microservice.py\", line 189, in main\n",
      "2019-05-21T05:18:26.083752800Z     logger.info(\"Importing \",args.interface_name)\n",
      "2019-05-21T05:18:26.083755600Z Message: 'Importing '\n",
      "2019-05-21T05:18:26.083758300Z Arguments: ('RedditClassifier',)\n",
      "2019-05-21T05:18:26.083774500Z /usr/local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "2019-05-21T05:18:26.083778100Z   UserWarning)\n",
      "2019-05-21T05:18:26.083781000Z /usr/local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "2019-05-21T05:18:26.083784100Z   UserWarning)\n",
      "2019-05-21T05:18:26.083786800Z /usr/local/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "2019-05-21T05:18:26.083789900Z   UserWarning)\n",
      "2019-05-21T05:18:26.083792600Z 2019-05-21 05:18:26,082 - seldon_core.microservice:main:226 - INFO:  REST microservice running on port 5000\n",
      "2019-05-21T05:18:26.083795500Z 2019-05-21 05:18:26,083 - seldon_core.microservice:main:260 - INFO:  Starting servers\n",
      "2019-05-21T05:18:26.084624400Z \n",
      "2019-05-21T05:18:26.084636400Z \u001b[93m    Linking successful\u001b[0m\n",
      "2019-05-21T05:18:26.084639800Z     /usr/local/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "2019-05-21T05:18:26.084642700Z     /usr/local/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "2019-05-21T05:18:26.084645500Z \n",
      "2019-05-21T05:18:26.084648200Z     You can now load the model via spacy.load('en_core_web_sm')\n",
      "2019-05-21T05:18:26.084651000Z \n",
      "2019-05-21T05:18:26.100898500Z  * Serving Flask app \"seldon_core.wrapper\" (lazy loading)\n",
      "2019-05-21T05:18:26.100924900Z  * Environment: production\n",
      "2019-05-21T05:18:26.100930600Z    WARNING: Do not use the development server in a production environment.\n",
      "2019-05-21T05:18:26.100936000Z    Use a production WSGI server instead.\n",
      "2019-05-21T05:18:26.100941400Z  * Debug mode: off\n",
      "2019-05-21T05:18:26.111993300Z 2019-05-21 05:18:26,111 - werkzeug:_log:122 - INFO:   * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Here we need to wait until we see \"Linking successful\", as it's downloading the Spacy English model\n",
    "# You can hit stop when this happens\n",
    "!docker logs -t -f reddit_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  names: \"tfidf\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "}\n",
      "data {\n",
      "  names: \"t:0\"\n",
      "  names: \"t:1\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      list_value {\n",
      "        values {\n",
      "          number_value: 0.8276705633548711\n",
      "        }\n",
      "        values {\n",
      "          number_value: 0.17232943664512895\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now test the REST endpoint expecting the same result\n",
    "endpoint = \"0.0.0.0:5000\"\n",
    "batch = sample\n",
    "payload_type = \"ndarray\"\n",
    "\n",
    "sc = SeldonClient(microservice_endpoint=endpoint)\n",
    "response = sc.microservice(\n",
    "    data=batch,\n",
    "    method=\"predict\",\n",
    "    payload_type=payload_type,\n",
    "    names=[\"tfidf\"])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_predictor\r\n"
     ]
    }
   ],
   "source": [
    "# We now stop it to run it in docker\n",
    "!docker stop reddit_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run in kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                  READY   STATUS    RESTARTS   AGE\r\n",
      "seldon-core-redis-master-0                            1/1     Running   0          59s\r\n",
      "seldon-core-seldon-apiserver-d8fd4cd68-5mzzt          1/1     Running   0          59s\r\n",
      "seldon-core-seldon-cluster-manager-67c64974b6-d8cfb   1/1     Running   0          59s\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that seldon is running \n",
    "!kubectl get pods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not running you can install it\n",
    "# First initialise helm\n",
    "!kubectl create clusterrolebinding kube-system-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default\n",
    "!helm init\n",
    "!kubectl rollout status deploy/tiller-deploy -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then install and run seldon\n",
    "helm install seldon-core-crd --name seldon-core-crd --repo https://storage.googleapis.com/seldon-charts\n",
    "helm install seldon-core --name seldon-core --repo https://storage.googleapis.com/seldon-charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\r\n",
      "    \"kind\": \"SeldonDeployment\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"labels\": {\r\n",
      "            \"app\": \"seldon\"\r\n",
      "        },\r\n",
      "        \"name\": \"reddit-classifier\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"annotations\": {\r\n",
      "            \"project_name\": \"Reddit classifier\",\r\n",
      "            \"deployment_version\": \"v1\"\r\n",
      "        },\r\n",
      "        \"name\": \"reddit-classifier\",\r\n",
      "        \"oauth_key\": \"oauth-key\",\r\n",
      "        \"oauth_secret\": \"oauth-secret\",\r\n",
      "        \"predictors\": [\r\n",
      "            {\r\n",
      "                \"componentSpecs\": [{\r\n",
      "                    \"spec\": {\r\n",
      "                        \"containers\": [\r\n",
      "                            {\r\n",
      "                                \"image\": \"reddit-classifier:0.1\",\r\n",
      "                                \"imagePullPolicy\": \"IfNotPresent\",\r\n",
      "                                \"name\": \"classifier\",\r\n",
      "                                \"resources\": {\r\n",
      "                                    \"requests\": {\r\n",
      "                                        \"memory\": \"1Mi\"\r\n",
      "                                    }\r\n",
      "                                }\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"terminationGracePeriodSeconds\": 20\r\n",
      "                    }\r\n",
      "                }],\r\n",
      "                \"graph\": {\r\n",
      "                    \"children\": [],\r\n",
      "                    \"name\": \"classifier\",\r\n",
      "                    \"endpoint\": {\r\n",
      "            \"type\" : \"REST\"\r\n",
      "            },\r\n",
      "                    \"type\": \"MODEL\"\r\n",
      "                },\r\n",
      "                \"name\": \"single-model\",\r\n",
      "                \"replicas\": 1,\r\n",
      "        \"annotations\": {\r\n",
      "            \"predictor_version\" : \"v1\"\r\n",
      "        }\r\n",
      "            }\r\n",
      "        ]\r\n",
      "    }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# We'll use our seldon deployment file\n",
    "!cat reddit_clf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/reddit-classifier created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f reddit_clf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE\r\n",
      "reddit-classifier-single-model-9199e4b-5c49c85c7d-mjvrh   2/2     Running   0          62s\r\n",
      "seldon-core-redis-master-0                                1/1     Running   0          2m14s\r\n",
      "seldon-core-seldon-apiserver-d8fd4cd68-5mzzt              1/1     Running   0          2m14s\r\n",
      "seldon-core-seldon-cluster-manager-67c64974b6-d8cfb       1/1     Running   0          2m14s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31203\r\n"
     ]
    }
   ],
   "source": [
    "!echo `kubectl get svc -l app=seldon-apiserver-container-app -o jsonpath='{.items[0].spec.ports[0].nodePort}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKS!!!!\n",
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  names: \"text\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "  puid: \"9359cpdudo3b6i6cv82ktsl4pe\"\n",
      "  requestPath {\n",
      "    key: \"classifier\"\n",
      "    value: \"reddit-classifier:0.1\"\n",
      "  }\n",
      "}\n",
      "data {\n",
      "  names: \"t:0\"\n",
      "  names: \"t:1\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      list_value {\n",
      "        values {\n",
      "          number_value: 0.8276705633548711\n",
      "        }\n",
      "        values {\n",
      "          number_value: 0.17232943664512895\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "host = \"localhost\"\n",
    "port = \"31203\" # Make sure you use the port above\n",
    "batch = sample\n",
    "payload_type = \"ndarray\"\n",
    "\n",
    "sc = SeldonClient(gateway=\"seldon\", seldon_rest_endpoint=host + \":\" + port,\n",
    "                          oauth_key=\"oauth-key\", oauth_secret=\"oauth-secret\")\n",
    "response = sc.predict(\n",
    "    data=batch, \n",
    "    deployment_name=\"mymodel\",\n",
    "    names=[\"text\"],\n",
    "    payload_type=payload_type)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
