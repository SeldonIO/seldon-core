{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Spacy Reddit Text Classification Example\n",
    "\n",
    "In this example we will be buiding a text classifier using the reddit content moderation dataset.\n",
    "\n",
    "For this, we will be using SpaCy for the word tokenization and lemmatization. \n",
    "\n",
    "The classification will be done with a Logistic Regression binary classifier.\n",
    "\n",
    "The steps in this tutorial include:\n",
    "\n",
    "1) Train and build your NLP model\n",
    "\n",
    "2) Build your containerized model\n",
    "\n",
    "3) Test your model as a docker container\n",
    "\n",
    "4) Run Seldon in your kubernetes cluster\n",
    "\n",
    "5) Deploy your model with Seldon\n",
    "\n",
    "6) Interact with your model through API\n",
    "\n",
    "7) Clean your environment\n",
    "\n",
    "\n",
    "### Before you start\n",
    "Make sure you install the following dependencies, as they are critical for this example to work:\n",
    "\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+\n",
    "* Python DEV requirements (we'll install them below)\n",
    "\n",
    "Let's get started! ðŸš€ðŸ”¥\n",
    "\n",
    "## 1) Train and build your NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "scikit-learn>=0.23.2\n",
    "spacy==2.3.2\n",
    "dill==0.3.2\n",
    "pandas==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "import dill\n",
    "import sys, os\n",
    "\n",
    "# This import may take a while as it will download the Spacy ENGLISH model\n",
    "from ml_utils import CleanTextTransformer, SpacyTokenTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_idx</th>\n",
       "      <th>parent_idx</th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8756</td>\n",
       "      <td>8877</td>\n",
       "      <td>Always be wary of news articles that cite unpu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7330</td>\n",
       "      <td>7432</td>\n",
       "      <td>The problem I have with this is that the artic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15711</td>\n",
       "      <td>15944</td>\n",
       "      <td>This is indicative of a typical power law, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1604</td>\n",
       "      <td>1625</td>\n",
       "      <td>This doesn't make sense. Chess obviously trans...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13327</td>\n",
       "      <td>13520</td>\n",
       "      <td>1. I dispute that gene engineering is burdenso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prev_idx  parent_idx                                               body  \\\n",
       "0      8756        8877  Always be wary of news articles that cite unpu...   \n",
       "1      7330        7432  The problem I have with this is that the artic...   \n",
       "2     15711       15944  This is indicative of a typical power law, and...   \n",
       "3      1604        1625  This doesn't make sense. Chess obviously trans...   \n",
       "4     13327       13520  1. I dispute that gene engineering is burdenso...   \n",
       "\n",
       "   removed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = [\"prev_idx\", \"parent_idx\", \"body\", \"removed\"]\n",
    "\n",
    "TEXT_COLUMN = \"body\" \n",
    "CLEAN_COLUMN = \"clean_body\"\n",
    "TOKEN_COLUMN = \"token_body\"\n",
    "\n",
    "# Downloading the 50k reddit dataset of moderated comments\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv\", \n",
    "                         names=df_cols, skiprows=1, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9db626d1d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many examples we have of each class\n",
    "df[\"removed\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"body\"].values\n",
    "y = df[\"removed\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, \n",
    "    stratify=y, \n",
    "    random_state=42, \n",
    "    test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "clean_text_transformer = CleanTextTransformer()\n",
    "x_train_clean = clean_text_transformer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and get the lemmas\n",
    "spacy_tokenizer = SpacyTokenTransformer()\n",
    "x_train_tokenized = spacy_tokenizer.transform(x_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n",
       "                preprocessor=<function <lambda> at 0x7f9db31f58c0>,\n",
       "                token_pattern=None,\n",
       "                tokenizer=<function <lambda> at 0x7f9db31f5830>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    preprocessor=lambda x: x, \n",
    "    tokenizer=lambda x: x, \n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vectorizer.fit(x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our tokens to tfidf vectors\n",
    "x_train_tfidf = tfidf_vectorizer.transform(x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, solver='sag')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier\n",
    "lr = LogisticRegression(C=0.1, solver='sag')\n",
    "lr.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the models we'll deploy\n",
    "with open('tfidf_vectorizer.model', 'wb') as model_file:\n",
    "    dill.dump(tfidf_vectorizer, model_file)\n",
    "with open('lr.model', 'wb') as model_file:\n",
    "    dill.dump(lr, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build your containerized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RedditClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RedditClassifier.py\n",
    "import dill\n",
    "\n",
    "from ml_utils import CleanTextTransformer, SpacyTokenTransformer\n",
    "\n",
    "\n",
    "class RedditClassifier(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self._clean_text_transformer = CleanTextTransformer()\n",
    "        self._spacy_tokenizer = SpacyTokenTransformer()\n",
    "\n",
    "        with open(\"tfidf_vectorizer.model\", \"rb\") as model_file:\n",
    "            self._tfidf_vectorizer = dill.load(model_file)\n",
    "\n",
    "        with open(\"lr.model\", \"rb\") as model_file:\n",
    "            self._lr_model = dill.load(model_file)\n",
    "\n",
    "    def predict(self, X, feature_names):\n",
    "        clean_text = self._clean_text_transformer.transform(X)\n",
    "        spacy_tokens = self._spacy_tokenizer.transform(clean_text)\n",
    "        tfidf_features = self._tfidf_vectorizer.transform(spacy_tokens)\n",
    "        predictions = self._lr_model.predict_proba(tfidf_features)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf']\n",
      "[[0.82791777 0.17208223]]\n"
     ]
    }
   ],
   "source": [
    "# test that our model works\n",
    "from RedditClassifier import RedditClassifier\n",
    "# With one sample\n",
    "sample = x_test[0:1]\n",
    "print(sample)\n",
    "print(RedditClassifier().predict(sample, [\"feature_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker Image with the S2i utility\n",
    "Using the S2I command line interface we wrap our current model to seve it through the Seldon interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME=RedditClassifier\r\n",
      "API_TYPE=REST\r\n",
      "SERVICE_TYPE=MODEL\r\n",
      "PERSISTENCE=0\r\n"
     ]
    }
   ],
   "source": [
    "# To create a docker image we need to create the .s2i folder configuration as below:\n",
    "!cat .s2i/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn>=0.23.2\r\n",
      "spacy==2.3.2\r\n",
      "dill==0.3.2\r\n",
      "pandas==1.1.1\r\n"
     ]
    }
   ],
   "source": [
    "# As well as a requirements.txt file with all the relevant dependencies\n",
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: the following arguments are required: filename\n"
     ]
    }
   ],
   "source": [
    "%%writefile\n",
    "FROM seldonio/seldon-core-s2i-python3:1.4.0
    "\n",
    "RUN pip install spacy\n",
    "RUN python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker build . -t seldonio/seldon-core-spacy-base:0.1\n",
    "s2i build . seldonio/seldon-core-spacy-base:0.1 seldonio/reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test your model as a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No such container: reddit_predictor\n"
     ]
    }
   ],
   "source": [
    "# Remove previously deployed containers for this model\n",
    "!docker rm -f reddit_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2743a0561c99be7371dbbbb6c87a036c8fe22690bcba6da03d8041c1011546cc\n"
     ]
    }
   ],
   "source": [
    "!docker run --name \"reddit_predictor\" -d --rm -p 5001:5000 seldonio/reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  names: \"tfidf\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "}\n",
      "data {\n",
      "  names: \"t:0\"\n",
      "  names: \"t:1\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      list_value {\n",
      "        values {\n",
      "          number_value: 0.8285423647440985\n",
      "        }\n",
      "        values {\n",
      "          number_value: 0.17145763525590152\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now test the REST endpoint expecting the same result\n",
    "endpoint = \"0.0.0.0:5001\"\n",
    "batch = sample\n",
    "payload_type = \"ndarray\"\n",
    "\n",
    "sc = SeldonClient(microservice_endpoint=endpoint)\n",
    "response = sc.microservice(\n",
    "    data=batch,\n",
    "    method=\"predict\",\n",
    "    payload_type=payload_type,\n",
    "    names=[\"tfidf\"])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_predictor\n"
     ]
    }
   ],
   "source": [
    "# We now stop it to run it in docker\n",
    "!docker stop reddit_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run Seldon in your kubernetes cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Core\n",
    "\n",
    "Use the setup notebook to [Setup Cluster](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Setup-Cluster) with [Ambassador Ingress](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Ambassador) and [Install Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Install-Seldon-Core). Instructions [also online](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Deploy your model with Seldon\n",
    "We can now deploy our model by using the Seldon graph definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reddit_clf.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile reddit_clf.json\n",
    "{\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"labels\": {\n",
    "            \"app\": \"seldon\"\n",
    "        },\n",
    "        \"name\": \"reddit-classifier\"\n",
    "    },\n",
    "    \"spec\": {\n",
    "        \"annotations\": {\n",
    "            \"project_name\": \"Reddit classifier\",\n",
    "            \"deployment_version\": \"v1\"\n",
    "        },\n",
    "        \"name\": \"reddit-classifier\",\n",
    "        \"oauth_key\": \"oauth-key\",\n",
    "        \"oauth_secret\": \"oauth-secret\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [{\n",
    "                    \"spec\": {\n",
    "                        \"containers\": [\n",
    "                            {\n",
    "                                \"image\": \"seldonio/reddit-classifier:0.1\",\n",
    "                                \"imagePullPolicy\": \"IfNotPresent\",\n",
    "                                \"name\": \"classifier\",\n",
    "                                \"resources\": {\n",
    "                                    \"requests\": {\n",
    "                                        \"memory\": \"1Mi\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                        \"terminationGracePeriodSeconds\": 20\n",
    "                    }\n",
    "                }],\n",
    "                \"graph\": {\n",
    "                    \"children\": [],\n",
    "                    \"name\": \"classifier\",\n",
    "                    \"endpoint\": {\n",
    "            \"type\" : \"REST\"\n",
    "            },\n",
    "                    \"type\": \"MODEL\"\n",
    "                },\n",
    "                \"name\": \"single-model\",\n",
    "                \"replicas\": 1,\n",
    "        \"annotations\": {\n",
    "            \"predictor_version\" : \"v1\"\n",
    "        }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are using kind preload image first with\n",
    "```bash\n",
    "kind load docker-image reddit-classifier:0.1 --name <name of your cluster>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/reddit-classifier created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f reddit_clf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
      "reddit-classifier-single-model-0-classifier-6fb8dbfd87-w8stj   2/2     Running   0          27s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Interact with your model through API\n",
    "Now that our Seldon Deployment is live, we are able to interact with it through its API.\n",
    "\n",
    "There are two options in which we can interact with our new model. These are:\n",
    "\n",
    "a) Using CURL from the CLI (or another rest client like Postman)\n",
    "\n",
    "b) Using the Python SeldonClient\n",
    "\n",
    "#### a) Using CURL from the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6821638979867455,0.3178361020132546]]},\"meta\":{}}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -H 'Content-Type: application/json' \\\n",
    "    -d '{\"data\": {\"names\": [\"text\"], \"ndarray\": [\"Hello world this is a test\"]}}' \\\n",
    "    http://localhost:8003/seldon/seldon/reddit-classifier/api/v1.0/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Using the Python SeldonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "meta {\n",
      "}\n",
      "data {\n",
      "  names: \"text\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"Hello world this is a test\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "{'data': {'names': ['t:0', 't:1'], 'ndarray': [[0.6821638979867455, 0.3178361020132546]]}, 'meta': {}}\n"
     ]
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    transport=\"rest\",\n",
    "    gateway_endpoint=\"localhost:8003\",   # Make sure you use the port above\n",
    "    namespace=\"seldon\"\n",
    ")\n",
    "\n",
    "client_prediction = sc.predict(\n",
    "    data=np.array([\"Hello world this is a test\"]), \n",
    "    deployment_name=\"reddit-classifier\",\n",
    "    names=[\"text\"],\n",
    "    payload_type=\"ndarray\",\n",
    ")\n",
    "\n",
    "print(client_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Clean your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"reddit-classifier\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f reddit_clf.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
