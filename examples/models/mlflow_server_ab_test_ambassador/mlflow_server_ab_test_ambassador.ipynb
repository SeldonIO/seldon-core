{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Pre-packaged Model Server AB Test Deployment \n",
    "In this example we will build two models with MLFlow and we will deploy them as an A/B test deployment. The reason this is powerful is because it allows you to deploy a new model next to the old one, distributing a percentage of traffic. These deployment strategies are quite simple using Seldon, and can be extended to shadow deployments, multi-armed-bandits, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "\n",
    "This tutorial will closely break down in the following sections:\n",
    "\n",
    "1. Dataset and problem\n",
    "\n",
    "2. Train a elastic net model using MLflow\n",
    "\n",
    "3. Deploy your trained model leveraging our pre-packaged MLFlow model server\n",
    "\n",
    "4. Test the deployed MLFlow model by sending requests\n",
    "\n",
    "5. Train a Deep Learning model using MLflow\n",
    "\n",
    "6. Visualise and monitor the performance of your models using Seldon Analytics\n",
    "\n",
    "It will follow closely our talk at the [Spark + AI Summit 2019 on Seldon and MLflow](https://www.youtube.com/watch?v=D6eSfd9w9eA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "For this example to work you must be running Seldon 0.3.2 or above - you can follow our [getting started guide for this](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html).\n",
    "\n",
    "In regards to other dependencies, make sure you have installed:\n",
    "\n",
    "* Helm v3.0.0+\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+\n",
    "* MLflow 1.6.0\n",
    "* [pygmentize](https://pygments.org/docs/cmdline/)\n",
    "* [tree](http://mama.indstate.edu/users/ice/tree/)\n",
    "\n",
    "We will also take this chance to load the Python dependencies we will use through the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seldon_core.seldon_client import SeldonClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get started!** ðŸš€ðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and problem\n",
    "\n",
    "For our example, we will use the elastic net wine example from [MLflow's tutorial](https://www.mlflow.org/docs/latest/tutorial.html).\n",
    "Our goal will be to develop a model which predicts the quality of a wine based on a set of features (e.g. acidity, residual sugar, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"wine-quality.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a SparkML model using MLflow\n",
    "\n",
    "For our first model, we will we will train a simple Linear Regression model using SparkML.\n",
    "On our A/B test we will denote this model as Model A.\n",
    "\n",
    "We will start by looking at the model's environment definition and the training script which will be used by MLflow. Afterwards, we'll kick-off training and we will look at the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLproject\n",
    "\n",
    "As any other MLflow project, it is defined by its `MLproject` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mname\u001b[39;49;00m: model-a\n",
      "\n",
      "\u001b[94mconda_env\u001b[39;49;00m: conda.yaml\n",
      "\n",
      "\u001b[94mentry_points\u001b[39;49;00m:\n",
      "  \u001b[94mmain\u001b[39;49;00m:\n",
      "    \u001b[94mparameters\u001b[39;49;00m:\n",
      "      \u001b[94malpha\u001b[39;49;00m: float\n",
      "      \u001b[94ml1_ratio\u001b[39;49;00m: {\u001b[94m type\u001b[39;49;00m: \u001b[31mfloat\u001b[39;49;00m,\u001b[94m default\u001b[39;49;00m: \u001b[31m0.1\u001b[39;49;00m }\n",
      "    \u001b[94mcommand\u001b[39;49;00m: spark-submit train.py --alpha {alpha} --l1_ratio {l1_ratio}\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l yaml model-a/MLproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this project uses Conda for the environment and that it's defined in the `conda.yaml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mname\u001b[39;49;00m: model-a\n",
      "\u001b[94mchannels\u001b[39;49;00m:\n",
      "  - defaults\n",
      "\u001b[94mdependencies\u001b[39;49;00m:\n",
      "  - python=3.6\n",
      "  - pyspark=2.4.0\n",
      "  - scikit-learn=0.22.1\n",
      "  - openjdk=8.0.152\n",
      "  - \u001b[94mpip\u001b[39;49;00m:\n",
      "      - mlflow==1.6.0\n",
      "      - pyyaml==5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model-a/conda.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also see that the training will be performed by the `train.py` file, which receives two parameters `--alpha` and `--l1_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmlflow\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36myaml\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmlflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m spark \u001b[34mas\u001b[39;49;00m mlflow_spark\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkContext\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mregression\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearRegression\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VectorAssembler\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m mean_squared_error, mean_absolute_error, r2_score\n",
      "\n",
      "\n",
      "parser = ArgumentParser()\n",
      "parser.add_argument(\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33m-a\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33m--alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    dest=\u001b[33m\"\u001b[39;49;00m\u001b[33malpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "    default=\u001b[34m0.5\u001b[39;49;00m,\n",
      "    help=\u001b[33m\"\u001b[39;49;00m\u001b[33mAlpha coefficient.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      ")\n",
      "parser.add_argument(\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33m-l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33m--l1_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    dest=\u001b[33m\"\u001b[39;49;00m\u001b[33ml1_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "    default=\u001b[34m0.2\u001b[39;49;00m,\n",
      "    help=\u001b[33m\"\u001b[39;49;00m\u001b[33mL1 regularizer ratio.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      ")\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32meval_metrics\u001b[39;49;00m(actual, pred):\n",
      "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
      "    mae = mean_absolute_error(actual, pred)\n",
      "    r2 = r2_score(actual, pred)\n",
      "    \u001b[34mreturn\u001b[39;49;00m rmse, mae, r2\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mread_data\u001b[39;49;00m(spark):\n",
      "    data = spark.read.option(\u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).csv(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m../wine-quality.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, inferSchema=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "    pdf = data.toPandas()\n",
      "\n",
      "    \u001b[37m# We normalize the inputs to both the SparkML & TensorFlow models\u001b[39;49;00m\n",
      "    \u001b[37m# so that they have the same input schema.\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m col \u001b[35min\u001b[39;49;00m pdf.columns[:-\u001b[34m1\u001b[39;49;00m]:\n",
      "        pdf[col] = (pdf[col] - pdf[col].mean()) / pdf[col].std()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m data\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_conda_env\u001b[39;49;00m():\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mconda.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m conda_file:\n",
      "        \u001b[34mreturn\u001b[39;49;00m yaml.safe_load(conda_file)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(alpha, l1_ratio):\n",
      "    \u001b[37m# Split data into training and test datasets.\u001b[39;49;00m\n",
      "    spark = SparkSession.builder.getOrCreate()\n",
      "    data = read_data(spark)\n",
      "    (training, test) = data.randomSplit([\u001b[34m0.8\u001b[39;49;00m, \u001b[34m0.2\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[37m# Assemble feature columns into a vector (excluding the \"quality\" label).\u001b[39;49;00m\n",
      "    assembler = VectorAssembler(inputCols=data.columns[\u001b[34m0\u001b[39;49;00m:-\u001b[34m1\u001b[39;49;00m], outputCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mfeatures\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Create elastic net regressor based on alpha  & l1 ratio hyperparameters.\u001b[39;49;00m\n",
      "    lr = LinearRegression(\n",
      "        maxIter=\u001b[34m10\u001b[39;49;00m, regParam=alpha, elasticNetParam=l1_ratio, labelCol=\u001b[33m\"\u001b[39;49;00m\u001b[33mquality\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Create SparkML pipeline, which we can save as one combined model.\u001b[39;49;00m\n",
      "    pipeline = Pipeline(stages=[assembler, lr])\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m mlflow.start_run(run_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mspark-a\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m-l\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (alpha, l1_ratio)):\n",
      "        mlflow.log_param(\u001b[33m\"\u001b[39;49;00m\u001b[33malpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, alpha)\n",
      "        mlflow.log_param(\u001b[33m\"\u001b[39;49;00m\u001b[33ml1_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, l1_ratio)\n",
      "\n",
      "        \u001b[37m# Train and save the model.\u001b[39;49;00m\n",
      "        lrModel = pipeline.fit(training)\n",
      "        conda_env = get_conda_env()\n",
      "        mlflow_spark.log_model(lrModel, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conda_env=conda_env)\n",
      "\n",
      "        \u001b[37m# Evaluate the model on the test set.\u001b[39;49;00m\n",
      "        predictions = lrModel.transform(test)\n",
      "        predict_df = predictions.toPandas()\n",
      "        (rmse, mae, r2) = eval_metrics(predict_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mquality\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], predict_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "        mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mrmse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, rmse)\n",
      "        mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mr2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, r2)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parser.parse_args()\n",
      "    sc = SparkContext()\n",
      "\n",
      "    train(args.alpha, args.l1_ratio)\n",
      "\n",
      "    sc.stop()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model-a/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We've set up our MLflow project and our dataset is ready, so we are now good to start training.\n",
    "MLflow allows us to train our model with the following command:\n",
    "\n",
    "``` bash\n",
    "$ mlflow run . -P alpha=... -P l1_ratio=...\n",
    "```\n",
    "\n",
    "On each run, `mlflow` will set up the Conda environment defined by the `conda.yaml` file and will run the training commands defined in the `MLproject` file.\n",
    "\n",
    "\n",
    "> Note that, since this is a `SparkML` model **you will need a Spark cluster** available in order to train it.\n",
    "As an alternative, you can just assume someone else has trained it for you and has stored the output on the S3 model registry.\n",
    "In this case, just skip to the [next section](#Deploy-your-model-using-the-Pre-packaged-model-server-for-mlflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/02/21 11:45:23 INFO mlflow.projects: === Creating conda environment mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30 ===\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.8.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "setuptools-45.2.0    | 520 KB    | ##################################### | 100% \n",
      "pandas-1.0.1         | 8.6 MB    | ##################################### | 100% \n",
      "openssl-1.1.1d       | 2.5 MB    | ##################################### | 100% \n",
      "openjdk-8.0.152      | 57.9 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Ran pip subprocess with arguments:\n",
      "['/home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/bin/python', '-m', 'pip', 'install', '-U', '-r', '/home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Processing /home/agm/.cache/pip/wheels/52/17/45/1370dd1b969ca154f4cd7db298a76d17b2a817426c8c9bf1e5/mlflow-1.6.0-py3-none-any.whl\n",
      "Processing /home/agm/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030/PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting click>=7.0\n",
      "  Using cached Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (1.18.1)\n",
      "Collecting sqlparse\n",
      "  Using cached sqlparse-0.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Using cached protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (1.14.0)\n",
      "Processing /home/agm/.cache/pip/wheels/45/6e/cc/3691602eeca883fa3db046605c675989117c9dbfa13ed46f34/prometheus_flask_exporter-0.12.2-py3-none-any.whl\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-4.2.0-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (1.0.1)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.0.8-py3-none-any.whl (450 kB)\n",
      "Processing /home/agm/.cache/pip/wheels/54/88/28/d771a55dfb3c62af8b3358c60f8034edcb5c9a57d44a9024cf/alembic-1.4.0-py2.py3-none-any.whl\n",
      "Processing /home/agm/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097/simplejson-3.17.0-cp36-none-any.whl\n",
      "Processing /home/agm/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e/querystring_parser-1.2.4-cp36-none-any.whl\n",
      "Collecting requests>=2.17.3\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting gorilla\n",
      "  Using cached gorilla-0.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Processing /home/agm/.cache/pip/wheels/28/3e/f9/8eca04781258bb6956ffba37e4e6e6951e5b3a16d4494b91cb/SQLAlchemy-1.3.13-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting gunicorn; platform_system != \"Windows\"\n",
      "  Using cached gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "Processing /home/agm/.cache/pip/wheels/bc/27/58/c6ef96e649962e9584a50f58d5b6abafb71a03512b2e381ad1/databricks_cli-0.9.1-cp36-none-any.whl\n",
      "Collecting Flask\n",
      "  Using cached Flask-1.1.1-py2.py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (2.8.1)\n",
      "Collecting entrypoints\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from protobuf>=3.6.0->mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (45.2.0.post20200210)\n",
      "Processing /home/agm/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea/prometheus_client-0.7.1-cp36-none-any.whl\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from pandas->mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (2019.3)\n",
      "Collecting gitdb2>=3\n",
      "  Downloading gitdb2-3.0.2-py2.py3-none-any.whl (63 kB)\n",
      "Processing /home/agm/.cache/pip/wheels/43/b1/7c/f14ef20f4683e5087ae684c6447194e09695315f20b9c45575/Mako-1.1.1-py3-none-any.whl\n",
      "Collecting python-editor>=0.3\n",
      "  Using cached python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/agm/.conda/envs/mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30/lib/python3.6/site-packages (from requests>=2.17.3->mlflow==1.6.0->-r /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/condaenv.wfpj6zhi.requirements.txt (line 1)) (2019.11.28)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting configparser>=0.3.5\n",
      "  Using cached configparser-4.0.2-py2.py3-none-any.whl (22 kB)\n",
      "Processing /home/agm/.cache/pip/wheels/9c/9b/f4/eb243fdb89676ec00588e8c54bb54360724c06e7fafe95278e/tabulate-0.8.6-cp36-none-any.whl\n",
      "Collecting itsdangerous>=0.24\n",
      "  Using cached itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Jinja2>=2.10.1\n",
      "  Using cached Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Using cached Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)\n",
      "Collecting smmap2>=2.0.0\n",
      "  Using cached smmap2-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Using cached MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Installing collected packages: click, sqlparse, protobuf, itsdangerous, MarkupSafe, Jinja2, Werkzeug, Flask, prometheus-client, prometheus-flask-exporter, websocket-client, urllib3, idna, chardet, requests, docker, smmap2, gitdb2, gitpython, Mako, python-editor, sqlalchemy, alembic, simplejson, querystring-parser, gorilla, pyyaml, cloudpickle, gunicorn, configparser, tabulate, databricks-cli, entrypoints, mlflow\n",
      "Successfully installed Flask-1.1.1 Jinja2-2.11.1 Mako-1.1.1 MarkupSafe-1.1.1 Werkzeug-1.0.0 alembic-1.4.0 chardet-3.0.4 click-7.0 cloudpickle-1.3.0 configparser-4.0.2 databricks-cli-0.9.1 docker-4.2.0 entrypoints-0.3 gitdb2-3.0.2 gitpython-3.0.8 gorilla-0.3.0 gunicorn-20.0.4 idna-2.9 itsdangerous-1.1.0 mlflow-1.6.0 prometheus-client-0.7.1 prometheus-flask-exporter-0.12.2 protobuf-3.11.3 python-editor-1.0.4 pyyaml-5.1.2 querystring-parser-1.2.4 requests-2.23.0 simplejson-3.17.0 smmap2-2.0.5 sqlalchemy-1.3.13 sqlparse-0.3.0 tabulate-0.8.6 urllib3-1.25.8 websocket-client-0.57.0\n",
      "\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "2020/02/21 11:46:09 INFO mlflow.projects: === Created directory /tmp/tmpxadt9qnk for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2020/02/21 11:46:09 INFO mlflow.projects: === Running command 'source /opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-df5966c24b6db37ea55c834930bad1137b6dfe30 1>&2 && spark-submit train.py --alpha 0.5 --l1_ratio 0.2' in run with ID 'adf949ae97ab455ab108accab9ccd14c' === \n",
      "20/02/21 11:46:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/apache-spark/python/lib/pyspark.zip/pyspark/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "/opt/apache-spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py:2020: DeprecationWarning: invalid escape sequence \\*\n",
      "/opt/apache-spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py:2020: DeprecationWarning: invalid escape sequence \\*\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/02/21 11:46:12 INFO SparkContext: Running Spark version 2.4.4\n",
      "20/02/21 11:46:12 INFO SparkContext: Submitted application: train.py\n",
      "20/02/21 11:46:12 INFO SecurityManager: Changing view acls to: agm\n",
      "20/02/21 11:46:12 INFO SecurityManager: Changing modify acls to: agm\n",
      "20/02/21 11:46:12 INFO SecurityManager: Changing view acls groups to: \n",
      "20/02/21 11:46:12 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/02/21 11:46:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(agm); groups with view permissions: Set(); users  with modify permissions: Set(agm); groups with modify permissions: Set()\n",
      "20/02/21 11:46:12 INFO Utils: Successfully started service 'sparkDriver' on port 46011.\n",
      "20/02/21 11:46:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/02/21 11:46:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/02/21 11:46:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/02/21 11:46:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/02/21 11:46:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0de2e7b3-a817-4942-9f8f-3c2cde23bbc0\n",
      "20/02/21 11:46:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/02/21 11:46:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/02/21 11:46:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/02/21 11:46:13 INFO SparkUI: Bound SparkUI to localhost, and started at http://localhost:4040\n",
      "20/02/21 11:46:13 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/02/21 11:46:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33121.\n",
      "20/02/21 11:46:13 INFO NettyBlockTransferService: Server created on localhost:33121\n",
      "20/02/21 11:46:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/02/21 11:46:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 33121, None)\n",
      "20/02/21 11:46:13 INFO BlockManagerMasterEndpoint: Registering block manager localhost:33121 with 366.3 MB RAM, BlockManagerId(driver, localhost, 33121, None)\n",
      "20/02/21 11:46:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 33121, None)\n",
      "20/02/21 11:46:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 33121, None)\n",
      "20/02/21 11:46:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/spark-warehouse').\n",
      "20/02/21 11:46:13 INFO SharedState: Warehouse path is 'file:/home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/spark-warehouse'.\n",
      "20/02/21 11:46:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/02/21 11:46:15 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:15 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "20/02/21 11:46:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/02/21 11:46:15 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:15 INFO CodeGenerator: Code generated in 126.049728 ms\n",
      "20/02/21 11:46:15 INFO CodeGenerator: Code generated in 12.551566 ms\n",
      "20/02/21 11:46:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 282.9 KB, free 366.0 MB)\n",
      "20/02/21 11:46:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)\n",
      "20/02/21 11:46:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.3 MB)\n",
      "20/02/21 11:46:15 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "20/02/21 11:46:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:16 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:33121 (size: 4.5 KB, free: 366.3 MB)\n",
      "20/02/21 11:46:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/02/21 11:46:16 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:16 INFO CodeGenerator: Code generated in 8.665024 ms\n",
      "20/02/21 11:46:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1406 bytes result sent to driver\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 93 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:16 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.165 s\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.194275 s\n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "20/02/21 11:46:16 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:16 INFO CodeGenerator: Code generated in 7.682439 ms\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 282.9 KB, free 365.7 MB)\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.3 KB, free 365.7 MB)\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.3 MB)\n",
      "20/02/21 11:46:16 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "20/02/21 11:46:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:16 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 365.7 MB)\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.8 KB, free 365.7 MB)\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:33121 (size: 8.8 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "20/02/21 11:46:16 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1617 bytes result sent to driver\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 96 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:16 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.105969 s\n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:16 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:16 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 282.9 KB, free 365.4 MB)\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.3 KB, free 365.4 MB)\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:16 INFO SparkContext: Created broadcast 4 from toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49\n",
      "20/02/21 11:46:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 15\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 13\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 40\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 48\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 11\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 59\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 38\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 46\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 9\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 52\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 49\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 39\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 25\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 29\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:33121 in memory (size: 4.5 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 26\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 24\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 19\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 43\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 22\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 44\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 47\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:33121 in memory (size: 8.8 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 60\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 28\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 8\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 53\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 21\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 45\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 27\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 30\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 12\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 18\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 37\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 54\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 31\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 58\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 23\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 41\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 10\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 42\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 61\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 16\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 20\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 55\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 17\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 7\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 57\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 51\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 14\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 50\n",
      "20/02/21 11:46:16 INFO ContextCleaner: Cleaned accumulator 56\n",
      "20/02/21 11:46:16 INFO SparkContext: Starting job: toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Got job 2 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49) with 1 output partitions\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Final stage: ResultStage 2 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49)\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49), which has no missing parents\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.8 KB, free 365.4 MB)\n",
      "20/02/21 11:46:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.5 KB, free 365.4 MB)\n",
      "20/02/21 11:46:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:33121 (size: 7.5 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "20/02/21 11:46:16 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:16 INFO CodeGenerator: Code generated in 18.943577 ms\n",
      "20/02/21 11:46:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 176302 bytes result sent to driver\n",
      "20/02/21 11:46:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 97 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:16 INFO DAGScheduler: ResultStage 2 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49) finished in 0.103 s\n",
      "20/02/21 11:46:16 INFO DAGScheduler: Job 2 finished: toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:49, took 0.104765 s\n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 7.573881 ms\n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 28.310673 ms\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 282.9 KB, free 365.1 MB)\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.3 KB, free 365.1 MB)\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO SparkContext: Created broadcast 6 from first at LinearRegression.scala:322\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:17 INFO SparkContext: Starting job: first at LinearRegression.scala:322\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Got job 3 (first at LinearRegression.scala:322) with 1 output partitions\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Final stage: ResultStage 3 (first at LinearRegression.scala:322)\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at first at LinearRegression.scala:322), which has no missing parents\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 32.1 KB, free 365.1 MB)\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 14.8 KB, free 365.0 MB)\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:33121 (size: 14.8 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at first at LinearRegression.scala:322) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\n",
      "20/02/21 11:46:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:17 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 30.297006 ms\n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 10.144459 ms\n",
      "20/02/21 11:46:17 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1923 bytes result sent to driver\n",
      "20/02/21 11:46:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 183 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:17 INFO DAGScheduler: ResultStage 3 (first at LinearRegression.scala:322) finished in 0.188 s\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Job 3 finished: first at LinearRegression.scala:322, took 0.191290 s\n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 23.533098 ms\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 282.9 KB, free 364.8 MB)\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 23.3 KB, free 364.7 MB)\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO SparkContext: Created broadcast 8 from rdd at LinearRegression.scala:326\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] Stage class: LinearRegression\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] Stage uid: LinearRegression_7ff1ac93c525\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:33121 in memory (size: 23.3 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 110\n",
      "20/02/21 11:46:17 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 77\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 96\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 109\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 91\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 80\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 114\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 103\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 74\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 105\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 111\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 93\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 70\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 90\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 98\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 71\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 87\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 76\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 124\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 125\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 122\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 68\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 116\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 85\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 107\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 94\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 113\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 81\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 84\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 120\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 82\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 119\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:33121 in memory (size: 14.8 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 83\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 75\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 99\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 97\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 108\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 73\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 104\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 88\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 118\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 92\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 79\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 67\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 101\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 89\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:33121 in memory (size: 7.5 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 115\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 86\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 123\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 95\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 100\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 72\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 106\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 78\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 117\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 121\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 112\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 102\n",
      "20/02/21 11:46:17 INFO ContextCleaner: Cleaned accumulator 69\n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 29.998786 ms\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 282.9 KB, free 364.8 MB)\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 23.3 KB, free 364.8 MB)\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO SparkContext: Created broadcast 9 from rdd at Instrumentation.scala:61\n",
      "20/02/21 11:46:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] {\"labelCol\":\"quality\",\"elasticNetParam\":0.2,\"maxIter\":10,\"regParam\":0.5}\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] {\"numFeatures\":11}\n",
      "20/02/21 11:46:17 INFO SparkContext: Starting job: treeAggregate at WeightedLeastSquares.scala:105\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Got job 4 (treeAggregate at WeightedLeastSquares.scala:105) with 1 output partitions\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Final stage: ResultStage 4 (treeAggregate at WeightedLeastSquares.scala:105)\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at treeAggregate at WeightedLeastSquares.scala:105), which has no missing parents\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 38.0 KB, free 364.8 MB)\n",
      "20/02/21 11:46:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.0 KB, free 364.8 MB)\n",
      "20/02/21 11:46:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:33121 (size: 17.0 KB, free: 366.2 MB)\n",
      "20/02/21 11:46:17 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at treeAggregate at WeightedLeastSquares.scala:105) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "20/02/21 11:46:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "20/02/21 11:46:17 INFO CodeGenerator: Code generated in 7.882985 ms\n",
      "20/02/21 11:46:17 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:17 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "20/02/21 11:46:17 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "20/02/21 11:46:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2832 bytes result sent to driver\n",
      "20/02/21 11:46:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 246 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:17 INFO DAGScheduler: ResultStage 4 (treeAggregate at WeightedLeastSquares.scala:105) finished in 0.252 s\n",
      "20/02/21 11:46:17 INFO DAGScheduler: Job 4 finished: treeAggregate at WeightedLeastSquares.scala:105, took 0.254290 s\n",
      "20/02/21 11:46:17 INFO Instrumentation: [7b9e5080] Number of instances: 3897.\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 0.07564\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.488745 (rel: 0.0225) 0.329817\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.463773 (rel: 0.0511) 0.103205\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.462068 (rel: 0.00368) 0.0475616\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460784 (rel: 0.00278) 0.0149288\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460656 (rel: 0.000278) 0.00207609\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460654 (rel: 4.22e-06) 0.00104915\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460654 (rel: 4.40e-07) 0.000606525\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460654 (rel: 1.84e-07) 0.000185150\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460654 (rel: 1.62e-08) 4.56139e-05\n",
      "20/02/21 11:46:18 INFO OWLQN: Step Size: 1.000\n",
      "20/02/21 11:46:18 INFO OWLQN: Val and Grad Norm: 0.460654 (rel: 1.05e-09) 1.68757e-05\n",
      "20/02/21 11:46:18 INFO OWLQN: Converged because max iterations reached\n",
      "20/02/21 11:46:18 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:18 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:18 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:18 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:18 INFO CodeGenerator: Code generated in 19.817212 ms\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 282.9 KB, free 364.5 MB)\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.3 KB, free 364.5 MB)\n",
      "20/02/21 11:46:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:18 INFO SparkContext: Created broadcast 11 from rdd at LinearRegression.scala:873\n",
      "20/02/21 11:46:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:18 INFO SparkContext: Starting job: treeAggregate at RegressionMetrics.scala:57\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Got job 5 (treeAggregate at RegressionMetrics.scala:57) with 1 output partitions\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Final stage: ResultStage 5 (treeAggregate at RegressionMetrics.scala:57)\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[32] at treeAggregate at RegressionMetrics.scala:57), which has no missing parents\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 39.6 KB, free 364.4 MB)\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 18.7 KB, free 364.4 MB)\n",
      "20/02/21 11:46:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:33121 (size: 18.7 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[32] at treeAggregate at RegressionMetrics.scala:57) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:18 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks\n",
      "20/02/21 11:46:18 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:18 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "20/02/21 11:46:18 INFO CodeGenerator: Code generated in 5.260893 ms\n",
      "20/02/21 11:46:18 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:18 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2300 bytes result sent to driver\n",
      "20/02/21 11:46:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 92 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:18 INFO DAGScheduler: ResultStage 5 (treeAggregate at RegressionMetrics.scala:57) finished in 0.097 s\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Job 5 finished: treeAggregate at RegressionMetrics.scala:57, took 0.100043 s\n",
      "20/02/21 11:46:18 INFO SparkContext: Starting job: sum at RegressionMetrics.scala:71\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Got job 6 (sum at RegressionMetrics.scala:71) with 1 output partitions\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Final stage: ResultStage 6 (sum at RegressionMetrics.scala:71)\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[33] at map at RegressionMetrics.scala:69), which has no missing parents\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.7 KB, free 364.4 MB)\n",
      "20/02/21 11:46:18 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 18.4 KB, free 364.3 MB)\n",
      "20/02/21 11:46:18 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:33121 (size: 18.4 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:18 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[33] at map at RegressionMetrics.scala:69) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks\n",
      "20/02/21 11:46:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:18 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "20/02/21 11:46:18 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:18 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1817 bytes result sent to driver\n",
      "20/02/21 11:46:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 42 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:18 INFO DAGScheduler: ResultStage 6 (sum at RegressionMetrics.scala:71) finished in 0.048 s\n",
      "20/02/21 11:46:18 INFO DAGScheduler: Job 6 finished: sum at RegressionMetrics.scala:71, took 0.050371 s\n",
      "20/02/21 11:46:19 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:19 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:19 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:19 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:19 INFO CodeGenerator: Code generated in 8.401907 ms\n",
      "20/02/21 11:46:19 INFO CodeGenerator: Code generated in 10.296764 ms\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 282.9 KB, free 364.1 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.3 KB, free 364.0 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 14 from count at LinearRegression.scala:953\n",
      "20/02/21 11:46:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:19 INFO SparkContext: Starting job: count at LinearRegression.scala:953\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Registering RDD 36 (count at LinearRegression.scala:953)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Got job 7 (count at LinearRegression.scala:953) with 1 output partitions\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Final stage: ResultStage 8 (count at LinearRegression.scala:953)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[36] at count at LinearRegression.scala:953), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 25.6 KB, free 364.0 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 13.0 KB, free 364.0 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:33121 (size: 13.0 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[36] at count at LinearRegression.scala:953) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8306 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 162\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 203\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 165\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 211\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 160\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 206\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 158\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:33121 in memory (size: 18.7 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 224\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 228\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 202\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 156\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 193\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 159\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 205\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 195\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 210\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 225\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 191\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 198\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 226\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 149\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 219\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 216\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 147\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 209\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 200\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 150\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 196\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 214\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 217\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 179\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 180\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 148\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 151\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 223\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 186\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 197\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 208\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 215\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 154\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 153\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 178\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 218\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 183\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 207\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 144\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 185\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 181\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 182\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 201\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 187\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 190\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 199\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 227\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 189\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 145\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 220\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 152\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:33121 in memory (size: 17.0 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 213\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 194\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 157\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 164\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:33121 in memory (size: 18.4 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 204\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 146\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 163\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 188\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 184\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 221\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 222\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 167\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 161\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 168\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 212\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 192\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 166\n",
      "20/02/21 11:46:19 INFO ContextCleaner: Cleaned accumulator 155\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2576 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 70 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ShuffleMapStage 7 (count at LinearRegression.scala:953) finished in 0.078 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "20/02/21 11:46:19 INFO DAGScheduler: running: Set()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: waiting: Set(ResultStage 8)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: failed: Set()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[39] at count at LinearRegression.scala:953), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.1 KB, free 364.2 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.8 KB, free 364.2 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:33121 (size: 3.8 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[39] at count at LinearRegression.scala:953) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "20/02/21 11:46:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "20/02/21 11:46:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1739 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 32 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ResultStage 8 (count at LinearRegression.scala:953) finished in 0.040 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Job 7 finished: count at LinearRegression.scala:953, took 0.132447 s\n",
      "20/02/21 11:46:19 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Got job 8 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[41] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 70.9 KB, free 364.1 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.2 KB, free 364.1 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:33121 (size: 25.2 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[41] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8111 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: Saved output of task 'attempt_20200221114619_0041_m_000000_0' to file:/tmp/mlflow/fab1dc45-09c7-4775-b7ec-85f25efdbacb/metadata/_temporary/0/task_20200221114619_0041_m_000000\n",
      "20/02/21 11:46:19 INFO SparkHadoopMapRedUtil: attempt_20200221114619_0041_m_000000_0: Committed\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1072 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 50 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 0.060 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Job 8 finished: runJob at SparkHadoopWriter.scala:78, took 0.062068 s\n",
      "20/02/21 11:46:19 INFO SparkHadoopWriter: Job job_20200221114619_0041 committed.\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Got job 9 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:78)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 71.0 KB, free 364.0 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.3 KB, free 364.0 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:33121 (size: 25.3 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: Saved output of task 'attempt_20200221114619_0043_m_000000_0' to file:/tmp/mlflow/fab1dc45-09c7-4775-b7ec-85f25efdbacb/stages/0_VectorAssembler_fb2ecfd028c7/metadata/_temporary/0/task_20200221114619_0043_m_000000\n",
      "20/02/21 11:46:19 INFO SparkHadoopMapRedUtil: attempt_20200221114619_0043_m_000000_0: Committed\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1029 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 13 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ResultStage 10 (runJob at SparkHadoopWriter.scala:78) finished in 0.021 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 0.023105 s\n",
      "20/02/21 11:46:19 INFO SparkHadoopWriter: Job job_20200221114619_0043 committed.\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Got job 10 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at SparkHadoopWriter.scala:78)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[45] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 71.0 KB, free 363.9 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.3 KB, free 363.9 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:33121 (size: 25.3 KB, free: 366.1 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[45] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8380 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "20/02/21 11:46:19 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: Saved output of task 'attempt_20200221114619_0045_m_000000_0' to file:/tmp/mlflow/fab1dc45-09c7-4775-b7ec-85f25efdbacb/stages/1_LinearRegression_7ff1ac93c525/metadata/_temporary/0/task_20200221114619_0045_m_000000\n",
      "20/02/21 11:46:19 INFO SparkHadoopMapRedUtil: attempt_20200221114619_0045_m_000000_0: Committed\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1029 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 14 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ResultStage 11 (runJob at SparkHadoopWriter.scala:78) finished in 0.024 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Job 10 finished: runJob at SparkHadoopWriter.scala:78, took 0.025968 s\n",
      "20/02/21 11:46:19 INFO SparkHadoopWriter: Job job_20200221114619_0045 committed.\n",
      "20/02/21 11:46:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/02/21 11:46:19 INFO CodeGenerator: Code generated in 9.629381 ms\n",
      "20/02/21 11:46:19 INFO SparkContext: Starting job: parquet at LinearRegression.scala:747\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Registering RDD 48 (parquet at LinearRegression.scala:747)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Got job 11 (parquet at LinearRegression.scala:747) with 1 output partitions\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at LinearRegression.scala:747)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[48] at parquet at LinearRegression.scala:747), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.7 KB, free 363.9 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.3 KB, free 363.9 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:33121 (size: 3.3 KB, free: 366.0 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[48] at parquet at LinearRegression.scala:747) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8193 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
      "20/02/21 11:46:19 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 1247 bytes result sent to driver\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:19 INFO DAGScheduler: ShuffleMapStage 12 (parquet at LinearRegression.scala:747) finished in 0.008 s\n",
      "20/02/21 11:46:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "20/02/21 11:46:19 INFO DAGScheduler: running: Set()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "20/02/21 11:46:19 INFO DAGScheduler: failed: Set()\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting ResultStage 13 (ShuffledRowRDD[49] at parquet at LinearRegression.scala:747), which has no missing parents\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 144.2 KB, free 363.7 MB)\n",
      "20/02/21 11:46:19 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 51.3 KB, free 363.7 MB)\n",
      "20/02/21 11:46:19 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:33121 (size: 51.3 KB, free: 366.0 MB)\n",
      "20/02/21 11:46:19 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (ShuffledRowRDD[49] at parquet at LinearRegression.scala:747) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:19 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "20/02/21 11:46:19 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, NODE_LOCAL, 7767 bytes)\n",
      "20/02/21 11:46:19 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
      "20/02/21 11:46:19 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "20/02/21 11:46:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/02/21 11:46:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "20/02/21 11:46:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "20/02/21 11:46:19 INFO CodecConfig: Compression: SNAPPY\n",
      "20/02/21 11:46:19 INFO CodecConfig: Compression: SNAPPY\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Parquet page size to 1048576\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Dictionary is on\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Validation is off\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Page size checking is: estimated\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Min row count for page size check is: 100\n",
      "20/02/21 11:46:19 INFO ParquetOutputFormat: Max row count for page size check is: 10000\n",
      "20/02/21 11:46:19 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"intercept\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"coefficients\",\n",
      "    \"type\" : {\n",
      "      \"type\" : \"udt\",\n",
      "      \"class\" : \"org.apache.spark.ml.linalg.VectorUDT\",\n",
      "      \"pyClass\" : \"pyspark.ml.linalg.VectorUDT\",\n",
      "      \"sqlType\" : {\n",
      "        \"type\" : \"struct\",\n",
      "        \"fields\" : [ {\n",
      "          \"name\" : \"type\",\n",
      "          \"type\" : \"byte\",\n",
      "          \"nullable\" : false,\n",
      "          \"metadata\" : { }\n",
      "        }, {\n",
      "          \"name\" : \"size\",\n",
      "          \"type\" : \"integer\",\n",
      "          \"nullable\" : true,\n",
      "          \"metadata\" : { }\n",
      "        }, {\n",
      "          \"name\" : \"indices\",\n",
      "          \"type\" : {\n",
      "            \"type\" : \"array\",\n",
      "            \"elementType\" : \"integer\",\n",
      "            \"containsNull\" : false\n",
      "          },\n",
      "          \"nullable\" : true,\n",
      "          \"metadata\" : { }\n",
      "        }, {\n",
      "          \"name\" : \"values\",\n",
      "          \"type\" : {\n",
      "            \"type\" : \"array\",\n",
      "            \"elementType\" : \"double\",\n",
      "            \"containsNull\" : false\n",
      "          },\n",
      "          \"nullable\" : true,\n",
      "          \"metadata\" : { }\n",
      "        } ]\n",
      "      }\n",
      "    },\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"scale\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  required double intercept;\n",
      "  optional group coefficients {\n",
      "    required int32 type (INT_8);\n",
      "    optional int32 size;\n",
      "    optional group indices (LIST) {\n",
      "      repeated group list {\n",
      "        required int32 element;\n",
      "      }\n",
      "    }\n",
      "    optional group values (LIST) {\n",
      "      repeated group list {\n",
      "        required double element;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  required double scale;\n",
      "}\n",
      "\n",
      "       \n",
      "20/02/21 11:46:19 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "20/02/21 11:46:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 180\n",
      "20/02/21 11:46:20 INFO FileOutputCommitter: Saved output of task 'attempt_20200221114619_0013_m_000000_13' to file:/tmp/mlflow/fab1dc45-09c7-4775-b7ec-85f25efdbacb/stages/1_LinearRegression_7ff1ac93c525/data/_temporary/0/task_20200221114619_0013_m_000000\n",
      "20/02/21 11:46:20 INFO SparkHadoopMapRedUtil: attempt_20200221114619_0013_m_000000_13: Committed\n",
      "20/02/21 11:46:20 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2334 bytes result sent to driver\n",
      "20/02/21 11:46:20 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 317 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:20 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:20 INFO DAGScheduler: ResultStage 13 (parquet at LinearRegression.scala:747) finished in 0.335 s\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Job 11 finished: parquet at LinearRegression.scala:747, took 0.346374 s\n",
      "20/02/21 11:46:20 INFO FileFormatWriter: Write Job b2a30019-6dae-480b-9572-6ac8e6744f3a committed.\n",
      "20/02/21 11:46:20 INFO FileFormatWriter: Finished processing stats for write job b2a30019-6dae-480b-9572-6ac8e6744f3a.\n",
      "20/02/21 11:46:20 INFO FileSourceStrategy: Pruning directories with: \n",
      "20/02/21 11:46:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "20/02/21 11:46:20 INFO FileSourceStrategy: Output Data Schema: struct<fixed acidity: double, volatile acidity: double, citric acid: double, residual sugar: double, chlorides: double ... 10 more fields>\n",
      "20/02/21 11:46:20 INFO FileSourceScanExec: Pushed Filters: \n",
      "20/02/21 11:46:20 INFO CodeGenerator: Code generated in 22.935173 ms\n",
      "20/02/21 11:46:20 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 277.6 KB, free 363.4 MB)\n",
      "20/02/21 11:46:20 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 23.3 KB, free 363.4 MB)\n",
      "20/02/21 11:46:20 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:33121 (size: 23.3 KB, free: 366.0 MB)\n",
      "20/02/21 11:46:20 INFO SparkContext: Created broadcast 22 from toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92\n",
      "20/02/21 11:46:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "20/02/21 11:46:20 INFO SparkContext: Starting job: toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Got job 12 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92) with 1 output partitions\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Final stage: ResultStage 14 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92)\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[53] at toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92), which has no missing parents\n",
      "20/02/21 11:46:20 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 42.8 KB, free 363.3 MB)\n",
      "20/02/21 11:46:20 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.2 KB, free 363.3 MB)\n",
      "20/02/21 11:46:20 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:33121 (size: 19.2 KB, free: 366.0 MB)\n",
      "20/02/21 11:46:20 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[53] at toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92) (first 15 tasks are for partitions Vector(0))\n",
      "20/02/21 11:46:20 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks\n",
      "20/02/21 11:46:20 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8317 bytes)\n",
      "20/02/21 11:46:20 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "20/02/21 11:46:20 INFO FileScanRDD: Reading File path: file:///home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/wine-quality.csv, range: 0-264426, partition values: [empty row]\n",
      "20/02/21 11:46:20 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 61163 bytes result sent to driver\n",
      "20/02/21 11:46:20 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 40 ms on localhost (executor driver) (1/1)\n",
      "20/02/21 11:46:20 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "20/02/21 11:46:20 INFO DAGScheduler: ResultStage 14 (toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92) finished in 0.046 s\n",
      "20/02/21 11:46:20 INFO DAGScheduler: Job 12 finished: toPandas at /home/agm/Seldon/seldon-core/examples/models/mlflow_server_ab_test_ambassador/model-a/train.py:92, took 0.047002 s\n",
      "20/02/21 11:46:20 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "20/02/21 11:46:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/02/21 11:46:20 INFO MemoryStore: MemoryStore cleared\n",
      "20/02/21 11:46:20 INFO BlockManager: BlockManager stopped\n",
      "20/02/21 11:46:20 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/02/21 11:46:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/02/21 11:46:20 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/02/21 11:46:20 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/02/21 11:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-1fe2a5f7-8bbd-4a78-8995-a7c82eb34bd6\n",
      "20/02/21 11:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f0db90e-58b7-487c-8e55-4e5ccae84c22\n",
      "20/02/21 11:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f0db90e-58b7-487c-8e55-4e5ccae84c22/pyspark-3705f373-729f-4757-9963-7e2af443343d\n",
      "2020/02/21 11:46:20 INFO mlflow.projects: === Run (ID 'adf949ae97ab455ab108accab9ccd14c') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run model-a -P alpha=0.5 -P l1_ratio=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these commands will create a new run which can be visualised through the MLFlow dashboard as per the screenshot below.\n",
    "\n",
    "![](images/mlflow-dashboard.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these models can actually be found on the `mlruns` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns/0\u001b[00m\n",
      "â”œâ”€â”€ \u001b[01;34madf949ae97ab455ab108accab9ccd14c\u001b[00m\n",
      "â””â”€â”€ meta.yaml\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree -L 1 mlruns/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLmodel\n",
    "\n",
    "Inside each of these folders, MLflow stores the parameters we used to train our model, any metric we logged during training, and a snapshot of our model.\n",
    "If we look into one of them, we can see the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns/0/adf949ae97ab455ab108accab9ccd14c\u001b[00m\n",
      "â”œâ”€â”€ \u001b[01;34martifacts\u001b[00m\n",
      "â”‚Â Â  â”œâ”€â”€ conda.yaml\n",
      "â”‚Â Â  â”œâ”€â”€ MLmodel\n",
      "â”‚Â Â  â””â”€â”€ \u001b[01;34msparkml\u001b[00m\n",
      "â”‚Â Â      â”œâ”€â”€ \u001b[01;34mmetadata\u001b[00m\n",
      "â”‚Â Â      â”‚Â Â  â”œâ”€â”€ part-00000\n",
      "â”‚Â Â      â”‚Â Â  â””â”€â”€ _SUCCESS\n",
      "â”‚Â Â      â””â”€â”€ \u001b[01;34mstages\u001b[00m\n",
      "â”‚Â Â          â”œâ”€â”€ \u001b[01;34m0_VectorAssembler_fb2ecfd028c7\u001b[00m\n",
      "â”‚Â Â          â”‚Â Â  â””â”€â”€ \u001b[01;34mmetadata\u001b[00m\n",
      "â”‚Â Â          â”‚Â Â      â”œâ”€â”€ part-00000\n",
      "â”‚Â Â          â”‚Â Â      â””â”€â”€ _SUCCESS\n",
      "â”‚Â Â          â””â”€â”€ \u001b[01;34m1_LinearRegression_7ff1ac93c525\u001b[00m\n",
      "â”‚Â Â              â”œâ”€â”€ \u001b[01;34mdata\u001b[00m\n",
      "â”‚Â Â              â”‚Â Â  â”œâ”€â”€ part-00000-8ee67d27-ff29-4402-9ad1-1ba7c8390ea9-c000.snappy.parquet\n",
      "â”‚Â Â              â”‚Â Â  â””â”€â”€ _SUCCESS\n",
      "â”‚Â Â              â””â”€â”€ \u001b[01;34mmetadata\u001b[00m\n",
      "â”‚Â Â                  â”œâ”€â”€ part-00000\n",
      "â”‚Â Â                  â””â”€â”€ _SUCCESS\n",
      "â”œâ”€â”€ meta.yaml\n",
      "â”œâ”€â”€ \u001b[01;34mmetrics\u001b[00m\n",
      "â”‚Â Â  â”œâ”€â”€ r2\n",
      "â”‚Â Â  â””â”€â”€ rmse\n",
      "â”œâ”€â”€ \u001b[01;34mparams\u001b[00m\n",
      "â”‚Â Â  â”œâ”€â”€ alpha\n",
      "â”‚Â Â  â””â”€â”€ l1_ratio\n",
      "â””â”€â”€ \u001b[01;34mtags\u001b[00m\n",
      "    â”œâ”€â”€ mlflow.gitRepoURL\n",
      "    â”œâ”€â”€ mlflow.project.backend\n",
      "    â”œâ”€â”€ mlflow.project.entryPoint\n",
      "    â”œâ”€â”€ mlflow.project.env\n",
      "    â”œâ”€â”€ mlflow.source.git.commit\n",
      "    â”œâ”€â”€ mlflow.source.git.repoURL\n",
      "    â”œâ”€â”€ mlflow.source.name\n",
      "    â”œâ”€â”€ mlflow.source.type\n",
      "    â””â”€â”€ mlflow.user\n",
      "\n",
      "12 directories, 24 files\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns/0/$(ls mlruns/0 | head -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we are interested in the `MLmodel` file stored under `artifacts/model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94martifact_path\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\u001b[94mflavors\u001b[39;49;00m:\n",
      "  \u001b[94mpython_function\u001b[39;49;00m:\n",
      "    \u001b[94mdata\u001b[39;49;00m: sparkml\n",
      "    \u001b[94menv\u001b[39;49;00m: conda.yaml\n",
      "    \u001b[94mloader_module\u001b[39;49;00m: mlflow.spark\n",
      "    \u001b[94mpython_version\u001b[39;49;00m: 3.6.10\n",
      "  \u001b[94mspark\u001b[39;49;00m:\n",
      "    \u001b[94mmodel_data\u001b[39;49;00m: sparkml\n",
      "    \u001b[94mpyspark_version\u001b[39;49;00m: 2.4.4\n",
      "\u001b[94mrun_id\u001b[39;49;00m: adf949ae97ab455ab108accab9ccd14c\n",
      "\u001b[94mutc_time_created\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m2020-02-21\u001b[39;49;00m\u001b[31m \u001b[39;49;00m\u001b[33m11:46:19.301362\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l yaml mlruns/0/$(ls mlruns/0 | head -1)/artifacts/MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file stores the details of how the model was stored.\n",
    "With this information (plus the other files in the folder), we are able to load the model back.\n",
    "Seldon's MLflow server will use this information to serve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model registry\n",
    "\n",
    "Finally, we will store the output of our training pipeline in a model registry.\n",
    "This output will involve both the `MLmodel` that we've seen above and any other artifacts like the serialised Spark pipelines or the `conda.yaml` environment definition.\n",
    "All these components represent our trained model.\n",
    "\n",
    "There are plenty of different options to use as model registry.\n",
    "However, for the sake of simplicity, we will use S3 on our example.\n",
    "\n",
    "Now we should upload our newly trained model into a public Google Bucket or S3 bucket.\n",
    "We have already done this to make it simpler, which you will be able to find at `gs://seldon-models/mlflow/model-a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your model using the Pre-packaged Model Server for MLflow\n",
    "\n",
    "Now we can deploy our trained MLflow model.\n",
    "\n",
    "For this we have to create a Seldon definition of the model server definition, which we will break down further below.\n",
    "\n",
    "We will be using the model that we uploaded to our S3 model registry (gs://seldon-models/mlflow/elasticnet_wine), but you can use your model if you uploaded it to a different bucket or any other supported storage backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To setup a K8s cluster for this example, you can use the [setup notebook](../../seldon_core_setup.ipynb#Setup-Cluster), where you will also find instructions to [install an Ambassador Ingress](../../seldon_core_setup.ipynb#Ambassador) and to [install Seldon Core](../../seldon_core_setup.ipynb#Install-Seldon-Core). You can also find these [instructions online](./seldon_core_setup.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: mlflow-deployment\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: mlflow-deployment\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "    - \u001b[94mgraph\u001b[39;49;00m:\n",
      "        \u001b[94mchildren\u001b[39;49;00m: []\n",
      "        \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "        \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/v2/model-a\n",
      "        \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "      \u001b[94mname\u001b[39;49;00m: mlflow-model-a\n",
      "      \u001b[94mreplicas\u001b[39;49;00m: 1\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model-a/seldon-deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we write our configuration file, we are able to deploy it to our cluster by running it with our command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/mlflow-deployment created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f model-a/seldon-deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's created we will just wait until it's deployed. \n",
    "\n",
    "From a high-level point of view, it will download the image for the pre-packaged MLflow model server, and initialise it with the model we specified above.\n",
    "This includes installing the environment specified on our `conda.yaml`.\n",
    "Note that in an actual production use case, your dependencies may be somehwat stable.\n",
    "Therefore, it may make sense to create a [custom image which already has these installed](https://docs.seldon.io/projects/seldon-core/en/latest/servers/mlflow.html#conda-environment-creation), saving time during service initialisation.\n",
    "\n",
    "You can check the status of the deployment and wait until it's ready with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment \"mlflow-deployment-mlflow-deployment-dag-77efeb1\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deployment.apps/mlflow-deployment-mlflow-deployment-dag-77efeb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's deployed, we should see a \"succcessfully rolled out\" message above. We can now test it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed MLFlow model by sending requests\n",
    "Now that our model is deployed in Kubernetes, we are able to send any requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first need the URL that is currently available through Ambassador. \n",
    "\n",
    "If you are running this locally, you should be able to reach it through localhost, in this case we can use port 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambassador                                                  NodePort    10.97.44.51      <none>        80:30080/TCP        23h\n",
      "ambassador-admin                                            ClusterIP   10.108.207.108   <none>        8877/TCP            23h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc | grep ambassador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will select the first datapoint in our dataset to send to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]\n"
     ]
    }
   ],
   "source": [
    "x_0 = data.drop([\"quality\"], axis=1).values[:1]\n",
    "print(list(x_0[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try sending a request first using curl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"meta\": {\n",
      "    \"puid\": \"n7i76rf930auf7u7ulhig51bu5\",\n",
      "    \"tags\": {\n",
      "    },\n",
      "    \"routing\": {\n",
      "    },\n",
      "    \"requestPath\": {\n",
      "      \"wines-classifier\": \"seldonio/mlflowserver_rest:0.2\"\n",
      "    },\n",
      "    \"metrics\": []\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"names\": [],\n",
      "    \"ndarray\": [5.550530190667395]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H 'Content-Type: application/json' \\\n",
    "    -d \"{'data': {'names': [], 'ndarray': [[7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.001, 3.0, 0.45, 8.8]]}}\" \\\n",
    "    http://localhost:80/seldon/seldon/mlflow-deployment/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send the request by using our python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta {\n",
      "  puid: \"cdjl6irq91taaavkam57g2eatu\"\n",
      "  requestPath {\n",
      "    key: \"wines-classifier\"\n",
      "    value: \"seldonio/mlflowserver_rest:0.2\"\n",
      "  }\n",
      "}\n",
      "data {\n",
      "  ndarray {\n",
      "    values {\n",
      "      number_value: 5.550530190667395\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import math\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "HOST = \"localhost\" # Add the URL you found above\n",
    "port = \"80\" # Make sure you use the port above\n",
    "batch = x_0\n",
    "payload_type = \"ndarray\"\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    namespace=\"seldon\",\n",
    "    gateway_endpoint=HOST + \":\" + port)\n",
    "\n",
    "client_prediction = sc.predict(\n",
    "    data=batch, \n",
    "    deployment_name=\"mlflow-deployment\",\n",
    "    names=[],\n",
    "    payload_type=payload_type)\n",
    "\n",
    "print(client_prediction.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your second model\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your second model as an A/B test\n",
    "\n",
    "Now that we have a model in production, it's possible to deploy a second model as an A/B test.\n",
    "Our model will also be an Elastic Net model but using a different set of parameters.\n",
    "We can easily train it by leveraging MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/11/20 11:38:36 INFO mlflow.projects: === Created directory /tmp/tmppr1ufom9 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/11/20 11:38:36 INFO mlflow.projects: === Running command 'source /opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-1ecba04797edb7e7f7212d429debd9b664c31651 1>&2 && python train.py 0.75 0.2' in run with ID '18f9f8c5d6a249f28f024011dea10e23' === \n",
      "Elasticnet model (alpha=0.750000, l1_ratio=0.200000):\n",
      "  RMSE: 0.8118203122913661\n",
      "  MAE: 0.6244638140789723\n",
      "  R2: 0.14878415499818187\n",
      "2019/11/20 11:38:37 INFO mlflow.projects: === Run (ID '18f9f8c5d6a249f28f024011dea10e23') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run . -P alpha=0.75 -P l1_ratio=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we will now need to upload our model to a cloud bucket.\n",
    "To speed things up, we already have done so and the second model is now accessible in `gs://seldon-models/mlflow/model-b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B test\n",
    "\n",
    "We will deploy our second model as an A/B test.\n",
    "In particular, we will redirect 20% of the traffic to the new model.\n",
    "\n",
    "This can be done by simply adding a `traffic` attribute on our `SeldonDeployment` spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[04m\u001b[36m---\u001b[39;49;00m\n",
      "\u001b[94mapiVersion\u001b[39;49;00m: machinelearning.seldon.io/v1alpha2\n",
      "\u001b[94mkind\u001b[39;49;00m: SeldonDeployment\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: mlflow-deployment\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: mlflow-deployment\n",
      "  \u001b[94mpredictors\u001b[39;49;00m:\n",
      "    - \u001b[94mgraph\u001b[39;49;00m:\n",
      "        \u001b[94mchildren\u001b[39;49;00m: []\n",
      "        \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "        \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-a\n",
      "        \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "      \u001b[94mname\u001b[39;49;00m: a-mlflow-deployment-dag\n",
      "      \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "      \u001b[94mtraffic\u001b[39;49;00m: 80\n",
      "    - \u001b[94mgraph\u001b[39;49;00m:\n",
      "        \u001b[94mchildren\u001b[39;49;00m: []\n",
      "        \u001b[94mimplementation\u001b[39;49;00m: MLFLOW_SERVER\n",
      "        \u001b[94mmodelUri\u001b[39;49;00m: gs://seldon-models/mlflow/model-b\n",
      "        \u001b[94mname\u001b[39;49;00m: wines-classifier\n",
      "      \u001b[94mname\u001b[39;49;00m: b-mlflow-deployment-dag\n",
      "      \u001b[94mreplicas\u001b[39;49;00m: 1\n",
      "      \u001b[94mtraffic\u001b[39;49;00m: 20\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ab-test-mlflow-model-server-seldon-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similar to the model above, we only need to run the following to deploy it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/mlflow-deployment created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ab-test-mlflow-model-server-seldon-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the models have been deployed and are running with the following command.\n",
    "\n",
    "We should now see the \"a-\" model and the \"b-\" models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS     RESTARTS   AGE\n",
      "ambassador-5d97b7df6f-tkrhq                                       1/1     Running    0          24h\n",
      "mlflow-deployment-a-mlflow-deployment-dag-77efeb1-56dd56dcpx54t   0/2     Init:0/1   0          6s\n",
      "mlflow-deployment-b-mlflow-deployment-dag-77efeb1-86cb459drl7fw   0/2     Init:0/1   0          6s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise and monitor the performance of your models using Seldon Analytics\n",
    "\n",
    "This section is optional, but by following the instructions you will be able to visualise the performance of both models as per the chart below.\n",
    "\n",
    "In order for this example to work you need to install and run the [Grafana Analytics package for Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/analytics/analytics.html#helm-analytics-chart).\n",
    "\n",
    "For this we can access the URL with the command below, it will request an admin and password which by default are set to the following:\n",
    "* Username: admin\n",
    "* Password: password\n",
    "\n",
    "You can access the grafana dashboard through the port provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31212"
     ]
    }
   ],
   "source": [
    "!kubectl get svc grafana-prom -o jsonpath='{.spec.ports[0].nodePort}'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both models running in our Kubernetes cluster, we can analyse their performance using Seldon Core's integration with Prometheus and Grafana.\n",
    "To do so, we will iterate over the training set (which can be found in `wine-quality.csv`), making a request and sending the feedback of the prediction.\n",
    "\n",
    "Since the `/feedback` endpoint requires a `reward` signal (i.e. the higher the better), we will simulate one as:\n",
    "\n",
    "$$\n",
    "  R(x_{n})\n",
    "    = \\begin{cases}\n",
    "        \\frac{1}{(y_{n} - f(x_{n}))^{2}} &, y_{n} \\neq f(x_{n}) \\\\\n",
    "        500 &, y_{n} = f(x_{n})\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    ", where $R(x_{n})$ is the reward for input point $x_{n}$, $f(x_{n})$ is our trained model and $y_{n}$ is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [4.949928760465064]\n",
       "1         [2.33866485520918]\n",
       "2       [16.671295276036165]\n",
       "3       [11.360528710955778]\n",
       "4       [10.762015969288063]\n",
       "                ...         \n",
       "4893     [270.7374890482452]\n",
       "4894    [1.8348875422756648]\n",
       "4895     [3.872377496349884]\n",
       "4896    [1.9544204216470193]\n",
       "4897     [22.25374886390087]\n",
       "Length: 4898, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_reward(y, y_pred):\n",
    "    if y == y_pred:\n",
    "        return 500    \n",
    "    \n",
    "    return 1 / np.square(y - y_pred)\n",
    "\n",
    "def _test_row(row):\n",
    "    input_features = row[:-1]\n",
    "    feature_names = input_features.index.to_list()\n",
    "    X = input_features.values.reshape(1, -1)\n",
    "    y = row[-1].reshape(1, -1)\n",
    "    \n",
    "    # Note that we are re-using the SeldonClient defined previously\n",
    "    r = sc.predict(\n",
    "        deployment_name=\"mlflow-deployment\",\n",
    "        data=X,\n",
    "        names=feature_names)\n",
    "    \n",
    "    y_pred = r.response.data.tensor.values\n",
    "    reward = _get_reward(y, y_pred)\n",
    "    sc.feedback(\n",
    "        deployment_name=\"mlflow-deployment\",\n",
    "        prediction_request=r.request,\n",
    "        prediction_response=r.response,\n",
    "        reward=reward)\n",
    "    \n",
    "    return reward[0]\n",
    "\n",
    "data.apply(_test_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see Seldon's pre-built Grafana dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/grafana-mlflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bottom of the dashboard you can see the following charts: \n",
    "\n",
    "- On the left: the requests per second, which shows the different traffic breakdown we specified.\n",
    "- On the center: the reward, where you can see how model `a` outperforms model `b` by a large margin.\n",
    "- On the right, the latency for each one of them.\n",
    "\n",
    "You are able to add your own custom metrics, and try out other more complex deployments by following further guides at https://docs.seldon.io/projects/seldon-core/en/latest/workflow/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seldon-core",
   "language": "python",
   "name": "seldon-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
