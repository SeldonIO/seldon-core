{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Iris Model using customData\n",
    "\n",
    "* Wrap a scikit-learn python model for use as a prediction microservice in seldon-core\n",
    "    * Run locally on Docker to test\n",
    "    * Deploy on seldon-core running on a Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "* [s2i](https://github.com/openshift/source-to-image)\n",
    "* Seldon Core v1.0.3+ installed\n",
    "* `pip install sklearn seldon-core protobuf grpcio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import datasets\n",
    "\n",
    "def main():\n",
    "    clf = LogisticRegression()\n",
    "    p = Pipeline([('clf', clf)])\n",
    "    print('Training model...')\n",
    "    p.fit(X, y)\n",
    "    print('Model trained!')\n",
    "\n",
    "    filename_p = 'IrisClassifier.sav'\n",
    "    print('Saving model in %s' % filename_p)\n",
    "    joblib.dump(p, filename_p)\n",
    "    print('Model saved!')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print('Loading iris data set...')\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    print('Dataset loaded!')\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Protobuf Specification\n",
    "\n",
    "First, we'll need to define our custom protobuf specification so that it can be leveraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris.proto\n",
    "\n",
    "syntax = \"proto3\";\n",
    "\n",
    "package iris;\n",
    "\n",
    "message IrisPredictRequest {\n",
    "    float sepal_length = 1;\n",
    "    float sepal_width = 2;\n",
    "    float petal_length = 3;\n",
    "    float petal_width = 4;\n",
    "}\n",
    "\n",
    "message IrisPredictResponse {\n",
    "    float setosa = 1;\n",
    "    float versicolor = 2;\n",
    "    float virginica = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Protobuf Compilation\n",
    "\n",
    "We will need to compile our custom protobuf for python so that we can unpack the `customData` field passed to our `predict` method later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m grpc.tools.protoc --python_out=./ --proto_path=. iris.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gRPC test\n",
    "\n",
    "Wrap model using s2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!s2i build . seldonio/seldon-core-s2i-python37:1.2.1-dev seldonio/sklearn-iris-customdata:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serve the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --name \"iris_predictor\" -d --rm -p 5000:5000 seldonio/sklearn-iris-customdata:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test using custom protobuf payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iris_pb2 import IrisPredictRequest, IrisPredictResponse\n",
    "from seldon_core.proto import prediction_pb2, prediction_pb2_grpc\n",
    "import grpc\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:5000\")\n",
    "stub = prediction_pb2_grpc.ModelStub(channel)\n",
    "\n",
    "iris_request = IrisPredictRequest(sepal_length=7.233, sepal_width=4.652, petal_length=7.39, petal_width=0.324)\n",
    "\n",
    "seldon_request = prediction_pb2.SeldonMessage()\n",
    "seldon_request.customData.Pack(iris_request)\n",
    "\n",
    "response = stub.Predict(seldon_request)\n",
    "\n",
    "iris_response = IrisPredictResponse()\n",
    "response.customData.Unpack(iris_response)\n",
    "\n",
    "print(iris_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop serving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker rm iris_predictor --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Core\n",
    "\n",
    "Use the [setup notebook](https://github.com/SeldonIO/seldon-core/blob/master/notebooks/seldon_core_setup.ipynb) to setup Seldon Core with an ingress - either Ambassador or Istio\n",
    "\n",
    "Then port-forward to that ingress on localhost:8003 in a separate terminal either with:\n",
    "\n",
    "* Ambassador: `kubectl port-forward $(kubectl get pods -n seldon -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') -n seldon 8003:8080`\n",
    "* Istio: `kubectl port-forward $(kubectl get pods -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') -n istio-system 8003:80`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create namespace seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl config set-context $(kubectl config current-context) --namespace=seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your Seldon Model\n",
    "\n",
    "We first create a configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sklearn_iris_customdata_deployment.yaml\n",
    "\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: seldon-deployment-example\n",
    "spec:\n",
    "  name: sklearn-iris-deployment\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: groszewn/sklearn-iris-customdata:0.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          name: sklearn-iris-classifier\n",
    "    graph:\n",
    "      children: []\n",
    "      endpoint:\n",
    "        type: GRPC\n",
    "      name: sklearn-iris-classifier\n",
    "      type: MODEL\n",
    "    name: sklearn-iris-predictor\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model in our cluster\n",
    "\n",
    "Apply the Seldon Deployment configuration file we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f sklearn_iris_customdata_deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the model has been deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=seldon-deployment-example -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test by sending prediction calls\n",
    "\n",
    "`IrisPredictRequest` sent via the `customData` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_request = IrisPredictRequest(sepal_length=7.233, sepal_width=4.652, petal_length=7.39, petal_width=0.324)\n",
    "\n",
    "seldon_request = prediction_pb2.SeldonMessage()\n",
    "seldon_request.customData.Pack(iris_request)\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:8003\")\n",
    "stub = prediction_pb2_grpc.SeldonStub(channel)\n",
    "\n",
    "metadata = [(\"seldon\", \"seldon-deployment-example\"), (\"namespace\", \"seldon\")]\n",
    "\n",
    "response = stub.Predict(request=seldon_request, metadata=metadata)\n",
    "\n",
    "iris_response = IrisPredictResponse()\n",
    "response.customData.Unpack(iris_response)\n",
    "\n",
    "print(iris_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup our deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f sklearn_iris_customdata_deployment.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
