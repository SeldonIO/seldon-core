{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Memory Example\n",
    "\n",
    "This example demonstrates how to run locally a Conversational Memory Runtime MLServer instance. It \n",
    "also illustrates the different ways it can be used. (Note: this example only showcases the Conversational \n",
    "Memory as a stand-alone component, for a more integrated example check out [chatbot](/examples/chat_bot/chat_bot.md).)\n",
    "\n",
    "To get up and running we need to install the runtime. To do this create a virtual environment within \n",
    "the base memory runtime directory:\n",
    "\n",
    "```sh\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "and then install:\n",
    "\n",
    "```sh\n",
    "pip install .\n",
    "```\n",
    "\n",
    "We are now ready to create the `model-settings.json`. If this kind of file is new to you, make sure to \n",
    "check out the [MLSerer docs](https://mlserver.readthedocs.io/en/latest/reference/model-settings.html) for \n",
    "a detailed description on what they are and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"conversational_memory\",\n",
      "  \"implementation\": \"mlserver_memory.ConversationalMemory\",\n",
      "  \"parameters\": {\n",
      "    \"extra\": {\n",
      "      \"database_config\": {\n",
      "        \"database\": \"filesys\"\n",
      "      },\n",
      "      \"memory_config\": {\n",
      "        \"window_size\": 2,\n",
      "        \"tensor_names\": [\"content\"]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat filesys/model-settings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the ConversationalMemory [specification](api-ref-cbms) for an explanation of the `memory_config` \n",
    "parameters as well as the file system backend [specification](api-ref-fsc). \n",
    "\n",
    "\n",
    "Now that the `model-settings.json` file is ready, we can start the runtime using:\n",
    "\n",
    "```sh\n",
    "mlserver start filesys\n",
    "```\n",
    "\n",
    "There are two intended workflows for using the memory runtime: memory updates and memory retrieval. In \n",
    "the first one, the developer wishes to update the memory allocated to an LLM with new messages and, \n",
    "in the second one, the developer wants to get messages from the memory store. When sending a request \n",
    "to the runtime the developer must always include a `memory_id` tensor that the runtime uses to identify \n",
    "the relevant part of a conversation needed. Regardless of whether the history of a conversation is updated, \n",
    "the runtime will return a `window_size` object with the number of messages from the memory.\n",
    "\n",
    "![developer updates conversation history](./update_workflow.png)\n",
    "\n",
    "For a more concrete example of the first workflow, imagine we want to get the conversational history from a conversation about \n",
    "some financial documents. A developer can do this by just sending the `memory_id` of a tensor corresponding to\n",
    "the question `Why did Apple acquire X company?`. In doing so, the runtime will return the last `window_size` \n",
    "number of messages it has stored, for example, `7` if that is the number of questions previously asked in the \n",
    "same conversation.\n",
    "\n",
    "For the second workflow, suppose we have set the max number of messages in our memory to 10 and that we would like \n",
    "to update the last one (`window_size - 1`) on behalf of our user. To do so we'd send a `memory_id` tensor as well \n",
    "as a `content` tensor to the runtime to update the memory with the new message, read the last `window_size` messages \n",
    "(including the new one) and return these. This is done to make the typical flow of a conversation easy to \n",
    "implement with the runtime. The next examples will shed some light on how this process works.\n",
    "\n",
    "**Note**: If the developer wished they could store more than one tensor by updating the list of `tensor_names` \n",
    "in the `model-settings.json` file. The only restriction is that if they specify multiple tensors, then they always \n",
    "send an equal number of each to prevent the store from getting out of sync.\n",
    "\n",
    "**Note**: The `window_size` parameter is intended to be used to control the size of requests being sent \n",
    "along the pipelines in which the memory runtime is deployed. It's not intended to be used to control the \n",
    "number of tokens sent to the LLM. Token bounds can be configured in the [LLM inference runtime](../api/index.md).\n",
    "\n",
    "To get started, let's send a request with a `memory_id` and some content to initialize a conversational \n",
    "store. We've chosen the `memory_id=6e1f1413` at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'dccd1dbd-4ec8-4172-a737-fb6480d20bc4',\n",
      " 'model_name': 'conversational_memory',\n",
      " 'outputs': [{'data': ['6e1f1413'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'memory_id',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]},\n",
      "             {'data': ['Hello, how are you?'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'content',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]}],\n",
      " 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pprint\n",
    "\n",
    "\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"memory_id\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"6e1f1413\"],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"content\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Hello, how are you?\"],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "endpoint = f\"http://localhost:8080/v2/models/conversational_memory/infer\"\n",
    "response_with_params = requests.post(\n",
    "    endpoint,\n",
    "    json=inference_request,\n",
    ")\n",
    "\n",
    "pprint.pprint(response_with_params.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response returned contains the `memory_id` and the content. The message you've sent \n",
    "has now been updated in the conversational store. We can now send a second request with the `memory_id` \n",
    "alone in order to see what's contained in the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '618a7d1d-1a2b-40d5-9c43-28ab7769f4dc',\n",
      " 'model_name': 'conversational_memory',\n",
      " 'outputs': [{'data': ['6e1f1413'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'memory_id',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]},\n",
      "             {'data': ['Hello, how are you?'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'content',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]}],\n",
      " 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"memory_id\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"6e1f1413\"],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "endpoint = f\"http://localhost:8080/v2/models/conversational_memory/infer\"\n",
    "response_with_params = requests.post(\n",
    "    endpoint,\n",
    "    json=inference_request\n",
    ")\n",
    "\n",
    "pprint.pprint(response_with_params.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got back the previous message we just sent.\n",
    "\n",
    "Multiple messages can be sent to the memory runtime at one time. As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'fac725c0-0195-408e-a8fe-be4ad2b2ad2a',\n",
      " 'model_name': 'conversational_memory',\n",
      " 'outputs': [{'data': ['5P5pB2Lv'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'memory_id',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]},\n",
      "             {'data': [\"Good, i'd like to buy a car.\",\n",
      "                       'Really, what kind of car?'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'content',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [2, 1]}],\n",
      " 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"memory_id\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"5P5pB2Lv\"],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"content\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\n",
    "                \"Hello, how are you?\",\n",
    "                \"I'm well, how are you?\",\n",
    "                \"Good, i'd like to buy a car.\",\n",
    "                \"Really, what kind of car?\"\n",
    "            ],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "endpoint = f\"http://localhost:8080/v2/models/conversational_memory/infer\"\n",
    "response_with_params = requests.post(\n",
    "    endpoint,\n",
    "    json=inference_request,\n",
    ")\n",
    "\n",
    "pprint.pprint(response_with_params.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will write all the content tensors at the same time. This is useful for loading conversations \n",
    "from a different store into the current conversational memory. Note that, we only received 2 messages back \n",
    "out of the 4 we sent. This is because we've set the `window_size` parameter in the `model-settings.json` to \n",
    "2. To change this we can either do so in the model settings themselves and then restart MLServer, or we \n",
    "can pass a parameter in the inference request that sets the `window_size` value dynamically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '2e970796-4f75-4372-9cef-84c2afca854c',\n",
      " 'model_name': 'conversational_memory',\n",
      " 'outputs': [{'data': ['5P5pB2Lv'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'memory_id',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [1, 1]},\n",
      "             {'data': ['Hello, how are you?',\n",
      "                       \"I'm well, how are you?\",\n",
      "                       \"Good, i'd like to buy a car.\",\n",
      "                       'Really, what kind of car?'],\n",
      "              'datatype': 'BYTES',\n",
      "              'name': 'content',\n",
      "              'parameters': {'content_type': 'str'},\n",
      "              'shape': [4, 1]}],\n",
      " 'parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"memory_id\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"5P5pB2Lv\"],\n",
    "            \"parameters\": {\"content_type\": \"str\"},\n",
    "        },\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"memory_parameters\": {\"window_size\": 5}\n",
    "    },\n",
    "}\n",
    "\n",
    "endpoint = f\"http://localhost:8080/v2/models/conversational_memory/infer\"\n",
    "response_with_params = requests.post(\n",
    "    endpoint,\n",
    "    json=inference_request,\n",
    ")\n",
    "\n",
    "pprint.pprint(response_with_params.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above showcases the memory runtime as a stand-alone server. The runtime itself is intended to be used \n",
    "as a component within a Seldon Core V2 pipeline along with other components such as large language models \n",
    "and other LLM-based tools. This [chatbot](/examples/chat_bot/chat_bot.md) example illustrates such a usecase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
