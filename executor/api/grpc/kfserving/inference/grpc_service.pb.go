// Code generated by protoc-gen-go. DO NOT EDIT.
// source: grpc_service.proto

package inference

import (
	context "context"
	fmt "fmt"
	math "math"

	proto "github.com/golang/protobuf/proto"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

//@@
//@@.. cpp:var:: message ServerLiveRequest
//@@
//@@   Request message for ServerLive.
//@@
type ServerLiveRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerLiveRequest) Reset()         { *m = ServerLiveRequest{} }
func (m *ServerLiveRequest) String() string { return proto.CompactTextString(m) }
func (*ServerLiveRequest) ProtoMessage()    {}
func (*ServerLiveRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{0}
}

func (m *ServerLiveRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerLiveRequest.Unmarshal(m, b)
}
func (m *ServerLiveRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerLiveRequest.Marshal(b, m, deterministic)
}
func (m *ServerLiveRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerLiveRequest.Merge(m, src)
}
func (m *ServerLiveRequest) XXX_Size() int {
	return xxx_messageInfo_ServerLiveRequest.Size(m)
}
func (m *ServerLiveRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerLiveRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerLiveRequest proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message ServerLiveResponse
//@@
//@@   Response message for ServerLive.
//@@
type ServerLiveResponse struct {
	//@@
	//@@  .. cpp:var:: bool live
	//@@
	//@@     True if the inference server is live, false it not live.
	//@@
	Live                 bool     `protobuf:"varint,1,opt,name=live,proto3" json:"live,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerLiveResponse) Reset()         { *m = ServerLiveResponse{} }
func (m *ServerLiveResponse) String() string { return proto.CompactTextString(m) }
func (*ServerLiveResponse) ProtoMessage()    {}
func (*ServerLiveResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{1}
}

func (m *ServerLiveResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerLiveResponse.Unmarshal(m, b)
}
func (m *ServerLiveResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerLiveResponse.Marshal(b, m, deterministic)
}
func (m *ServerLiveResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerLiveResponse.Merge(m, src)
}
func (m *ServerLiveResponse) XXX_Size() int {
	return xxx_messageInfo_ServerLiveResponse.Size(m)
}
func (m *ServerLiveResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerLiveResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerLiveResponse proto.InternalMessageInfo

func (m *ServerLiveResponse) GetLive() bool {
	if m != nil {
		return m.Live
	}
	return false
}

//@@
//@@.. cpp:var:: message ServerReadyRequest
//@@
//@@   Request message for ServerReady.
//@@
type ServerReadyRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerReadyRequest) Reset()         { *m = ServerReadyRequest{} }
func (m *ServerReadyRequest) String() string { return proto.CompactTextString(m) }
func (*ServerReadyRequest) ProtoMessage()    {}
func (*ServerReadyRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{2}
}

func (m *ServerReadyRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerReadyRequest.Unmarshal(m, b)
}
func (m *ServerReadyRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerReadyRequest.Marshal(b, m, deterministic)
}
func (m *ServerReadyRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerReadyRequest.Merge(m, src)
}
func (m *ServerReadyRequest) XXX_Size() int {
	return xxx_messageInfo_ServerReadyRequest.Size(m)
}
func (m *ServerReadyRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerReadyRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerReadyRequest proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message ServerReadyResponse
//@@
//@@   Response message for ServerReady.
//@@
type ServerReadyResponse struct {
	//@@
	//@@  .. cpp:var:: bool ready
	//@@
	//@@     True if the inference server is ready, false it not ready.
	//@@
	Ready                bool     `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerReadyResponse) Reset()         { *m = ServerReadyResponse{} }
func (m *ServerReadyResponse) String() string { return proto.CompactTextString(m) }
func (*ServerReadyResponse) ProtoMessage()    {}
func (*ServerReadyResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{3}
}

func (m *ServerReadyResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerReadyResponse.Unmarshal(m, b)
}
func (m *ServerReadyResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerReadyResponse.Marshal(b, m, deterministic)
}
func (m *ServerReadyResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerReadyResponse.Merge(m, src)
}
func (m *ServerReadyResponse) XXX_Size() int {
	return xxx_messageInfo_ServerReadyResponse.Size(m)
}
func (m *ServerReadyResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerReadyResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerReadyResponse proto.InternalMessageInfo

func (m *ServerReadyResponse) GetReady() bool {
	if m != nil {
		return m.Ready
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelReadyRequest
//@@
//@@   Request message for ModelReady.
//@@
type ModelReadyRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model to check for readiness.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string version
	//@@
	//@@     The version of the model to check for readiness. If not given the
	//@@     server will choose a version based on the model and internal policy.
	//@@
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelReadyRequest) Reset()         { *m = ModelReadyRequest{} }
func (m *ModelReadyRequest) String() string { return proto.CompactTextString(m) }
func (*ModelReadyRequest) ProtoMessage()    {}
func (*ModelReadyRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{4}
}

func (m *ModelReadyRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelReadyRequest.Unmarshal(m, b)
}
func (m *ModelReadyRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelReadyRequest.Marshal(b, m, deterministic)
}
func (m *ModelReadyRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelReadyRequest.Merge(m, src)
}
func (m *ModelReadyRequest) XXX_Size() int {
	return xxx_messageInfo_ModelReadyRequest.Size(m)
}
func (m *ModelReadyRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelReadyRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelReadyRequest proto.InternalMessageInfo

func (m *ModelReadyRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelReadyRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelReadyResponse
//@@
//@@   Response message for ModelReady.
//@@
type ModelReadyResponse struct {
	//@@
	//@@  .. cpp:var:: bool ready
	//@@
	//@@     True if the model is ready, false it not ready.
	//@@
	Ready                bool     `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelReadyResponse) Reset()         { *m = ModelReadyResponse{} }
func (m *ModelReadyResponse) String() string { return proto.CompactTextString(m) }
func (*ModelReadyResponse) ProtoMessage()    {}
func (*ModelReadyResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{5}
}

func (m *ModelReadyResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelReadyResponse.Unmarshal(m, b)
}
func (m *ModelReadyResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelReadyResponse.Marshal(b, m, deterministic)
}
func (m *ModelReadyResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelReadyResponse.Merge(m, src)
}
func (m *ModelReadyResponse) XXX_Size() int {
	return xxx_messageInfo_ModelReadyResponse.Size(m)
}
func (m *ModelReadyResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelReadyResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelReadyResponse proto.InternalMessageInfo

func (m *ModelReadyResponse) GetReady() bool {
	if m != nil {
		return m.Ready
	}
	return false
}

//@@
//@@.. cpp:var:: message ServerMetadataRequest
//@@
//@@   Request message for ServerMetadata.
//@@
type ServerMetadataRequest struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerMetadataRequest) Reset()         { *m = ServerMetadataRequest{} }
func (m *ServerMetadataRequest) String() string { return proto.CompactTextString(m) }
func (*ServerMetadataRequest) ProtoMessage()    {}
func (*ServerMetadataRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{6}
}

func (m *ServerMetadataRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerMetadataRequest.Unmarshal(m, b)
}
func (m *ServerMetadataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerMetadataRequest.Marshal(b, m, deterministic)
}
func (m *ServerMetadataRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerMetadataRequest.Merge(m, src)
}
func (m *ServerMetadataRequest) XXX_Size() int {
	return xxx_messageInfo_ServerMetadataRequest.Size(m)
}
func (m *ServerMetadataRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerMetadataRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ServerMetadataRequest proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message ServerMetadataResponse
//@@
//@@   Response message for ServerMetadata.
//@@
type ServerMetadataResponse struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The server name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@
	//@@  .. cpp:var:: string version
	//@@
	//@@     The server version.
	//@@
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	//@@
	//@@  .. cpp:var:: string extensions (repeated)
	//@@
	//@@     The extensions supported by the server.
	//@@
	Extensions           []string `protobuf:"bytes,3,rep,name=extensions,proto3" json:"extensions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ServerMetadataResponse) Reset()         { *m = ServerMetadataResponse{} }
func (m *ServerMetadataResponse) String() string { return proto.CompactTextString(m) }
func (*ServerMetadataResponse) ProtoMessage()    {}
func (*ServerMetadataResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{7}
}

func (m *ServerMetadataResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ServerMetadataResponse.Unmarshal(m, b)
}
func (m *ServerMetadataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ServerMetadataResponse.Marshal(b, m, deterministic)
}
func (m *ServerMetadataResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ServerMetadataResponse.Merge(m, src)
}
func (m *ServerMetadataResponse) XXX_Size() int {
	return xxx_messageInfo_ServerMetadataResponse.Size(m)
}
func (m *ServerMetadataResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ServerMetadataResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ServerMetadataResponse proto.InternalMessageInfo

func (m *ServerMetadataResponse) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ServerMetadataResponse) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

func (m *ServerMetadataResponse) GetExtensions() []string {
	if m != nil {
		return m.Extensions
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelMetadataRequest
//@@
//@@   Request message for ModelMetadata.
//@@
type ModelMetadataRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string version
	//@@
	//@@     The version of the model to check for readiness. If not
	//@@     given the server will choose a version based on the
	//@@     model and internal policy.
	//@@
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelMetadataRequest) Reset()         { *m = ModelMetadataRequest{} }
func (m *ModelMetadataRequest) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataRequest) ProtoMessage()    {}
func (*ModelMetadataRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{8}
}

func (m *ModelMetadataRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataRequest.Unmarshal(m, b)
}
func (m *ModelMetadataRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataRequest.Marshal(b, m, deterministic)
}
func (m *ModelMetadataRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataRequest.Merge(m, src)
}
func (m *ModelMetadataRequest) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataRequest.Size(m)
}
func (m *ModelMetadataRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataRequest proto.InternalMessageInfo

func (m *ModelMetadataRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelMetadataResponse
//@@
//@@   Response message for ModelMetadata.
//@@
type ModelMetadataResponse struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The model name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@
	//@@  .. cpp:var:: string versions (repeated)
	//@@
	//@@     The versions of the model.
	//@@
	Versions []string `protobuf:"bytes,2,rep,name=versions,proto3" json:"versions,omitempty"`
	//@@
	//@@  .. cpp:var:: string platform
	//@@
	//@@     The model's platform.
	//@@
	Platform string `protobuf:"bytes,3,opt,name=platform,proto3" json:"platform,omitempty"`
	//@@
	//@@  .. cpp:var:: TensorMetadata inputs (repeated)
	//@@
	//@@     The model's inputs.
	//@@
	Inputs []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,4,rep,name=inputs,proto3" json:"inputs,omitempty"`
	//@@
	//@@  .. cpp:var:: TensorMetadata outputs (repeated)
	//@@
	//@@     The model's outputs.
	//@@
	Outputs              []*ModelMetadataResponse_TensorMetadata `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                                `json:"-"`
	XXX_unrecognized     []byte                                  `json:"-"`
	XXX_sizecache        int32                                   `json:"-"`
}

func (m *ModelMetadataResponse) Reset()         { *m = ModelMetadataResponse{} }
func (m *ModelMetadataResponse) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataResponse) ProtoMessage()    {}
func (*ModelMetadataResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{9}
}

func (m *ModelMetadataResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataResponse.Unmarshal(m, b)
}
func (m *ModelMetadataResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataResponse.Marshal(b, m, deterministic)
}
func (m *ModelMetadataResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataResponse.Merge(m, src)
}
func (m *ModelMetadataResponse) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataResponse.Size(m)
}
func (m *ModelMetadataResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataResponse proto.InternalMessageInfo

func (m *ModelMetadataResponse) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataResponse) GetVersions() []string {
	if m != nil {
		return m.Versions
	}
	return nil
}

func (m *ModelMetadataResponse) GetPlatform() string {
	if m != nil {
		return m.Platform
	}
	return ""
}

func (m *ModelMetadataResponse) GetInputs() []*ModelMetadataResponse_TensorMetadata {
	if m != nil {
		return m.Inputs
	}
	return nil
}

func (m *ModelMetadataResponse) GetOutputs() []*ModelMetadataResponse_TensorMetadata {
	if m != nil {
		return m.Outputs
	}
	return nil
}

//@@
//@@  .. cpp:var:: message TensorMetadata
//@@
//@@     Metadata for a tensor.
//@@
type ModelMetadataResponse_TensorMetadata struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The tensor name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@
	//@@    .. cpp:var:: string datatype
	//@@
	//@@       The tensor data type.
	//@@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	//@@
	//@@    .. cpp:var:: int64 shape (repeated)
	//@@
	//@@       The tensor shape. A variable-size dimension is represented
	//@@       by a -1 value.
	//@@
	Shape                []int64  `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelMetadataResponse_TensorMetadata) Reset()         { *m = ModelMetadataResponse_TensorMetadata{} }
func (m *ModelMetadataResponse_TensorMetadata) String() string { return proto.CompactTextString(m) }
func (*ModelMetadataResponse_TensorMetadata) ProtoMessage()    {}
func (*ModelMetadataResponse_TensorMetadata) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{9, 0}
}

func (m *ModelMetadataResponse_TensorMetadata) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Unmarshal(m, b)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Marshal(b, m, deterministic)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Merge(m, src)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_Size() int {
	return xxx_messageInfo_ModelMetadataResponse_TensorMetadata.Size(m)
}
func (m *ModelMetadataResponse_TensorMetadata) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelMetadataResponse_TensorMetadata.DiscardUnknown(m)
}

var xxx_messageInfo_ModelMetadataResponse_TensorMetadata proto.InternalMessageInfo

func (m *ModelMetadataResponse_TensorMetadata) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelMetadataResponse_TensorMetadata) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelMetadataResponse_TensorMetadata) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

//@@
//@@.. cpp:var:: message InferParameter
//@@
//@@   An inference parameter value.
//@@
type InferParameter struct {
	//@@  .. cpp:var:: oneof parameter_choice
	//@@
	//@@     The parameter value can be a string, an int64 or
	//@@     a boolean
	//@@
	//
	// Types that are valid to be assigned to ParameterChoice:
	//	*InferParameter_BoolParam
	//	*InferParameter_Int64Param
	//	*InferParameter_StringParam
	ParameterChoice      isInferParameter_ParameterChoice `protobuf_oneof:"parameter_choice"`
	XXX_NoUnkeyedLiteral struct{}                         `json:"-"`
	XXX_unrecognized     []byte                           `json:"-"`
	XXX_sizecache        int32                            `json:"-"`
}

func (m *InferParameter) Reset()         { *m = InferParameter{} }
func (m *InferParameter) String() string { return proto.CompactTextString(m) }
func (*InferParameter) ProtoMessage()    {}
func (*InferParameter) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{10}
}

func (m *InferParameter) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferParameter.Unmarshal(m, b)
}
func (m *InferParameter) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferParameter.Marshal(b, m, deterministic)
}
func (m *InferParameter) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferParameter.Merge(m, src)
}
func (m *InferParameter) XXX_Size() int {
	return xxx_messageInfo_InferParameter.Size(m)
}
func (m *InferParameter) XXX_DiscardUnknown() {
	xxx_messageInfo_InferParameter.DiscardUnknown(m)
}

var xxx_messageInfo_InferParameter proto.InternalMessageInfo

type isInferParameter_ParameterChoice interface {
	isInferParameter_ParameterChoice()
}

type InferParameter_BoolParam struct {
	BoolParam bool `protobuf:"varint,1,opt,name=bool_param,json=boolParam,proto3,oneof"`
}

type InferParameter_Int64Param struct {
	Int64Param int64 `protobuf:"varint,2,opt,name=int64_param,json=int64Param,proto3,oneof"`
}

type InferParameter_StringParam struct {
	StringParam string `protobuf:"bytes,3,opt,name=string_param,json=stringParam,proto3,oneof"`
}

func (*InferParameter_BoolParam) isInferParameter_ParameterChoice() {}

func (*InferParameter_Int64Param) isInferParameter_ParameterChoice() {}

func (*InferParameter_StringParam) isInferParameter_ParameterChoice() {}

func (m *InferParameter) GetParameterChoice() isInferParameter_ParameterChoice {
	if m != nil {
		return m.ParameterChoice
	}
	return nil
}

func (m *InferParameter) GetBoolParam() bool {
	if x, ok := m.GetParameterChoice().(*InferParameter_BoolParam); ok {
		return x.BoolParam
	}
	return false
}

func (m *InferParameter) GetInt64Param() int64 {
	if x, ok := m.GetParameterChoice().(*InferParameter_Int64Param); ok {
		return x.Int64Param
	}
	return 0
}

func (m *InferParameter) GetStringParam() string {
	if x, ok := m.GetParameterChoice().(*InferParameter_StringParam); ok {
		return x.StringParam
	}
	return ""
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*InferParameter) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*InferParameter_BoolParam)(nil),
		(*InferParameter_Int64Param)(nil),
		(*InferParameter_StringParam)(nil),
	}
}

//@@
//@@.. cpp:var:: message InferTensorContents
//@@
//@@   The data contained in a tensor represented by the repeated type
//@@   that matches the tensor's data type. Protobuf oneof is not used
//@@   because oneofs cannot contain repeated fields.
//@@
type InferTensorContents struct {
	//@@
	//@@  .. cpp:var:: bool bool_contents (repeated)
	//@@
	//@@     Representation for BOOL data type. The size must match what is
	//@@     expected by the tensor's shape. The contents must be the flattened,
	//@@     one-dimensional, row-major order of the tensor elements.
	//@@
	BoolContents []bool `protobuf:"varint,1,rep,packed,name=bool_contents,json=boolContents,proto3" json:"bool_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: int32 int_contents (repeated)
	//@@
	//@@     Representation for INT8, INT16, and INT32 data types. The size
	//@@     must match what is expected by the tensor's shape. The contents
	//@@     must be the flattened, one-dimensional, row-major order of the
	//@@     tensor elements.
	//@@
	IntContents []int32 `protobuf:"varint,2,rep,packed,name=int_contents,json=intContents,proto3" json:"int_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: int64 int64_contents (repeated)
	//@@
	//@@     Representation for INT64 data types. The size must match what
	//@@     is expected by the tensor's shape. The contents must be the
	//@@     flattened, one-dimensional, row-major order of the tensor elements.
	//@@
	Int64Contents []int64 `protobuf:"varint,3,rep,packed,name=int64_contents,json=int64Contents,proto3" json:"int64_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: uint32 uint_contents (repeated)
	//@@
	//@@     Representation for UINT8, UINT16, and UINT32 data types. The size
	//@@     must match what is expected by the tensor's shape. The contents
	//@@     must be the flattened, one-dimensional, row-major order of the
	//@@     tensor elements.
	//@@
	UintContents []uint32 `protobuf:"varint,4,rep,packed,name=uint_contents,json=uintContents,proto3" json:"uint_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: uint64 uint64_contents (repeated)
	//@@
	//@@     Representation for UINT64 data types. The size must match what
	//@@     is expected by the tensor's shape. The contents must be the
	//@@     flattened, one-dimensional, row-major order of the tensor elements.
	//@@
	Uint64Contents []uint64 `protobuf:"varint,5,rep,packed,name=uint64_contents,json=uint64Contents,proto3" json:"uint64_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: float fp32_contents (repeated)
	//@@
	//@@     Representation for FP32 data type. The size must match what is
	//@@     expected by the tensor's shape. The contents must be the flattened,
	//@@     one-dimensional, row-major order of the tensor elements.
	//@@
	Fp32Contents []float32 `protobuf:"fixed32,6,rep,packed,name=fp32_contents,json=fp32Contents,proto3" json:"fp32_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: double fp64_contents (repeated)
	//@@
	//@@     Representation for FP64 data type. The size must match what is
	//@@     expected by the tensor's shape. The contents must be the flattened,
	//@@     one-dimensional, row-major order of the tensor elements.
	//@@
	Fp64Contents []float64 `protobuf:"fixed64,7,rep,packed,name=fp64_contents,json=fp64Contents,proto3" json:"fp64_contents,omitempty"`
	//@@
	//@@  .. cpp:var:: bytes byte_contents (repeated)
	//@@
	//@@     Representation for BYTES data type. The size must match what is
	//@@     expected by the tensor's shape. The contents must be the flattened,
	//@@     one-dimensional, row-major order of the tensor elements.
	//@@
	ByteContents         [][]byte `protobuf:"bytes,8,rep,name=byte_contents,json=byteContents,proto3" json:"byte_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferTensorContents) Reset()         { *m = InferTensorContents{} }
func (m *InferTensorContents) String() string { return proto.CompactTextString(m) }
func (*InferTensorContents) ProtoMessage()    {}
func (*InferTensorContents) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{11}
}

func (m *InferTensorContents) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferTensorContents.Unmarshal(m, b)
}
func (m *InferTensorContents) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferTensorContents.Marshal(b, m, deterministic)
}
func (m *InferTensorContents) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferTensorContents.Merge(m, src)
}
func (m *InferTensorContents) XXX_Size() int {
	return xxx_messageInfo_InferTensorContents.Size(m)
}
func (m *InferTensorContents) XXX_DiscardUnknown() {
	xxx_messageInfo_InferTensorContents.DiscardUnknown(m)
}

var xxx_messageInfo_InferTensorContents proto.InternalMessageInfo

func (m *InferTensorContents) GetBoolContents() []bool {
	if m != nil {
		return m.BoolContents
	}
	return nil
}

func (m *InferTensorContents) GetIntContents() []int32 {
	if m != nil {
		return m.IntContents
	}
	return nil
}

func (m *InferTensorContents) GetInt64Contents() []int64 {
	if m != nil {
		return m.Int64Contents
	}
	return nil
}

func (m *InferTensorContents) GetUintContents() []uint32 {
	if m != nil {
		return m.UintContents
	}
	return nil
}

func (m *InferTensorContents) GetUint64Contents() []uint64 {
	if m != nil {
		return m.Uint64Contents
	}
	return nil
}

func (m *InferTensorContents) GetFp32Contents() []float32 {
	if m != nil {
		return m.Fp32Contents
	}
	return nil
}

func (m *InferTensorContents) GetFp64Contents() []float64 {
	if m != nil {
		return m.Fp64Contents
	}
	return nil
}

func (m *InferTensorContents) GetByteContents() [][]byte {
	if m != nil {
		return m.ByteContents
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelInferRequest
//@@
//@@   Request message for ModelInfer.
//@@
type ModelInferRequest struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model to use for inferencing.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: string model_version
	//@@
	//@@     The version of the model to use for inference. If not
	//@@     given the latest/most-recent version of the model is used.
	//@@
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: string id
	//@@
	//@@     Optional identifier for the request. If specified will be
	//@@     returned in the response.
	//@@
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	//@@  .. cpp:var:: map<string,InferParameter> parameters
	//@@
	//@@     Optional inference parameters.
	//@@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@
	//@@  .. cpp:var:: InferInputTensor inputs (repeated)
	//@@
	//@@     The input tensors for the inference.
	//@@
	Inputs []*ModelInferRequest_InferInputTensor `protobuf:"bytes,5,rep,name=inputs,proto3" json:"inputs,omitempty"`
	//@@
	//@@  .. cpp:var:: InferRequestedOutputTensor outputs (repeated)
	//@@
	//@@     The requested output tensors for the inference. Optional, if not
	//@@     specified all outputs specified in the model config will be
	//@@     returned.
	//@@
	Outputs []*ModelInferRequest_InferRequestedOutputTensor `protobuf:"bytes,6,rep,name=outputs,proto3" json:"outputs,omitempty"`
	//@@
	//@@  .. cpp:var:: bytes raw_input_contents
	//@@
	//@@     The data contained in an input tensor can be represented in
	//@@     "raw" bytes form or in the repeated type that matches the
	//@@     tensor's data type. Using the "raw" bytes form will
	//@@     typically allow higher performance due to the way protobuf
	//@@     allocation and reuse interacts with GRPC. For example, see
	//@@     https://github.com/grpc/grpc/issues/23231.
	//@@
	//@@     To use the raw representation 'raw_input_contents' must be
	//@@     initialized with data for each tensor in the same order as
	//@@     'inputs'. For each tensor, the size of this content must
	//@@     match what is expected by the tensor's shape and data
	//@@     type. The raw data must be the flattened, one-dimensional,
	//@@     row-major order of the tensor elements without any stride
	//@@     or padding between the elements. Note that the FP16 data
	//@@     type must be represented as raw content as there is no
	//@@     specific data type for a 16-bit float type.
	//@@
	//@@     If this field is specified then InferInputTensor::contents
	//@@     must not be specified for any input tensor.
	//@@
	RawInputContents     [][]byte `protobuf:"bytes,7,rep,name=raw_input_contents,json=rawInputContents,proto3" json:"raw_input_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInferRequest) Reset()         { *m = ModelInferRequest{} }
func (m *ModelInferRequest) String() string { return proto.CompactTextString(m) }
func (*ModelInferRequest) ProtoMessage()    {}
func (*ModelInferRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{12}
}

func (m *ModelInferRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest.Unmarshal(m, b)
}
func (m *ModelInferRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest.Merge(m, src)
}
func (m *ModelInferRequest) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest.Size(m)
}
func (m *ModelInferRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest proto.InternalMessageInfo

func (m *ModelInferRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelInferRequest) GetModelVersion() string {
	if m != nil {
		return m.ModelVersion
	}
	return ""
}

func (m *ModelInferRequest) GetId() string {
	if m != nil {
		return m.Id
	}
	return ""
}

func (m *ModelInferRequest) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferRequest) GetInputs() []*ModelInferRequest_InferInputTensor {
	if m != nil {
		return m.Inputs
	}
	return nil
}

func (m *ModelInferRequest) GetOutputs() []*ModelInferRequest_InferRequestedOutputTensor {
	if m != nil {
		return m.Outputs
	}
	return nil
}

func (m *ModelInferRequest) GetRawInputContents() [][]byte {
	if m != nil {
		return m.RawInputContents
	}
	return nil
}

//@@
//@@  .. cpp:var:: message InferInputTensor
//@@
//@@     An input tensor for an inference request.
//@@
type ModelInferRequest_InferInputTensor struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The tensor name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@
	//@@    .. cpp:var:: string datatype
	//@@
	//@@       The tensor data type.
	//@@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	//@@
	//@@    .. cpp:var:: int64 shape (repeated)
	//@@
	//@@       The tensor shape.
	//@@
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	//@@    .. cpp:var:: map<string,InferParameter> parameters
	//@@
	//@@       Optional inference input tensor parameters.
	//@@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@    .. cpp:var:: InferTensorContents contents
	//@@
	//@@       The tensor contents using a data-type format. This field
	//@@       must not be specified if tensor contents are being specified
	//@@       in ModelInferRequest.raw_input_contents.
	//@@
	Contents             *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *ModelInferRequest_InferInputTensor) Reset()         { *m = ModelInferRequest_InferInputTensor{} }
func (m *ModelInferRequest_InferInputTensor) String() string { return proto.CompactTextString(m) }
func (*ModelInferRequest_InferInputTensor) ProtoMessage()    {}
func (*ModelInferRequest_InferInputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{12, 0}
}

func (m *ModelInferRequest_InferInputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Unmarshal(m, b)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest_InferInputTensor.Merge(m, src)
}
func (m *ModelInferRequest_InferInputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest_InferInputTensor.Size(m)
}
func (m *ModelInferRequest_InferInputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest_InferInputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest_InferInputTensor proto.InternalMessageInfo

func (m *ModelInferRequest_InferInputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferRequest_InferInputTensor) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelInferRequest_InferInputTensor) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

func (m *ModelInferRequest_InferInputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferRequest_InferInputTensor) GetContents() *InferTensorContents {
	if m != nil {
		return m.Contents
	}
	return nil
}

//@@
//@@  .. cpp:var:: message InferRequestedOutputTensor
//@@
//@@     An output tensor requested for an inference request.
//@@
type ModelInferRequest_InferRequestedOutputTensor struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The tensor name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: map<string,InferParameter> parameters
	//@@
	//@@       Optional requested output tensor parameters.
	//@@
	Parameters           map[string]*InferParameter `protobuf:"bytes,2,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                   `json:"-"`
	XXX_unrecognized     []byte                     `json:"-"`
	XXX_sizecache        int32                      `json:"-"`
}

func (m *ModelInferRequest_InferRequestedOutputTensor) Reset() {
	*m = ModelInferRequest_InferRequestedOutputTensor{}
}
func (m *ModelInferRequest_InferRequestedOutputTensor) String() string {
	return proto.CompactTextString(m)
}
func (*ModelInferRequest_InferRequestedOutputTensor) ProtoMessage() {}
func (*ModelInferRequest_InferRequestedOutputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{12, 1}
}

func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Unmarshal(m, b)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Merge(m, src)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.Size(m)
}
func (m *ModelInferRequest_InferRequestedOutputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferRequest_InferRequestedOutputTensor proto.InternalMessageInfo

func (m *ModelInferRequest_InferRequestedOutputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferRequest_InferRequestedOutputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelInferResponse
//@@
//@@   Response message for ModelInfer.
//@@
type ModelInferResponse struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model used for inference.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: string model_version
	//@@
	//@@     The version of the model used for inference.
	//@@
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: string id
	//@@
	//@@     The id of the inference request if one was specified.
	//@@
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	//@@  .. cpp:var:: map<string,InferParameter> parameters
	//@@
	//@@     Optional inference response parameters.
	//@@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@
	//@@  .. cpp:var:: InferOutputTensor outputs (repeated)
	//@@
	//@@     The output tensors holding inference results.
	//@@
	Outputs []*ModelInferResponse_InferOutputTensor `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	//@@
	//@@  .. cpp:var:: bytes raw_output_contents
	//@@
	//@@     The data contained in an output tensor can be represented in
	//@@     "raw" bytes form or in the repeated type that matches the
	//@@     tensor's data type. Using the "raw" bytes form will
	//@@     typically allow higher performance due to the way protobuf
	//@@     allocation and reuse interacts with GRPC. For example, see
	//@@     https://github.com/grpc/grpc/issues/23231.
	//@@
	//@@     To use the raw representation 'raw_output_contents' must be
	//@@     initialized with data for each tensor in the same order as
	//@@     'outputs'. For each tensor, the size of this content must
	//@@     match what is expected by the tensor's shape and data
	//@@     type. The raw data must be the flattened, one-dimensional,
	//@@     row-major order of the tensor elements without any stride
	//@@     or padding between the elements. Note that the FP16 data
	//@@     type must be represented as raw content as there is no
	//@@     specific data type for a 16-bit float type.
	//@@
	//@@     If this field is specified then InferOutputTensor::contents
	//@@     must not be specified for any output tensor.
	//@@
	RawOutputContents    [][]byte `protobuf:"bytes,6,rep,name=raw_output_contents,json=rawOutputContents,proto3" json:"raw_output_contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInferResponse) Reset()         { *m = ModelInferResponse{} }
func (m *ModelInferResponse) String() string { return proto.CompactTextString(m) }
func (*ModelInferResponse) ProtoMessage()    {}
func (*ModelInferResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{13}
}

func (m *ModelInferResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferResponse.Unmarshal(m, b)
}
func (m *ModelInferResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferResponse.Marshal(b, m, deterministic)
}
func (m *ModelInferResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferResponse.Merge(m, src)
}
func (m *ModelInferResponse) XXX_Size() int {
	return xxx_messageInfo_ModelInferResponse.Size(m)
}
func (m *ModelInferResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferResponse proto.InternalMessageInfo

func (m *ModelInferResponse) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelInferResponse) GetModelVersion() string {
	if m != nil {
		return m.ModelVersion
	}
	return ""
}

func (m *ModelInferResponse) GetId() string {
	if m != nil {
		return m.Id
	}
	return ""
}

func (m *ModelInferResponse) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferResponse) GetOutputs() []*ModelInferResponse_InferOutputTensor {
	if m != nil {
		return m.Outputs
	}
	return nil
}

func (m *ModelInferResponse) GetRawOutputContents() [][]byte {
	if m != nil {
		return m.RawOutputContents
	}
	return nil
}

//@@
//@@  .. cpp:var:: message InferOutputTensor
//@@
//@@     An output tensor returned for an inference request.
//@@
type ModelInferResponse_InferOutputTensor struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The tensor name.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@
	//@@    .. cpp:var:: string datatype
	//@@
	//@@       The tensor data type.
	//@@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	//@@
	//@@    .. cpp:var:: int64 shape (repeated)
	//@@
	//@@       The tensor shape.
	//@@
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	//@@    .. cpp:var:: map<string,InferParameter> parameters
	//@@
	//@@       Optional output tensor parameters.
	//@@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@    .. cpp:var:: InferTensorContents contents
	//@@
	//@@       The tensor contents using a data-type format. This field
	//@@       must not be specified if tensor contents are being specified
	//@@       in ModelInferResponse.raw_output_contents.
	//@@
	Contents             *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	XXX_NoUnkeyedLiteral struct{}             `json:"-"`
	XXX_unrecognized     []byte               `json:"-"`
	XXX_sizecache        int32                `json:"-"`
}

func (m *ModelInferResponse_InferOutputTensor) Reset()         { *m = ModelInferResponse_InferOutputTensor{} }
func (m *ModelInferResponse_InferOutputTensor) String() string { return proto.CompactTextString(m) }
func (*ModelInferResponse_InferOutputTensor) ProtoMessage()    {}
func (*ModelInferResponse_InferOutputTensor) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{13, 0}
}

func (m *ModelInferResponse_InferOutputTensor) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Unmarshal(m, b)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Marshal(b, m, deterministic)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInferResponse_InferOutputTensor.Merge(m, src)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_Size() int {
	return xxx_messageInfo_ModelInferResponse_InferOutputTensor.Size(m)
}
func (m *ModelInferResponse_InferOutputTensor) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInferResponse_InferOutputTensor.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInferResponse_InferOutputTensor proto.InternalMessageInfo

func (m *ModelInferResponse_InferOutputTensor) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInferResponse_InferOutputTensor) GetDatatype() string {
	if m != nil {
		return m.Datatype
	}
	return ""
}

func (m *ModelInferResponse_InferOutputTensor) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

func (m *ModelInferResponse_InferOutputTensor) GetParameters() map[string]*InferParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelInferResponse_InferOutputTensor) GetContents() *InferTensorContents {
	if m != nil {
		return m.Contents
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelStreamInferResponse
//@@
//@@   Response message for ModelStreamInfer.
//@@
type ModelStreamInferResponse struct {
	//@@
	//@@  .. cpp:var:: string error_message
	//@@
	//@@     The message describing the error. The empty message
	//@@     indicates the inference was successful without errors.
	//@@
	ErrorMessage string `protobuf:"bytes,1,opt,name=error_message,json=errorMessage,proto3" json:"error_message,omitempty"`
	//@@
	//@@  .. cpp:var:: ModelInferResponse infer_response
	//@@
	//@@     Holds the results of the request.
	//@@
	InferResponse        *ModelInferResponse `protobuf:"bytes,2,opt,name=infer_response,json=inferResponse,proto3" json:"infer_response,omitempty"`
	XXX_NoUnkeyedLiteral struct{}            `json:"-"`
	XXX_unrecognized     []byte              `json:"-"`
	XXX_sizecache        int32               `json:"-"`
}

func (m *ModelStreamInferResponse) Reset()         { *m = ModelStreamInferResponse{} }
func (m *ModelStreamInferResponse) String() string { return proto.CompactTextString(m) }
func (*ModelStreamInferResponse) ProtoMessage()    {}
func (*ModelStreamInferResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{14}
}

func (m *ModelStreamInferResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelStreamInferResponse.Unmarshal(m, b)
}
func (m *ModelStreamInferResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelStreamInferResponse.Marshal(b, m, deterministic)
}
func (m *ModelStreamInferResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelStreamInferResponse.Merge(m, src)
}
func (m *ModelStreamInferResponse) XXX_Size() int {
	return xxx_messageInfo_ModelStreamInferResponse.Size(m)
}
func (m *ModelStreamInferResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelStreamInferResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelStreamInferResponse proto.InternalMessageInfo

func (m *ModelStreamInferResponse) GetErrorMessage() string {
	if m != nil {
		return m.ErrorMessage
	}
	return ""
}

func (m *ModelStreamInferResponse) GetInferResponse() *ModelInferResponse {
	if m != nil {
		return m.InferResponse
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelConfigRequest
//@@
//@@   Request message for ModelConfig.
//@@
type ModelConfigRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string version
	//@@
	//@@     The version of the model. If not given the model version
	//@@     is selected automatically based on the version policy.
	//@@
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelConfigRequest) Reset()         { *m = ModelConfigRequest{} }
func (m *ModelConfigRequest) String() string { return proto.CompactTextString(m) }
func (*ModelConfigRequest) ProtoMessage()    {}
func (*ModelConfigRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{15}
}

func (m *ModelConfigRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelConfigRequest.Unmarshal(m, b)
}
func (m *ModelConfigRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelConfigRequest.Marshal(b, m, deterministic)
}
func (m *ModelConfigRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelConfigRequest.Merge(m, src)
}
func (m *ModelConfigRequest) XXX_Size() int {
	return xxx_messageInfo_ModelConfigRequest.Size(m)
}
func (m *ModelConfigRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelConfigRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelConfigRequest proto.InternalMessageInfo

func (m *ModelConfigRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelConfigRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelConfigResponse
//@@
//@@   Response message for ModelConfig.
//@@
type ModelConfigResponse struct {
	//@@
	//@@  .. cpp:var:: ModelConfig config
	//@@
	//@@     The model configuration.
	//@@
	Config               *ModelConfig `protobuf:"bytes,1,opt,name=config,proto3" json:"config,omitempty"`
	XXX_NoUnkeyedLiteral struct{}     `json:"-"`
	XXX_unrecognized     []byte       `json:"-"`
	XXX_sizecache        int32        `json:"-"`
}

func (m *ModelConfigResponse) Reset()         { *m = ModelConfigResponse{} }
func (m *ModelConfigResponse) String() string { return proto.CompactTextString(m) }
func (*ModelConfigResponse) ProtoMessage()    {}
func (*ModelConfigResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{16}
}

func (m *ModelConfigResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelConfigResponse.Unmarshal(m, b)
}
func (m *ModelConfigResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelConfigResponse.Marshal(b, m, deterministic)
}
func (m *ModelConfigResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelConfigResponse.Merge(m, src)
}
func (m *ModelConfigResponse) XXX_Size() int {
	return xxx_messageInfo_ModelConfigResponse.Size(m)
}
func (m *ModelConfigResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelConfigResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelConfigResponse proto.InternalMessageInfo

func (m *ModelConfigResponse) GetConfig() *ModelConfig {
	if m != nil {
		return m.Config
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelStatisticsRequest
//@@
//@@   Request message for ModelStatistics.
//@@
type ModelStatisticsRequest struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model. If not given returns statistics for
	//@@     all models.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string version
	//@@
	//@@     The version of the model. If not given returns statistics for
	//@@     all model versions.
	//@@
	Version              string   `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelStatisticsRequest) Reset()         { *m = ModelStatisticsRequest{} }
func (m *ModelStatisticsRequest) String() string { return proto.CompactTextString(m) }
func (*ModelStatisticsRequest) ProtoMessage()    {}
func (*ModelStatisticsRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{17}
}

func (m *ModelStatisticsRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelStatisticsRequest.Unmarshal(m, b)
}
func (m *ModelStatisticsRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelStatisticsRequest.Marshal(b, m, deterministic)
}
func (m *ModelStatisticsRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelStatisticsRequest.Merge(m, src)
}
func (m *ModelStatisticsRequest) XXX_Size() int {
	return xxx_messageInfo_ModelStatisticsRequest.Size(m)
}
func (m *ModelStatisticsRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelStatisticsRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelStatisticsRequest proto.InternalMessageInfo

func (m *ModelStatisticsRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelStatisticsRequest) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

//@@
//@@.. cpp:var:: message StatisticDuration
//@@
//@@   Statistic recording a cumulative duration metric.
//@@
type StatisticDuration struct {
	//@@  .. cpp:var:: uint64 count
	//@@
	//@@     Cumulative number of times this metric occurred.
	//@@
	Count uint64 `protobuf:"varint,1,opt,name=count,proto3" json:"count,omitempty"`
	//@@  .. cpp:var:: uint64 total_time_ns
	//@@
	//@@     Total collected duration of this metric in nanoseconds.
	//@@
	Ns                   uint64   `protobuf:"varint,2,opt,name=ns,proto3" json:"ns,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StatisticDuration) Reset()         { *m = StatisticDuration{} }
func (m *StatisticDuration) String() string { return proto.CompactTextString(m) }
func (*StatisticDuration) ProtoMessage()    {}
func (*StatisticDuration) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{18}
}

func (m *StatisticDuration) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StatisticDuration.Unmarshal(m, b)
}
func (m *StatisticDuration) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StatisticDuration.Marshal(b, m, deterministic)
}
func (m *StatisticDuration) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StatisticDuration.Merge(m, src)
}
func (m *StatisticDuration) XXX_Size() int {
	return xxx_messageInfo_StatisticDuration.Size(m)
}
func (m *StatisticDuration) XXX_DiscardUnknown() {
	xxx_messageInfo_StatisticDuration.DiscardUnknown(m)
}

var xxx_messageInfo_StatisticDuration proto.InternalMessageInfo

func (m *StatisticDuration) GetCount() uint64 {
	if m != nil {
		return m.Count
	}
	return 0
}

func (m *StatisticDuration) GetNs() uint64 {
	if m != nil {
		return m.Ns
	}
	return 0
}

//@@
//@@.. cpp:var:: message InferStatistics
//@@
//@@   Inference statistics.
//@@
type InferStatistics struct {
	//@@  .. cpp:var:: StatisticDuration success
	//@@
	//@@     Cumulative count and duration for successful inference
	//@@     request.
	//@@
	Success *StatisticDuration `protobuf:"bytes,1,opt,name=success,proto3" json:"success,omitempty"`
	//@@  .. cpp:var:: StatisticDuration fail
	//@@
	//@@     Cumulative count and duration for failed inference
	//@@     request.
	//@@
	Fail *StatisticDuration `protobuf:"bytes,2,opt,name=fail,proto3" json:"fail,omitempty"`
	//@@  .. cpp:var:: StatisticDuration queue
	//@@
	//@@     The count and cumulative duration that inference requests wait in
	//@@     scheduling or other queues.
	//@@
	Queue *StatisticDuration `protobuf:"bytes,3,opt,name=queue,proto3" json:"queue,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_input
	//@@
	//@@    The count and cumulative duration to prepare input tensor data as
	//@@    required by the model framework / backend. For example, this duration
	//@@    should include the time to copy input tensor data to the GPU.
	//@@
	ComputeInput *StatisticDuration `protobuf:"bytes,4,opt,name=compute_input,json=computeInput,proto3" json:"compute_input,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_infer
	//@@
	//@@     The count and cumulative duration to execute the model.
	//@@
	ComputeInfer *StatisticDuration `protobuf:"bytes,5,opt,name=compute_infer,json=computeInfer,proto3" json:"compute_infer,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_output
	//@@
	//@@     The count and cumulative duration to extract output tensor data
	//@@     produced by the model framework / backend. For example, this duration
	//@@     should include the time to copy output tensor data from the GPU.
	//@@
	ComputeOutput        *StatisticDuration `protobuf:"bytes,6,opt,name=compute_output,json=computeOutput,proto3" json:"compute_output,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *InferStatistics) Reset()         { *m = InferStatistics{} }
func (m *InferStatistics) String() string { return proto.CompactTextString(m) }
func (*InferStatistics) ProtoMessage()    {}
func (*InferStatistics) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{19}
}

func (m *InferStatistics) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferStatistics.Unmarshal(m, b)
}
func (m *InferStatistics) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferStatistics.Marshal(b, m, deterministic)
}
func (m *InferStatistics) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferStatistics.Merge(m, src)
}
func (m *InferStatistics) XXX_Size() int {
	return xxx_messageInfo_InferStatistics.Size(m)
}
func (m *InferStatistics) XXX_DiscardUnknown() {
	xxx_messageInfo_InferStatistics.DiscardUnknown(m)
}

var xxx_messageInfo_InferStatistics proto.InternalMessageInfo

func (m *InferStatistics) GetSuccess() *StatisticDuration {
	if m != nil {
		return m.Success
	}
	return nil
}

func (m *InferStatistics) GetFail() *StatisticDuration {
	if m != nil {
		return m.Fail
	}
	return nil
}

func (m *InferStatistics) GetQueue() *StatisticDuration {
	if m != nil {
		return m.Queue
	}
	return nil
}

func (m *InferStatistics) GetComputeInput() *StatisticDuration {
	if m != nil {
		return m.ComputeInput
	}
	return nil
}

func (m *InferStatistics) GetComputeInfer() *StatisticDuration {
	if m != nil {
		return m.ComputeInfer
	}
	return nil
}

func (m *InferStatistics) GetComputeOutput() *StatisticDuration {
	if m != nil {
		return m.ComputeOutput
	}
	return nil
}

//@@
//@@.. cpp:var:: message InferBatchStatistics
//@@
//@@   Inference batch statistics.
//@@
type InferBatchStatistics struct {
	//@@  .. cpp:var:: uint64 batch_size
	//@@
	//@@     The size of the batch.
	//@@
	BatchSize uint64 `protobuf:"varint,1,opt,name=batch_size,json=batchSize,proto3" json:"batch_size,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_input
	//@@
	//@@     The count and cumulative duration to prepare input tensor data as
	//@@     required by the model framework / backend with the given batch size.
	//@@     For example, this duration should include the time to copy input
	//@@     tensor data to the GPU.
	//@@
	ComputeInput *StatisticDuration `protobuf:"bytes,2,opt,name=compute_input,json=computeInput,proto3" json:"compute_input,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_infer
	//@@
	//@@     The count and cumulative duration to execute the model with the given
	//@@     batch size.
	//@@
	ComputeInfer *StatisticDuration `protobuf:"bytes,3,opt,name=compute_infer,json=computeInfer,proto3" json:"compute_infer,omitempty"`
	//@@  .. cpp:var:: StatisticDuration compute_output
	//@@
	//@@     The count and cumulative duration to extract output tensor data
	//@@     produced by the model framework / backend with the given batch size.
	//@@     For example, this duration should include the time to copy output
	//@@     tensor data from the GPU.
	//@@
	ComputeOutput        *StatisticDuration `protobuf:"bytes,4,opt,name=compute_output,json=computeOutput,proto3" json:"compute_output,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *InferBatchStatistics) Reset()         { *m = InferBatchStatistics{} }
func (m *InferBatchStatistics) String() string { return proto.CompactTextString(m) }
func (*InferBatchStatistics) ProtoMessage()    {}
func (*InferBatchStatistics) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{20}
}

func (m *InferBatchStatistics) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferBatchStatistics.Unmarshal(m, b)
}
func (m *InferBatchStatistics) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferBatchStatistics.Marshal(b, m, deterministic)
}
func (m *InferBatchStatistics) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferBatchStatistics.Merge(m, src)
}
func (m *InferBatchStatistics) XXX_Size() int {
	return xxx_messageInfo_InferBatchStatistics.Size(m)
}
func (m *InferBatchStatistics) XXX_DiscardUnknown() {
	xxx_messageInfo_InferBatchStatistics.DiscardUnknown(m)
}

var xxx_messageInfo_InferBatchStatistics proto.InternalMessageInfo

func (m *InferBatchStatistics) GetBatchSize() uint64 {
	if m != nil {
		return m.BatchSize
	}
	return 0
}

func (m *InferBatchStatistics) GetComputeInput() *StatisticDuration {
	if m != nil {
		return m.ComputeInput
	}
	return nil
}

func (m *InferBatchStatistics) GetComputeInfer() *StatisticDuration {
	if m != nil {
		return m.ComputeInfer
	}
	return nil
}

func (m *InferBatchStatistics) GetComputeOutput() *StatisticDuration {
	if m != nil {
		return m.ComputeOutput
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelStatistics
//@@
//@@   Statistics for a specific model and version.
//@@
type ModelStatistics struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model. If not given returns statistics for all
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string version
	//@@
	//@@     The version of the model.
	//@@
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	//@@  .. cpp:var:: uint64 last_inference
	//@@
	//@@     The timestamp of the last inference request made for this model,
	//@@     as milliseconds since the epoch.
	//@@
	LastInference uint64 `protobuf:"varint,3,opt,name=last_inference,json=lastInference,proto3" json:"last_inference,omitempty"`
	//@@  .. cpp:var:: uint64 last_inference
	//@@
	//@@     The cumulative count of successful inference requests made for this
	//@@     model. Each inference in a batched request is counted as an
	//@@     individual inference. For example, if a client sends a single
	//@@     inference request with batch size 64, "inference_count" will be
	//@@     incremented by 64. Similarly, if a clients sends 64 individual
	//@@     requests each with batch size 1, "inference_count" will be
	//@@     incremented by 64.
	//@@
	InferenceCount uint64 `protobuf:"varint,4,opt,name=inference_count,json=inferenceCount,proto3" json:"inference_count,omitempty"`
	//@@  .. cpp:var:: uint64 last_inference
	//@@
	//@@     The cumulative count of the number of successful inference executions
	//@@     performed for the model. When dynamic batching is enabled, a single
	//@@     model execution can perform inferencing for more than one inference
	//@@     request. For example, if a clients sends 64 individual requests each
	//@@     with batch size 1 and the dynamic batcher batches them into a single
	//@@     large batch for model execution then "execution_count" will be
	//@@     incremented by 1. If, on the other hand, the dynamic batcher is not
	//@@     enabled for that each of the 64 individual requests is executed
	//@@     independently, then "execution_count" will be incremented by 64.
	//@@
	ExecutionCount uint64 `protobuf:"varint,5,opt,name=execution_count,json=executionCount,proto3" json:"execution_count,omitempty"`
	//@@  .. cpp:var:: InferStatistics inference_stats
	//@@
	//@@     The aggregate statistics for the model/version.
	//@@
	InferenceStats *InferStatistics `protobuf:"bytes,6,opt,name=inference_stats,json=inferenceStats,proto3" json:"inference_stats,omitempty"`
	//@@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
	//@@
	//@@     The aggregate statistics for each different batch size that is
	//@@     executed in the model. The batch statistics indicate how many actual
	//@@     model executions were performed and show differences due to different
	//@@     batch size (for example, larger batches typically take longer to
	//@@     compute).
	//@@
	BatchStats           []*InferBatchStatistics `protobuf:"bytes,7,rep,name=batch_stats,json=batchStats,proto3" json:"batch_stats,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                `json:"-"`
	XXX_unrecognized     []byte                  `json:"-"`
	XXX_sizecache        int32                   `json:"-"`
}

func (m *ModelStatistics) Reset()         { *m = ModelStatistics{} }
func (m *ModelStatistics) String() string { return proto.CompactTextString(m) }
func (*ModelStatistics) ProtoMessage()    {}
func (*ModelStatistics) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{21}
}

func (m *ModelStatistics) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelStatistics.Unmarshal(m, b)
}
func (m *ModelStatistics) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelStatistics.Marshal(b, m, deterministic)
}
func (m *ModelStatistics) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelStatistics.Merge(m, src)
}
func (m *ModelStatistics) XXX_Size() int {
	return xxx_messageInfo_ModelStatistics.Size(m)
}
func (m *ModelStatistics) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelStatistics.DiscardUnknown(m)
}

var xxx_messageInfo_ModelStatistics proto.InternalMessageInfo

func (m *ModelStatistics) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelStatistics) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

func (m *ModelStatistics) GetLastInference() uint64 {
	if m != nil {
		return m.LastInference
	}
	return 0
}

func (m *ModelStatistics) GetInferenceCount() uint64 {
	if m != nil {
		return m.InferenceCount
	}
	return 0
}

func (m *ModelStatistics) GetExecutionCount() uint64 {
	if m != nil {
		return m.ExecutionCount
	}
	return 0
}

func (m *ModelStatistics) GetInferenceStats() *InferStatistics {
	if m != nil {
		return m.InferenceStats
	}
	return nil
}

func (m *ModelStatistics) GetBatchStats() []*InferBatchStatistics {
	if m != nil {
		return m.BatchStats
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelStatisticsResponse
//@@
//@@   Response message for ModelStatistics.
//@@
type ModelStatisticsResponse struct {
	//@@  .. cpp:var:: ModelStatistics model_stats (repeated)
	//@@
	//@@     Statistics for each requested model.
	//@@
	ModelStats           []*ModelStatistics `protobuf:"bytes,1,rep,name=model_stats,json=modelStats,proto3" json:"model_stats,omitempty"`
	XXX_NoUnkeyedLiteral struct{}           `json:"-"`
	XXX_unrecognized     []byte             `json:"-"`
	XXX_sizecache        int32              `json:"-"`
}

func (m *ModelStatisticsResponse) Reset()         { *m = ModelStatisticsResponse{} }
func (m *ModelStatisticsResponse) String() string { return proto.CompactTextString(m) }
func (*ModelStatisticsResponse) ProtoMessage()    {}
func (*ModelStatisticsResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{22}
}

func (m *ModelStatisticsResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelStatisticsResponse.Unmarshal(m, b)
}
func (m *ModelStatisticsResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelStatisticsResponse.Marshal(b, m, deterministic)
}
func (m *ModelStatisticsResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelStatisticsResponse.Merge(m, src)
}
func (m *ModelStatisticsResponse) XXX_Size() int {
	return xxx_messageInfo_ModelStatisticsResponse.Size(m)
}
func (m *ModelStatisticsResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelStatisticsResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ModelStatisticsResponse proto.InternalMessageInfo

func (m *ModelStatisticsResponse) GetModelStats() []*ModelStatistics {
	if m != nil {
		return m.ModelStats
	}
	return nil
}

//@@
//@@.. cpp:var:: message RepositoryIndexRequest
//@@
//@@   Request message for RepositoryIndex.
//@@
type RepositoryIndexRequest struct {
	//@@  .. cpp:var:: string repository_name
	//@@
	//@@     The name of the repository. If empty the index is returned
	//@@     for all repositories.
	//@@
	RepositoryName string `protobuf:"bytes,1,opt,name=repository_name,json=repositoryName,proto3" json:"repository_name,omitempty"`
	//@@  .. cpp:var:: bool ready
	//@@
	//@@     If true returned only models currently ready for inferencing.
	//@@
	Ready                bool     `protobuf:"varint,2,opt,name=ready,proto3" json:"ready,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryIndexRequest) Reset()         { *m = RepositoryIndexRequest{} }
func (m *RepositoryIndexRequest) String() string { return proto.CompactTextString(m) }
func (*RepositoryIndexRequest) ProtoMessage()    {}
func (*RepositoryIndexRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{23}
}

func (m *RepositoryIndexRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryIndexRequest.Unmarshal(m, b)
}
func (m *RepositoryIndexRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryIndexRequest.Marshal(b, m, deterministic)
}
func (m *RepositoryIndexRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryIndexRequest.Merge(m, src)
}
func (m *RepositoryIndexRequest) XXX_Size() int {
	return xxx_messageInfo_RepositoryIndexRequest.Size(m)
}
func (m *RepositoryIndexRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryIndexRequest.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryIndexRequest proto.InternalMessageInfo

func (m *RepositoryIndexRequest) GetRepositoryName() string {
	if m != nil {
		return m.RepositoryName
	}
	return ""
}

func (m *RepositoryIndexRequest) GetReady() bool {
	if m != nil {
		return m.Ready
	}
	return false
}

//@@
//@@.. cpp:var:: message RepositoryIndexResponse
//@@
//@@   Response message for RepositoryIndex.
//@@
type RepositoryIndexResponse struct {
	//@@
	//@@  .. cpp:var:: ModelIndex models (repeated)
	//@@
	//@@     An index entry for each model.
	//@@
	Models               []*RepositoryIndexResponse_ModelIndex `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                              `json:"-"`
	XXX_unrecognized     []byte                                `json:"-"`
	XXX_sizecache        int32                                 `json:"-"`
}

func (m *RepositoryIndexResponse) Reset()         { *m = RepositoryIndexResponse{} }
func (m *RepositoryIndexResponse) String() string { return proto.CompactTextString(m) }
func (*RepositoryIndexResponse) ProtoMessage()    {}
func (*RepositoryIndexResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{24}
}

func (m *RepositoryIndexResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryIndexResponse.Unmarshal(m, b)
}
func (m *RepositoryIndexResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryIndexResponse.Marshal(b, m, deterministic)
}
func (m *RepositoryIndexResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryIndexResponse.Merge(m, src)
}
func (m *RepositoryIndexResponse) XXX_Size() int {
	return xxx_messageInfo_RepositoryIndexResponse.Size(m)
}
func (m *RepositoryIndexResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryIndexResponse.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryIndexResponse proto.InternalMessageInfo

func (m *RepositoryIndexResponse) GetModels() []*RepositoryIndexResponse_ModelIndex {
	if m != nil {
		return m.Models
	}
	return nil
}

//@@
//@@  .. cpp:var:: message ModelIndex
//@@
//@@     Index entry for a model.
//@@
type RepositoryIndexResponse_ModelIndex struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name of the model.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: string version
	//@@
	//@@       The version of the model.
	//@@
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	//@@
	//@@    .. cpp:var:: string state
	//@@
	//@@       The state of the model.
	//@@
	State string `protobuf:"bytes,3,opt,name=state,proto3" json:"state,omitempty"`
	//@@
	//@@    .. cpp:var:: string reason
	//@@
	//@@       The reason, if any, that the model is in the given state.
	//@@
	Reason               string   `protobuf:"bytes,4,opt,name=reason,proto3" json:"reason,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryIndexResponse_ModelIndex) Reset()         { *m = RepositoryIndexResponse_ModelIndex{} }
func (m *RepositoryIndexResponse_ModelIndex) String() string { return proto.CompactTextString(m) }
func (*RepositoryIndexResponse_ModelIndex) ProtoMessage()    {}
func (*RepositoryIndexResponse_ModelIndex) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{24, 0}
}

func (m *RepositoryIndexResponse_ModelIndex) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryIndexResponse_ModelIndex.Unmarshal(m, b)
}
func (m *RepositoryIndexResponse_ModelIndex) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryIndexResponse_ModelIndex.Marshal(b, m, deterministic)
}
func (m *RepositoryIndexResponse_ModelIndex) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryIndexResponse_ModelIndex.Merge(m, src)
}
func (m *RepositoryIndexResponse_ModelIndex) XXX_Size() int {
	return xxx_messageInfo_RepositoryIndexResponse_ModelIndex.Size(m)
}
func (m *RepositoryIndexResponse_ModelIndex) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryIndexResponse_ModelIndex.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryIndexResponse_ModelIndex proto.InternalMessageInfo

func (m *RepositoryIndexResponse_ModelIndex) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *RepositoryIndexResponse_ModelIndex) GetVersion() string {
	if m != nil {
		return m.Version
	}
	return ""
}

func (m *RepositoryIndexResponse_ModelIndex) GetState() string {
	if m != nil {
		return m.State
	}
	return ""
}

func (m *RepositoryIndexResponse_ModelIndex) GetReason() string {
	if m != nil {
		return m.Reason
	}
	return ""
}

//@@
//@@.. cpp:var:: message RepositoryModelLoadRequest
//@@
//@@   Request message for RepositoryModelLoad.
//@@
type RepositoryModelLoadRequest struct {
	//@@  .. cpp:var:: string repository_name
	//@@
	//@@     The name of the repository to load from. If empty the model
	//@@     is loaded from any repository.
	//@@
	RepositoryName string `protobuf:"bytes,1,opt,name=repository_name,json=repositoryName,proto3" json:"repository_name,omitempty"`
	//@@  .. cpp:var:: string repository_name
	//@@
	//@@     The name of the model to load, or reload.
	//@@
	ModelName            string   `protobuf:"bytes,2,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelLoadRequest) Reset()         { *m = RepositoryModelLoadRequest{} }
func (m *RepositoryModelLoadRequest) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelLoadRequest) ProtoMessage()    {}
func (*RepositoryModelLoadRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{25}
}

func (m *RepositoryModelLoadRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelLoadRequest.Unmarshal(m, b)
}
func (m *RepositoryModelLoadRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelLoadRequest.Marshal(b, m, deterministic)
}
func (m *RepositoryModelLoadRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelLoadRequest.Merge(m, src)
}
func (m *RepositoryModelLoadRequest) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelLoadRequest.Size(m)
}
func (m *RepositoryModelLoadRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelLoadRequest.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelLoadRequest proto.InternalMessageInfo

func (m *RepositoryModelLoadRequest) GetRepositoryName() string {
	if m != nil {
		return m.RepositoryName
	}
	return ""
}

func (m *RepositoryModelLoadRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

//@@
//@@.. cpp:var:: message RepositoryModelLoadResponse
//@@
//@@   Response message for RepositoryModelLoad.
//@@
type RepositoryModelLoadResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelLoadResponse) Reset()         { *m = RepositoryModelLoadResponse{} }
func (m *RepositoryModelLoadResponse) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelLoadResponse) ProtoMessage()    {}
func (*RepositoryModelLoadResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{26}
}

func (m *RepositoryModelLoadResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelLoadResponse.Unmarshal(m, b)
}
func (m *RepositoryModelLoadResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelLoadResponse.Marshal(b, m, deterministic)
}
func (m *RepositoryModelLoadResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelLoadResponse.Merge(m, src)
}
func (m *RepositoryModelLoadResponse) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelLoadResponse.Size(m)
}
func (m *RepositoryModelLoadResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelLoadResponse.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelLoadResponse proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message RepositoryModelUnloadRequest
//@@
//@@   Request message for RepositoryModelUnload.
//@@
type RepositoryModelUnloadRequest struct {
	//@@  .. cpp:var:: string repository_name
	//@@
	//@@     The name of the repository from which the model was originally
	//@@     loaded. If empty the repository is not considered.
	//@@
	RepositoryName string `protobuf:"bytes,1,opt,name=repository_name,json=repositoryName,proto3" json:"repository_name,omitempty"`
	//@@  .. cpp:var:: string repository_name
	//@@
	//@@     The name of the model to unload.
	//@@
	ModelName            string   `protobuf:"bytes,2,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelUnloadRequest) Reset()         { *m = RepositoryModelUnloadRequest{} }
func (m *RepositoryModelUnloadRequest) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelUnloadRequest) ProtoMessage()    {}
func (*RepositoryModelUnloadRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{27}
}

func (m *RepositoryModelUnloadRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Unmarshal(m, b)
}
func (m *RepositoryModelUnloadRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Marshal(b, m, deterministic)
}
func (m *RepositoryModelUnloadRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelUnloadRequest.Merge(m, src)
}
func (m *RepositoryModelUnloadRequest) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelUnloadRequest.Size(m)
}
func (m *RepositoryModelUnloadRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelUnloadRequest.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelUnloadRequest proto.InternalMessageInfo

func (m *RepositoryModelUnloadRequest) GetRepositoryName() string {
	if m != nil {
		return m.RepositoryName
	}
	return ""
}

func (m *RepositoryModelUnloadRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

//@@
//@@.. cpp:var:: message RepositoryModelUnloadResponse
//@@
//@@   Response message for RepositoryModelUnload.
//@@
type RepositoryModelUnloadResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *RepositoryModelUnloadResponse) Reset()         { *m = RepositoryModelUnloadResponse{} }
func (m *RepositoryModelUnloadResponse) String() string { return proto.CompactTextString(m) }
func (*RepositoryModelUnloadResponse) ProtoMessage()    {}
func (*RepositoryModelUnloadResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{28}
}

func (m *RepositoryModelUnloadResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Unmarshal(m, b)
}
func (m *RepositoryModelUnloadResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Marshal(b, m, deterministic)
}
func (m *RepositoryModelUnloadResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RepositoryModelUnloadResponse.Merge(m, src)
}
func (m *RepositoryModelUnloadResponse) XXX_Size() int {
	return xxx_messageInfo_RepositoryModelUnloadResponse.Size(m)
}
func (m *RepositoryModelUnloadResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_RepositoryModelUnloadResponse.DiscardUnknown(m)
}

var xxx_messageInfo_RepositoryModelUnloadResponse proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message SystemSharedMemoryStatusRequest
//@@
//@@   Request message for SystemSharedMemoryStatus.
//@@
type SystemSharedMemoryStatusRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the region to get status for. If empty the
	//@@     status is returned for all registered regions.
	//@@
	Name                 string   `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryStatusRequest) Reset()         { *m = SystemSharedMemoryStatusRequest{} }
func (m *SystemSharedMemoryStatusRequest) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryStatusRequest) ProtoMessage()    {}
func (*SystemSharedMemoryStatusRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{29}
}

func (m *SystemSharedMemoryStatusRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryStatusRequest.Unmarshal(m, b)
}
func (m *SystemSharedMemoryStatusRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryStatusRequest.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryStatusRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryStatusRequest.Merge(m, src)
}
func (m *SystemSharedMemoryStatusRequest) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryStatusRequest.Size(m)
}
func (m *SystemSharedMemoryStatusRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryStatusRequest.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryStatusRequest proto.InternalMessageInfo

func (m *SystemSharedMemoryStatusRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

//@@
//@@.. cpp:var:: message SystemSharedMemoryStatusResponse
//@@
//@@   Response message for SystemSharedMemoryStatus.
//@@
type SystemSharedMemoryStatusResponse struct {
	//@@
	//@@  .. cpp:var:: map<string,RegionStatus> regions
	//@@
	//@@     Status for each of the registered regions, indexed by
	//@@     region name.
	//@@
	Regions              map[string]*SystemSharedMemoryStatusResponse_RegionStatus `protobuf:"bytes,1,rep,name=regions,proto3" json:"regions,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                                                  `json:"-"`
	XXX_unrecognized     []byte                                                    `json:"-"`
	XXX_sizecache        int32                                                     `json:"-"`
}

func (m *SystemSharedMemoryStatusResponse) Reset()         { *m = SystemSharedMemoryStatusResponse{} }
func (m *SystemSharedMemoryStatusResponse) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryStatusResponse) ProtoMessage()    {}
func (*SystemSharedMemoryStatusResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{30}
}

func (m *SystemSharedMemoryStatusResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse.Unmarshal(m, b)
}
func (m *SystemSharedMemoryStatusResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryStatusResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryStatusResponse.Merge(m, src)
}
func (m *SystemSharedMemoryStatusResponse) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse.Size(m)
}
func (m *SystemSharedMemoryStatusResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryStatusResponse.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryStatusResponse proto.InternalMessageInfo

func (m *SystemSharedMemoryStatusResponse) GetRegions() map[string]*SystemSharedMemoryStatusResponse_RegionStatus {
	if m != nil {
		return m.Regions
	}
	return nil
}

//@@
//@@  .. cpp:var:: message RegionStatus
//@@
//@@     Status for a shared memory region.
//@@
type SystemSharedMemoryStatusResponse_RegionStatus struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name for the shared memory region.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: string shared_memory_key
	//@@
	//@@       The key of the underlying memory object that contains the
	//@@       shared memory region.
	//@@
	Key string `protobuf:"bytes,2,opt,name=key,proto3" json:"key,omitempty"`
	//@@    .. cpp:var:: uint64 offset
	//@@
	//@@       Offset, in bytes, within the underlying memory object to
	//@@       the start of the shared memory region.
	//@@
	Offset uint64 `protobuf:"varint,3,opt,name=offset,proto3" json:"offset,omitempty"`
	//@@    .. cpp:var:: uint64 byte_size
	//@@
	//@@       Size of the shared memory region, in bytes.
	//@@
	ByteSize             uint64   `protobuf:"varint,4,opt,name=byte_size,json=byteSize,proto3" json:"byte_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryStatusResponse_RegionStatus) Reset() {
	*m = SystemSharedMemoryStatusResponse_RegionStatus{}
}
func (m *SystemSharedMemoryStatusResponse_RegionStatus) String() string {
	return proto.CompactTextString(m)
}
func (*SystemSharedMemoryStatusResponse_RegionStatus) ProtoMessage() {}
func (*SystemSharedMemoryStatusResponse_RegionStatus) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{30, 0}
}

func (m *SystemSharedMemoryStatusResponse_RegionStatus) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus.Unmarshal(m, b)
}
func (m *SystemSharedMemoryStatusResponse_RegionStatus) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryStatusResponse_RegionStatus) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus.Merge(m, src)
}
func (m *SystemSharedMemoryStatusResponse_RegionStatus) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus.Size(m)
}
func (m *SystemSharedMemoryStatusResponse_RegionStatus) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryStatusResponse_RegionStatus proto.InternalMessageInfo

func (m *SystemSharedMemoryStatusResponse_RegionStatus) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *SystemSharedMemoryStatusResponse_RegionStatus) GetKey() string {
	if m != nil {
		return m.Key
	}
	return ""
}

func (m *SystemSharedMemoryStatusResponse_RegionStatus) GetOffset() uint64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

func (m *SystemSharedMemoryStatusResponse_RegionStatus) GetByteSize() uint64 {
	if m != nil {
		return m.ByteSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message SystemSharedMemoryRegisterRequest
//@@
//@@   Request message for SystemSharedMemoryRegister.
//@@
type SystemSharedMemoryRegisterRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the region to register.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string shared_memory_key
	//@@
	//@@     The key of the underlying memory object that contains the
	//@@     shared memory region.
	//@@
	Key string `protobuf:"bytes,2,opt,name=key,proto3" json:"key,omitempty"`
	//@@  .. cpp:var:: uint64 offset
	//@@
	//@@     Offset, in bytes, within the underlying memory object to
	//@@     the start of the shared memory region.
	//@@
	Offset uint64 `protobuf:"varint,3,opt,name=offset,proto3" json:"offset,omitempty"`
	//@@  .. cpp:var:: uint64 byte_size
	//@@
	//@@     Size of the shared memory region, in bytes.
	//@@
	ByteSize             uint64   `protobuf:"varint,4,opt,name=byte_size,json=byteSize,proto3" json:"byte_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryRegisterRequest) Reset()         { *m = SystemSharedMemoryRegisterRequest{} }
func (m *SystemSharedMemoryRegisterRequest) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryRegisterRequest) ProtoMessage()    {}
func (*SystemSharedMemoryRegisterRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{31}
}

func (m *SystemSharedMemoryRegisterRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryRegisterRequest.Unmarshal(m, b)
}
func (m *SystemSharedMemoryRegisterRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryRegisterRequest.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryRegisterRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryRegisterRequest.Merge(m, src)
}
func (m *SystemSharedMemoryRegisterRequest) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryRegisterRequest.Size(m)
}
func (m *SystemSharedMemoryRegisterRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryRegisterRequest.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryRegisterRequest proto.InternalMessageInfo

func (m *SystemSharedMemoryRegisterRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *SystemSharedMemoryRegisterRequest) GetKey() string {
	if m != nil {
		return m.Key
	}
	return ""
}

func (m *SystemSharedMemoryRegisterRequest) GetOffset() uint64 {
	if m != nil {
		return m.Offset
	}
	return 0
}

func (m *SystemSharedMemoryRegisterRequest) GetByteSize() uint64 {
	if m != nil {
		return m.ByteSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message SystemSharedMemoryRegisterResponse
//@@
//@@   Response message for SystemSharedMemoryRegister.
//@@
type SystemSharedMemoryRegisterResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryRegisterResponse) Reset()         { *m = SystemSharedMemoryRegisterResponse{} }
func (m *SystemSharedMemoryRegisterResponse) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryRegisterResponse) ProtoMessage()    {}
func (*SystemSharedMemoryRegisterResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{32}
}

func (m *SystemSharedMemoryRegisterResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryRegisterResponse.Unmarshal(m, b)
}
func (m *SystemSharedMemoryRegisterResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryRegisterResponse.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryRegisterResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryRegisterResponse.Merge(m, src)
}
func (m *SystemSharedMemoryRegisterResponse) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryRegisterResponse.Size(m)
}
func (m *SystemSharedMemoryRegisterResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryRegisterResponse.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryRegisterResponse proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message SystemSharedMemoryUnregisterRequest
//@@
//@@   Request message for SystemSharedMemoryUnregister.
//@@
type SystemSharedMemoryUnregisterRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the system region to unregister. If empty
	//@@     all system shared-memory regions are unregistered.
	//@@
	Name                 string   `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryUnregisterRequest) Reset()         { *m = SystemSharedMemoryUnregisterRequest{} }
func (m *SystemSharedMemoryUnregisterRequest) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryUnregisterRequest) ProtoMessage()    {}
func (*SystemSharedMemoryUnregisterRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{33}
}

func (m *SystemSharedMemoryUnregisterRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryUnregisterRequest.Unmarshal(m, b)
}
func (m *SystemSharedMemoryUnregisterRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryUnregisterRequest.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryUnregisterRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryUnregisterRequest.Merge(m, src)
}
func (m *SystemSharedMemoryUnregisterRequest) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryUnregisterRequest.Size(m)
}
func (m *SystemSharedMemoryUnregisterRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryUnregisterRequest.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryUnregisterRequest proto.InternalMessageInfo

func (m *SystemSharedMemoryUnregisterRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

//@@
//@@.. cpp:var:: message SystemSharedMemoryUnregisterResponse
//@@
//@@   Response message for SystemSharedMemoryUnregister.
//@@
type SystemSharedMemoryUnregisterResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SystemSharedMemoryUnregisterResponse) Reset()         { *m = SystemSharedMemoryUnregisterResponse{} }
func (m *SystemSharedMemoryUnregisterResponse) String() string { return proto.CompactTextString(m) }
func (*SystemSharedMemoryUnregisterResponse) ProtoMessage()    {}
func (*SystemSharedMemoryUnregisterResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{34}
}

func (m *SystemSharedMemoryUnregisterResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SystemSharedMemoryUnregisterResponse.Unmarshal(m, b)
}
func (m *SystemSharedMemoryUnregisterResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SystemSharedMemoryUnregisterResponse.Marshal(b, m, deterministic)
}
func (m *SystemSharedMemoryUnregisterResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SystemSharedMemoryUnregisterResponse.Merge(m, src)
}
func (m *SystemSharedMemoryUnregisterResponse) XXX_Size() int {
	return xxx_messageInfo_SystemSharedMemoryUnregisterResponse.Size(m)
}
func (m *SystemSharedMemoryUnregisterResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_SystemSharedMemoryUnregisterResponse.DiscardUnknown(m)
}

var xxx_messageInfo_SystemSharedMemoryUnregisterResponse proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message CudaSharedMemoryStatusRequest
//@@
//@@   Request message for CudaSharedMemoryStatus.
//@@
type CudaSharedMemoryStatusRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the region to get status for. If empty the
	//@@     status is returned for all registered regions.
	//@@
	Name                 string   `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryStatusRequest) Reset()         { *m = CudaSharedMemoryStatusRequest{} }
func (m *CudaSharedMemoryStatusRequest) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryStatusRequest) ProtoMessage()    {}
func (*CudaSharedMemoryStatusRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{35}
}

func (m *CudaSharedMemoryStatusRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryStatusRequest.Unmarshal(m, b)
}
func (m *CudaSharedMemoryStatusRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryStatusRequest.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryStatusRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryStatusRequest.Merge(m, src)
}
func (m *CudaSharedMemoryStatusRequest) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryStatusRequest.Size(m)
}
func (m *CudaSharedMemoryStatusRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryStatusRequest.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryStatusRequest proto.InternalMessageInfo

func (m *CudaSharedMemoryStatusRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

//@@
//@@.. cpp:var:: message CudaSharedMemoryStatusResponse
//@@
//@@   Response message for CudaSharedMemoryStatus.
//@@
type CudaSharedMemoryStatusResponse struct {
	//@@
	//@@  .. cpp:var:: map<string,RegionStatus> regions
	//@@
	//@@     Status for each of the registered regions, indexed by
	//@@     region name.
	//@@
	Regions              map[string]*CudaSharedMemoryStatusResponse_RegionStatus `protobuf:"bytes,1,rep,name=regions,proto3" json:"regions,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                                                `json:"-"`
	XXX_unrecognized     []byte                                                  `json:"-"`
	XXX_sizecache        int32                                                   `json:"-"`
}

func (m *CudaSharedMemoryStatusResponse) Reset()         { *m = CudaSharedMemoryStatusResponse{} }
func (m *CudaSharedMemoryStatusResponse) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryStatusResponse) ProtoMessage()    {}
func (*CudaSharedMemoryStatusResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{36}
}

func (m *CudaSharedMemoryStatusResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse.Unmarshal(m, b)
}
func (m *CudaSharedMemoryStatusResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryStatusResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryStatusResponse.Merge(m, src)
}
func (m *CudaSharedMemoryStatusResponse) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse.Size(m)
}
func (m *CudaSharedMemoryStatusResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryStatusResponse.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryStatusResponse proto.InternalMessageInfo

func (m *CudaSharedMemoryStatusResponse) GetRegions() map[string]*CudaSharedMemoryStatusResponse_RegionStatus {
	if m != nil {
		return m.Regions
	}
	return nil
}

//@@
//@@  .. cpp:var:: message RegionStatus
//@@
//@@     Status for a shared memory region.
//@@
type CudaSharedMemoryStatusResponse_RegionStatus struct {
	//@@
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name for the shared memory region.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: uin64 device_id
	//@@
	//@@       The GPU device ID where the cudaIPC handle was created.
	//@@
	DeviceId uint64 `protobuf:"varint,2,opt,name=device_id,json=deviceId,proto3" json:"device_id,omitempty"`
	//@@    .. cpp:var:: uint64 byte_size
	//@@
	//@@       Size of the shared memory region, in bytes.
	//@@
	ByteSize             uint64   `protobuf:"varint,3,opt,name=byte_size,json=byteSize,proto3" json:"byte_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryStatusResponse_RegionStatus) Reset() {
	*m = CudaSharedMemoryStatusResponse_RegionStatus{}
}
func (m *CudaSharedMemoryStatusResponse_RegionStatus) String() string {
	return proto.CompactTextString(m)
}
func (*CudaSharedMemoryStatusResponse_RegionStatus) ProtoMessage() {}
func (*CudaSharedMemoryStatusResponse_RegionStatus) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{36, 0}
}

func (m *CudaSharedMemoryStatusResponse_RegionStatus) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus.Unmarshal(m, b)
}
func (m *CudaSharedMemoryStatusResponse_RegionStatus) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryStatusResponse_RegionStatus) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus.Merge(m, src)
}
func (m *CudaSharedMemoryStatusResponse_RegionStatus) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus.Size(m)
}
func (m *CudaSharedMemoryStatusResponse_RegionStatus) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryStatusResponse_RegionStatus proto.InternalMessageInfo

func (m *CudaSharedMemoryStatusResponse_RegionStatus) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *CudaSharedMemoryStatusResponse_RegionStatus) GetDeviceId() uint64 {
	if m != nil {
		return m.DeviceId
	}
	return 0
}

func (m *CudaSharedMemoryStatusResponse_RegionStatus) GetByteSize() uint64 {
	if m != nil {
		return m.ByteSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message CudaSharedMemoryRegisterRequest
//@@
//@@   Request message for CudaSharedMemoryRegister.
//@@
type CudaSharedMemoryRegisterRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the region to register.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: bytes raw_handle
	//@@
	//@@     The raw serialized cudaIPC handle.
	//@@
	RawHandle []byte `protobuf:"bytes,2,opt,name=raw_handle,json=rawHandle,proto3" json:"raw_handle,omitempty"`
	//@@  .. cpp:var:: int64 device_id
	//@@
	//@@     The GPU device ID on which the cudaIPC handle was created.
	//@@
	DeviceId int64 `protobuf:"varint,3,opt,name=device_id,json=deviceId,proto3" json:"device_id,omitempty"`
	//@@  .. cpp:var:: uint64 byte_size
	//@@
	//@@     Size of the shared memory block, in bytes.
	//@@
	ByteSize             uint64   `protobuf:"varint,4,opt,name=byte_size,json=byteSize,proto3" json:"byte_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryRegisterRequest) Reset()         { *m = CudaSharedMemoryRegisterRequest{} }
func (m *CudaSharedMemoryRegisterRequest) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryRegisterRequest) ProtoMessage()    {}
func (*CudaSharedMemoryRegisterRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{37}
}

func (m *CudaSharedMemoryRegisterRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryRegisterRequest.Unmarshal(m, b)
}
func (m *CudaSharedMemoryRegisterRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryRegisterRequest.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryRegisterRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryRegisterRequest.Merge(m, src)
}
func (m *CudaSharedMemoryRegisterRequest) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryRegisterRequest.Size(m)
}
func (m *CudaSharedMemoryRegisterRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryRegisterRequest.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryRegisterRequest proto.InternalMessageInfo

func (m *CudaSharedMemoryRegisterRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *CudaSharedMemoryRegisterRequest) GetRawHandle() []byte {
	if m != nil {
		return m.RawHandle
	}
	return nil
}

func (m *CudaSharedMemoryRegisterRequest) GetDeviceId() int64 {
	if m != nil {
		return m.DeviceId
	}
	return 0
}

func (m *CudaSharedMemoryRegisterRequest) GetByteSize() uint64 {
	if m != nil {
		return m.ByteSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message CudaSharedMemoryRegisterResponse
//@@
//@@   Response message for CudaSharedMemoryRegister.
//@@
type CudaSharedMemoryRegisterResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryRegisterResponse) Reset()         { *m = CudaSharedMemoryRegisterResponse{} }
func (m *CudaSharedMemoryRegisterResponse) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryRegisterResponse) ProtoMessage()    {}
func (*CudaSharedMemoryRegisterResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{38}
}

func (m *CudaSharedMemoryRegisterResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryRegisterResponse.Unmarshal(m, b)
}
func (m *CudaSharedMemoryRegisterResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryRegisterResponse.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryRegisterResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryRegisterResponse.Merge(m, src)
}
func (m *CudaSharedMemoryRegisterResponse) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryRegisterResponse.Size(m)
}
func (m *CudaSharedMemoryRegisterResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryRegisterResponse.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryRegisterResponse proto.InternalMessageInfo

//@@
//@@.. cpp:var:: message CudaSharedMemoryUnregisterRequest
//@@
//@@   Request message for CudaSharedMemoryUnregister.
//@@
type CudaSharedMemoryUnregisterRequest struct {
	//@@
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the cuda region to unregister. If empty
	//@@     all cuda shared-memory regions are unregistered.
	//@@
	Name                 string   `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryUnregisterRequest) Reset()         { *m = CudaSharedMemoryUnregisterRequest{} }
func (m *CudaSharedMemoryUnregisterRequest) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryUnregisterRequest) ProtoMessage()    {}
func (*CudaSharedMemoryUnregisterRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{39}
}

func (m *CudaSharedMemoryUnregisterRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryUnregisterRequest.Unmarshal(m, b)
}
func (m *CudaSharedMemoryUnregisterRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryUnregisterRequest.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryUnregisterRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryUnregisterRequest.Merge(m, src)
}
func (m *CudaSharedMemoryUnregisterRequest) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryUnregisterRequest.Size(m)
}
func (m *CudaSharedMemoryUnregisterRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryUnregisterRequest.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryUnregisterRequest proto.InternalMessageInfo

func (m *CudaSharedMemoryUnregisterRequest) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

//@@
//@@.. cpp:var:: message CudaSharedMemoryUnregisterResponse
//@@
//@@   Response message for CudaSharedMemoryUnregister.
//@@
type CudaSharedMemoryUnregisterResponse struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *CudaSharedMemoryUnregisterResponse) Reset()         { *m = CudaSharedMemoryUnregisterResponse{} }
func (m *CudaSharedMemoryUnregisterResponse) String() string { return proto.CompactTextString(m) }
func (*CudaSharedMemoryUnregisterResponse) ProtoMessage()    {}
func (*CudaSharedMemoryUnregisterResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_626e658682f5c341, []int{40}
}

func (m *CudaSharedMemoryUnregisterResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_CudaSharedMemoryUnregisterResponse.Unmarshal(m, b)
}
func (m *CudaSharedMemoryUnregisterResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_CudaSharedMemoryUnregisterResponse.Marshal(b, m, deterministic)
}
func (m *CudaSharedMemoryUnregisterResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_CudaSharedMemoryUnregisterResponse.Merge(m, src)
}
func (m *CudaSharedMemoryUnregisterResponse) XXX_Size() int {
	return xxx_messageInfo_CudaSharedMemoryUnregisterResponse.Size(m)
}
func (m *CudaSharedMemoryUnregisterResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_CudaSharedMemoryUnregisterResponse.DiscardUnknown(m)
}

var xxx_messageInfo_CudaSharedMemoryUnregisterResponse proto.InternalMessageInfo

func init() {
	proto.RegisterType((*ServerLiveRequest)(nil), "inference.ServerLiveRequest")
	proto.RegisterType((*ServerLiveResponse)(nil), "inference.ServerLiveResponse")
	proto.RegisterType((*ServerReadyRequest)(nil), "inference.ServerReadyRequest")
	proto.RegisterType((*ServerReadyResponse)(nil), "inference.ServerReadyResponse")
	proto.RegisterType((*ModelReadyRequest)(nil), "inference.ModelReadyRequest")
	proto.RegisterType((*ModelReadyResponse)(nil), "inference.ModelReadyResponse")
	proto.RegisterType((*ServerMetadataRequest)(nil), "inference.ServerMetadataRequest")
	proto.RegisterType((*ServerMetadataResponse)(nil), "inference.ServerMetadataResponse")
	proto.RegisterType((*ModelMetadataRequest)(nil), "inference.ModelMetadataRequest")
	proto.RegisterType((*ModelMetadataResponse)(nil), "inference.ModelMetadataResponse")
	proto.RegisterType((*ModelMetadataResponse_TensorMetadata)(nil), "inference.ModelMetadataResponse.TensorMetadata")
	proto.RegisterType((*InferParameter)(nil), "inference.InferParameter")
	proto.RegisterType((*InferTensorContents)(nil), "inference.InferTensorContents")
	proto.RegisterType((*ModelInferRequest)(nil), "inference.ModelInferRequest")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.ParametersEntry")
	proto.RegisterType((*ModelInferRequest_InferInputTensor)(nil), "inference.ModelInferRequest.InferInputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.InferInputTensor.ParametersEntry")
	proto.RegisterType((*ModelInferRequest_InferRequestedOutputTensor)(nil), "inference.ModelInferRequest.InferRequestedOutputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry")
	proto.RegisterType((*ModelInferResponse)(nil), "inference.ModelInferResponse")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferResponse.ParametersEntry")
	proto.RegisterType((*ModelInferResponse_InferOutputTensor)(nil), "inference.ModelInferResponse.InferOutputTensor")
	proto.RegisterMapType((map[string]*InferParameter)(nil), "inference.ModelInferResponse.InferOutputTensor.ParametersEntry")
	proto.RegisterType((*ModelStreamInferResponse)(nil), "inference.ModelStreamInferResponse")
	proto.RegisterType((*ModelConfigRequest)(nil), "inference.ModelConfigRequest")
	proto.RegisterType((*ModelConfigResponse)(nil), "inference.ModelConfigResponse")
	proto.RegisterType((*ModelStatisticsRequest)(nil), "inference.ModelStatisticsRequest")
	proto.RegisterType((*StatisticDuration)(nil), "inference.StatisticDuration")
	proto.RegisterType((*InferStatistics)(nil), "inference.InferStatistics")
	proto.RegisterType((*InferBatchStatistics)(nil), "inference.InferBatchStatistics")
	proto.RegisterType((*ModelStatistics)(nil), "inference.ModelStatistics")
	proto.RegisterType((*ModelStatisticsResponse)(nil), "inference.ModelStatisticsResponse")
	proto.RegisterType((*RepositoryIndexRequest)(nil), "inference.RepositoryIndexRequest")
	proto.RegisterType((*RepositoryIndexResponse)(nil), "inference.RepositoryIndexResponse")
	proto.RegisterType((*RepositoryIndexResponse_ModelIndex)(nil), "inference.RepositoryIndexResponse.ModelIndex")
	proto.RegisterType((*RepositoryModelLoadRequest)(nil), "inference.RepositoryModelLoadRequest")
	proto.RegisterType((*RepositoryModelLoadResponse)(nil), "inference.RepositoryModelLoadResponse")
	proto.RegisterType((*RepositoryModelUnloadRequest)(nil), "inference.RepositoryModelUnloadRequest")
	proto.RegisterType((*RepositoryModelUnloadResponse)(nil), "inference.RepositoryModelUnloadResponse")
	proto.RegisterType((*SystemSharedMemoryStatusRequest)(nil), "inference.SystemSharedMemoryStatusRequest")
	proto.RegisterType((*SystemSharedMemoryStatusResponse)(nil), "inference.SystemSharedMemoryStatusResponse")
	proto.RegisterMapType((map[string]*SystemSharedMemoryStatusResponse_RegionStatus)(nil), "inference.SystemSharedMemoryStatusResponse.RegionsEntry")
	proto.RegisterType((*SystemSharedMemoryStatusResponse_RegionStatus)(nil), "inference.SystemSharedMemoryStatusResponse.RegionStatus")
	proto.RegisterType((*SystemSharedMemoryRegisterRequest)(nil), "inference.SystemSharedMemoryRegisterRequest")
	proto.RegisterType((*SystemSharedMemoryRegisterResponse)(nil), "inference.SystemSharedMemoryRegisterResponse")
	proto.RegisterType((*SystemSharedMemoryUnregisterRequest)(nil), "inference.SystemSharedMemoryUnregisterRequest")
	proto.RegisterType((*SystemSharedMemoryUnregisterResponse)(nil), "inference.SystemSharedMemoryUnregisterResponse")
	proto.RegisterType((*CudaSharedMemoryStatusRequest)(nil), "inference.CudaSharedMemoryStatusRequest")
	proto.RegisterType((*CudaSharedMemoryStatusResponse)(nil), "inference.CudaSharedMemoryStatusResponse")
	proto.RegisterMapType((map[string]*CudaSharedMemoryStatusResponse_RegionStatus)(nil), "inference.CudaSharedMemoryStatusResponse.RegionsEntry")
	proto.RegisterType((*CudaSharedMemoryStatusResponse_RegionStatus)(nil), "inference.CudaSharedMemoryStatusResponse.RegionStatus")
	proto.RegisterType((*CudaSharedMemoryRegisterRequest)(nil), "inference.CudaSharedMemoryRegisterRequest")
	proto.RegisterType((*CudaSharedMemoryRegisterResponse)(nil), "inference.CudaSharedMemoryRegisterResponse")
	proto.RegisterType((*CudaSharedMemoryUnregisterRequest)(nil), "inference.CudaSharedMemoryUnregisterRequest")
	proto.RegisterType((*CudaSharedMemoryUnregisterResponse)(nil), "inference.CudaSharedMemoryUnregisterResponse")
}

func init() { proto.RegisterFile("grpc_service.proto", fileDescriptor_626e658682f5c341) }

var fileDescriptor_626e658682f5c341 = []byte{
	// 2012 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xd4, 0x5a, 0xcd, 0x6f, 0xdb, 0xc8,
	0x15, 0x97, 0xa8, 0x0f, 0x4b, 0x4f, 0x1f, 0xb6, 0xc7, 0x8e, 0xa3, 0x32, 0x56, 0x2c, 0xd3, 0xc9,
	0x46, 0xdd, 0xcd, 0x2a, 0x0b, 0xa7, 0xcd, 0x7e, 0x14, 0x45, 0xbb, 0x71, 0xd2, 0xd8, 0x58, 0x3b,
	0x4d, 0xe9, 0xdd, 0x6c, 0xd1, 0x0f, 0x08, 0xb4, 0x34, 0xb2, 0x89, 0x4a, 0xa4, 0x76, 0x38, 0xb4,
	0xa3, 0x2d, 0x50, 0xb4, 0x87, 0x1e, 0xdb, 0x53, 0xaf, 0xfd, 0x5b, 0x8a, 0x02, 0x3d, 0x15, 0x28,
	0xda, 0x5b, 0x8f, 0xfd, 0x23, 0x7a, 0xeb, 0xa9, 0x98, 0x0f, 0x52, 0x24, 0x45, 0x52, 0x92, 0x1b,
	0x1f, 0x7a, 0xd3, 0xbc, 0x79, 0xef, 0xf7, 0xde, 0xbc, 0x8f, 0x79, 0x8f, 0xa4, 0x00, 0x9d, 0x93,
	0x71, 0xaf, 0xeb, 0x60, 0x72, 0x69, 0xf6, 0x70, 0x67, 0x4c, 0x6c, 0x6a, 0xa3, 0xb2, 0x69, 0x0d,
	0x30, 0xc1, 0x56, 0x0f, 0xab, 0x68, 0x64, 0xf7, 0xf1, 0xb0, 0xdb, 0xb3, 0xad, 0x81, 0x79, 0x2e,
	0xb6, 0xb5, 0x0d, 0x58, 0x3f, 0xc5, 0xe4, 0x12, 0x93, 0x63, 0xf3, 0x12, 0xeb, 0xf8, 0x2b, 0x17,
	0x3b, 0x54, 0x6b, 0x03, 0x0a, 0x12, 0x9d, 0xb1, 0x6d, 0x39, 0x18, 0x21, 0xc8, 0x0f, 0xcd, 0x4b,
	0xdc, 0xc8, 0xb6, 0xb2, 0xed, 0x92, 0xce, 0x7f, 0x6b, 0x9b, 0x1e, 0xa7, 0x8e, 0x8d, 0xfe, 0xc4,
	0x93, 0x7f, 0x0f, 0x36, 0x42, 0x54, 0x09, 0xb0, 0x09, 0x05, 0xc2, 0x08, 0x12, 0x41, 0x2c, 0xb4,
	0x4f, 0x61, 0xfd, 0x84, 0xd9, 0x15, 0x44, 0x60, 0xba, 0x2c, 0x63, 0x24, 0x74, 0x95, 0x75, 0xfe,
	0x1b, 0x35, 0x60, 0xe5, 0x12, 0x13, 0xc7, 0xb4, 0xad, 0x86, 0xc2, 0xc9, 0xde, 0x52, 0x7b, 0x17,
	0x50, 0x10, 0x22, 0x55, 0xdd, 0x6d, 0xb8, 0x25, 0x6c, 0x3b, 0xc1, 0xd4, 0xe8, 0x1b, 0xd4, 0xf0,
	0x8c, 0x1e, 0xc0, 0x56, 0x74, 0x63, 0x7a, 0xf0, 0xc5, 0x8d, 0x41, 0x77, 0x01, 0xf0, 0x1b, 0x8a,
	0x2d, 0xb6, 0x70, 0x1a, 0xb9, 0x56, 0xae, 0x5d, 0xd6, 0x03, 0x14, 0xed, 0x19, 0x6c, 0x72, 0x63,
	0x23, 0xfa, 0x97, 0x3c, 0xf2, 0xdf, 0x15, 0xb8, 0x15, 0x81, 0x49, 0xb1, 0x56, 0x85, 0x92, 0x14,
	0x74, 0x1a, 0x0a, 0xb7, 0xc8, 0x5f, 0xb3, 0xbd, 0xf1, 0xd0, 0xa0, 0x03, 0x9b, 0x8c, 0x1a, 0x39,
	0x2e, 0xe3, 0xaf, 0xd1, 0x0b, 0x28, 0x9a, 0xd6, 0xd8, 0xa5, 0x4e, 0x23, 0xdf, 0xca, 0xb5, 0x2b,
	0xfb, 0x8f, 0x3a, 0x7e, 0x36, 0x75, 0x62, 0xb5, 0x77, 0x3e, 0xc7, 0x96, 0x63, 0x4f, 0x5d, 0x28,
	0xc5, 0xd1, 0x11, 0xac, 0xd8, 0x2e, 0xe5, 0x48, 0x85, 0xeb, 0x21, 0x79, 0xf2, 0xea, 0x6b, 0xa8,
	0x87, 0xb7, 0x92, 0x4e, 0xcc, 0xf6, 0xe8, 0x64, 0x8c, 0xa5, 0xeb, 0xfc, 0x35, 0x4b, 0x0c, 0xe7,
	0xc2, 0x18, 0x63, 0x1e, 0x9c, 0x9c, 0x2e, 0x16, 0xda, 0xef, 0xb2, 0x50, 0x3f, 0x62, 0x36, 0xbd,
	0x32, 0x88, 0x31, 0xc2, 0x14, 0x13, 0xb4, 0x03, 0x70, 0x66, 0xdb, 0xc3, 0xee, 0x98, 0x51, 0x44,
	0x1a, 0x1d, 0x66, 0xf4, 0x32, 0xa3, 0x71, 0x26, 0xb4, 0x0b, 0x15, 0xd3, 0xa2, 0x4f, 0xbe, 0x25,
	0x39, 0x98, 0xa2, 0xdc, 0x61, 0x46, 0x07, 0x4e, 0x14, 0x2c, 0x7b, 0x50, 0x75, 0x28, 0x31, 0xad,
	0x73, 0xc9, 0xc3, 0x5d, 0x7c, 0x98, 0xd1, 0x2b, 0x82, 0xca, 0x99, 0x9e, 0x22, 0x58, 0x1b, 0x7b,
	0x5a, 0xbb, 0xbd, 0x0b, 0xdb, 0xec, 0x61, 0xed, 0xcf, 0x0a, 0x6c, 0x70, 0x7b, 0xc4, 0x69, 0x0f,
	0x6c, 0x8b, 0x62, 0x8b, 0x3a, 0x68, 0x0f, 0x6a, 0xdc, 0xa8, 0x9e, 0x24, 0x34, 0xb2, 0xad, 0x5c,
	0xbb, 0xa4, 0x57, 0x19, 0xd1, 0x67, 0xda, 0x85, 0xaa, 0x69, 0xd1, 0x29, 0x0f, 0x0b, 0x7a, 0x41,
	0x67, 0xc6, 0xfa, 0x2c, 0xf7, 0xa1, 0x2e, 0x6c, 0xf7, 0x99, 0x84, 0x3b, 0x6a, 0x9c, 0x1a, 0x54,
	0xe7, 0x86, 0xa0, 0x58, 0x26, 0xd4, 0xf4, 0xaa, 0x1b, 0xc4, 0x7a, 0x00, 0xab, 0x6e, 0x04, 0x8c,
	0x85, 0x39, 0xaf, 0xd7, 0xdd, 0x19, 0xb4, 0xc1, 0xf8, 0xf1, 0xfe, 0x94, 0xad, 0xd8, 0xca, 0xb5,
	0x15, 0xbd, 0xca, 0x88, 0x61, 0xa6, 0x20, 0xd6, 0x4a, 0x2b, 0xd7, 0xce, 0x32, 0xa6, 0x30, 0xd2,
	0xd9, 0x84, 0xe2, 0x29, 0x53, 0xa9, 0x95, 0x6b, 0x57, 0xf5, 0x2a, 0x23, 0x7a, 0x4c, 0xda, 0x1f,
	0x4b, 0xf2, 0x72, 0xe1, 0x8e, 0xf4, 0x2a, 0xad, 0x09, 0x20, 0x6e, 0xc2, 0x40, 0xd6, 0x94, 0x39,
	0xe5, 0x25, 0x4b, 0x9d, 0x3d, 0xa8, 0x89, 0xed, 0x70, 0xe9, 0x55, 0x39, 0xf1, 0xb5, 0xac, 0xf2,
	0x3a, 0x28, 0x66, 0x5f, 0xd6, 0x8b, 0x62, 0xf6, 0xd1, 0x31, 0x80, 0x1f, 0x41, 0xaf, 0x5a, 0x1e,
	0x46, 0x73, 0x3c, 0x68, 0x45, 0xc7, 0x4f, 0x33, 0xe7, 0xb9, 0x45, 0xc9, 0x44, 0x0f, 0xc8, 0xa3,
	0xe7, 0x7e, 0xdd, 0x89, 0x6a, 0x79, 0x3f, 0x15, 0x89, 0x2f, 0x8e, 0x18, 0xbf, 0x48, 0x15, 0xbf,
	0xea, 0x7e, 0x34, 0xad, 0xba, 0x22, 0xc7, 0xf9, 0x70, 0x3e, 0x8e, 0x5c, 0xe0, 0xfe, 0x0f, 0xb9,
	0xa4, 0x44, 0xf4, 0x70, 0xd0, 0x43, 0x40, 0xc4, 0xb8, 0xea, 0x72, 0x05, 0xe1, 0x00, 0x55, 0xf5,
	0x35, 0x62, 0x5c, 0x71, 0x33, 0x3c, 0xff, 0xab, 0x7f, 0x53, 0x60, 0x2d, 0x6a, 0xdd, 0xdb, 0x29,
	0x57, 0xf4, 0xf3, 0x18, 0x87, 0x7f, 0x77, 0x29, 0x37, 0xa5, 0x46, 0xe0, 0x13, 0x28, 0x05, 0x52,
	0x39, 0xdb, 0xae, 0xec, 0xdf, 0x0d, 0x80, 0xc7, 0xd4, 0xa5, 0xee, 0xf3, 0xab, 0x3f, 0x86, 0xd5,
	0x08, 0x34, 0x5a, 0x83, 0xdc, 0x2f, 0xf0, 0x44, 0x1e, 0x99, 0xfd, 0x44, 0x8f, 0xa0, 0x70, 0x69,
	0x0c, 0x5d, 0x71, 0xdc, 0xca, 0xfe, 0x37, 0xa2, 0xe8, 0x3e, 0x82, 0x2e, 0xf8, 0x3e, 0x51, 0x3e,
	0xca, 0xaa, 0xff, 0xce, 0x82, 0x9a, 0x1c, 0xa5, 0x58, 0xcf, 0x9e, 0x87, 0xfc, 0xa4, 0x70, 0x3f,
	0xbd, 0xb8, 0x66, 0x1a, 0xa4, 0x79, 0xec, 0x06, 0x4f, 0x7d, 0x63, 0xc8, 0xda, 0xbf, 0x0a, 0x72,
	0x72, 0x90, 0x67, 0x96, 0x2d, 0xf4, 0x26, 0x2e, 0x88, 0x93, 0x98, 0x7c, 0x4d, 0x2a, 0x6b, 0xd9,
	0x01, 0xd3, 0xf2, 0x73, 0x7e, 0x43, 0x0d, 0x63, 0xf1, 0x55, 0x7c, 0x49, 0x77, 0x60, 0x83, 0x95,
	0xb4, 0x58, 0x86, 0x6f, 0xe6, 0xaa, 0xbe, 0x4e, 0x8c, 0x2b, 0x21, 0xe6, 0x17, 0xf5, 0x3f, 0x14,
	0x58, 0x9f, 0x81, 0x7b, 0x4b, 0x55, 0xdd, 0x8d, 0xf1, 0xd2, 0xf7, 0x96, 0x3c, 0xd9, 0xff, 0x61,
	0x5d, 0xdf, 0x5c, 0x86, 0xff, 0x36, 0x0b, 0x0d, 0xee, 0xb4, 0x53, 0x4a, 0xb0, 0x31, 0x0a, 0xe7,
	0xf9, 0x1e, 0xd4, 0x30, 0x21, 0x36, 0xe9, 0x8e, 0xb0, 0xe3, 0x18, 0xe7, 0x5e, 0xf0, 0xaa, 0x9c,
	0x78, 0x22, 0x68, 0xe8, 0x19, 0x9b, 0x13, 0x06, 0x98, 0x74, 0x89, 0x14, 0x93, 0xfa, 0x9b, 0xa9,
	0x61, 0x61, 0x63, 0x44, 0x60, 0xa9, 0x3d, 0x95, 0x85, 0x76, 0xc0, 0x1f, 0x3e, 0xae, 0x37, 0xf3,
	0x3e, 0x87, 0x8d, 0x10, 0x86, 0x3c, 0x45, 0x07, 0x8a, 0xe2, 0x91, 0x86, 0xc3, 0x54, 0xf6, 0xb7,
	0xa2, 0x86, 0x49, 0x7e, 0xc9, 0xa5, 0xfd, 0x00, 0xb6, 0xa4, 0x47, 0x0c, 0x6a, 0x3a, 0xd4, 0xec,
	0x39, 0xd7, 0x33, 0xe7, 0x63, 0x58, 0xf7, 0x21, 0x9e, 0xb9, 0xc4, 0xa0, 0xac, 0xec, 0x37, 0xa1,
	0xd0, 0xb3, 0x5d, 0x8b, 0x72, 0x8c, 0xbc, 0x2e, 0x16, 0xec, 0x32, 0xe0, 0x93, 0x37, 0x23, 0x29,
	0x96, 0xa3, 0xfd, 0x47, 0x81, 0x55, 0xee, 0xae, 0xa9, 0x0d, 0xe8, 0x09, 0xac, 0x38, 0x6e, 0xaf,
	0x87, 0x1d, 0x47, 0x9e, 0x63, 0x3b, 0x70, 0x8e, 0x19, 0x45, 0xba, 0xc7, 0x8c, 0x3e, 0x80, 0xfc,
	0xc0, 0x30, 0x87, 0x32, 0x2a, 0xe9, 0x42, 0x9c, 0x13, 0xed, 0x43, 0xe1, 0x2b, 0x17, 0xbb, 0x98,
	0xdf, 0x4e, 0xf3, 0x44, 0x04, 0x2b, 0xfa, 0x14, 0x6a, 0x3d, 0x7b, 0x34, 0x76, 0x29, 0x16, 0xbd,
	0xbf, 0x91, 0x5f, 0x40, 0xb6, 0x2a, 0x45, 0x78, 0xd3, 0x0d, 0x43, 0x0c, 0x30, 0x91, 0xf5, 0xb7,
	0x28, 0xc4, 0x00, 0x13, 0x74, 0x00, 0x75, 0x0f, 0x42, 0x5c, 0x57, 0x8d, 0xe2, 0x02, 0x18, 0x9e,
	0x5a, 0x71, 0x49, 0x68, 0xbf, 0x56, 0x60, 0x93, 0xc3, 0x3d, 0x35, 0x68, 0xef, 0x22, 0x10, 0x81,
	0x26, 0xc0, 0x19, 0x23, 0x75, 0x1d, 0xf3, 0x6b, 0x2c, 0x03, 0x58, 0xe6, 0x94, 0x53, 0xf3, 0xeb,
	0x18, 0x17, 0x28, 0xff, 0xbb, 0x0b, 0x72, 0x6f, 0xc1, 0x05, 0xf9, 0xe5, 0x5d, 0xf0, 0x27, 0x05,
	0x56, 0x23, 0x35, 0xb0, 0xe4, 0x53, 0xee, 0x7d, 0xa8, 0x0f, 0x0d, 0x87, 0x76, 0x7d, 0xa5, 0xfc,
	0x28, 0x79, 0xbd, 0xc6, 0xa8, 0x47, 0x1e, 0x91, 0x3d, 0x18, 0xf8, 0x1c, 0x5d, 0x51, 0x18, 0x79,
	0xce, 0x57, 0xf7, 0xc9, 0x07, 0xbc, 0x42, 0x1e, 0xc0, 0x2a, 0x7e, 0x83, 0x7b, 0x2e, 0xb3, 0x56,
	0x32, 0x16, 0x04, 0xa3, 0x4f, 0x16, 0x8c, 0x07, 0x41, 0x44, 0x87, 0x1a, 0xbc, 0x53, 0x31, 0x07,
	0xa8, 0xd1, 0xfb, 0x30, 0x50, 0xdf, 0x53, 0x6d, 0x8c, 0xe8, 0xa0, 0xef, 0x43, 0x45, 0x46, 0x9a,
	0x03, 0xac, 0xf0, 0x3e, 0xb3, 0x13, 0x05, 0x88, 0xe4, 0x87, 0x2e, 0xb2, 0x83, 0x23, 0x68, 0xaf,
	0xe1, 0xf6, 0xcc, 0x25, 0x22, 0xef, 0xa3, 0xef, 0x40, 0x45, 0x8c, 0x07, 0x02, 0x3c, 0xcb, 0xc1,
	0xd5, 0xe8, 0xa5, 0x14, 0xc4, 0x1d, 0x79, 0x04, 0x47, 0xfb, 0x12, 0xb6, 0x74, 0x3c, 0xb6, 0x1d,
	0x93, 0xda, 0x64, 0x72, 0x64, 0xf5, 0xf1, 0x1b, 0xef, 0x72, 0x7a, 0x00, 0xab, 0xc4, 0xdf, 0x09,
	0x4e, 0x26, 0xf5, 0x29, 0x99, 0x8f, 0x27, 0xfe, 0x7b, 0x0f, 0x25, 0xf8, 0xde, 0xe3, 0xaf, 0x59,
	0xb8, 0x3d, 0x83, 0x2c, 0x2d, 0x7e, 0x0e, 0x45, 0x6e, 0x82, 0x67, 0x6c, 0x70, 0x2e, 0x49, 0x90,
	0xf1, 0xae, 0x7c, 0x46, 0x92, 0xc2, 0xea, 0x05, 0xc0, 0x94, 0xba, 0x64, 0x3e, 0xb1, 0x71, 0x80,
	0x1a, 0x14, 0xcb, 0x89, 0x49, 0x2c, 0xd0, 0x16, 0x14, 0x09, 0x36, 0x1c, 0xdb, 0xe2, 0x59, 0x53,
	0xd6, 0xe5, 0x4a, 0xeb, 0x83, 0x3a, 0xb5, 0x8b, 0xeb, 0x3c, 0xb6, 0x8d, 0xfe, 0xd2, 0x9e, 0x0a,
	0xcf, 0x79, 0x4a, 0x64, 0xce, 0xd3, 0x9a, 0x70, 0x27, 0x56, 0x8b, 0x6c, 0x69, 0x03, 0xd8, 0x8e,
	0x6c, 0x7f, 0x61, 0x0d, 0x6f, 0xc0, 0x8c, 0x1d, 0x68, 0x26, 0xe8, 0x91, 0x86, 0x7c, 0x1b, 0x76,
	0x4e, 0x27, 0x0e, 0xc5, 0xa3, 0xd3, 0x0b, 0x83, 0xe0, 0xfe, 0x09, 0x1e, 0xd9, 0x64, 0xc2, 0xd2,
	0xc9, 0x4d, 0xeb, 0x6c, 0xda, 0x3f, 0x15, 0x68, 0x25, 0xcb, 0xc9, 0xd4, 0xd0, 0x61, 0x85, 0xe0,
	0x73, 0xfe, 0xe2, 0x48, 0xe4, 0xc6, 0x47, 0xc1, 0x7b, 0x66, 0x8e, 0x74, 0x47, 0x17, 0xa2, 0x62,
	0x0c, 0xf3, 0x80, 0x54, 0x13, 0xaa, 0x62, 0x43, 0x70, 0xc7, 0x66, 0x8a, 0x1c, 0x7f, 0x94, 0xe9,
	0xf8, 0xb3, 0x05, 0x45, 0x7b, 0x30, 0x70, 0x30, 0x95, 0x37, 0x8d, 0x5c, 0xa1, 0x3b, 0x50, 0xe6,
	0x2f, 0x02, 0xf8, 0xa5, 0x2d, 0x2e, 0x97, 0x12, 0x23, 0xb0, 0x3b, 0x5b, 0xa5, 0x9e, 0xaa, 0xc4,
	0xa9, 0xea, 0x65, 0x78, 0xaa, 0xba, 0xc6, 0xf1, 0x24, 0x31, 0x30, 0x74, 0xfd, 0x0a, 0x76, 0x67,
	0x65, 0x19, 0xb3, 0x43, 0xa7, 0x6f, 0x21, 0x6e, 0xee, 0xd4, 0xda, 0x3d, 0xd0, 0xd2, 0xf4, 0xcb,
	0xb4, 0xf9, 0x18, 0xf6, 0x66, 0xb9, 0xbe, 0xb0, 0xc8, 0x7c, 0x3b, 0xb5, 0x77, 0xe0, 0x5e, 0xba,
	0xa8, 0x54, 0xf1, 0x18, 0x9a, 0x07, 0x6e, 0xdf, 0x58, 0x2e, 0x2f, 0xff, 0xa2, 0xc0, 0xdd, 0x24,
	0x29, 0x99, 0x95, 0xaf, 0xa2, 0x59, 0xf9, 0x24, 0x10, 0xb6, 0x74, 0xd9, 0x84, 0x9c, 0xfc, 0xd9,
	0x02, 0x39, 0x79, 0x07, 0xca, 0x7d, 0x7c, 0x69, 0xf6, 0x70, 0xd7, 0xec, 0xcb, 0x61, 0xae, 0x24,
	0x08, 0x47, 0xfd, 0x70, 0x40, 0x72, 0x91, 0x34, 0x24, 0x73, 0xd3, 0xf0, 0x38, 0x9c, 0x86, 0x4b,
	0x9f, 0x67, 0x36, 0x09, 0x7f, 0x9f, 0x85, 0x9d, 0xa8, 0xe8, 0x22, 0x39, 0xd8, 0x04, 0x60, 0x8f,
	0x83, 0x17, 0x86, 0xd5, 0x1f, 0x0a, 0x73, 0xaa, 0x7a, 0x99, 0x18, 0x57, 0x87, 0x9c, 0x10, 0x76,
	0x02, 0x3b, 0x67, 0x2e, 0xc9, 0x09, 0xd1, 0xac, 0xd4, 0xa0, 0x95, 0x6c, 0x8f, 0x4c, 0x98, 0x0f,
	0x61, 0x37, 0xca, 0xb3, 0x58, 0x46, 0xde, 0x03, 0x2d, 0x4d, 0x50, 0xc0, 0xef, 0xff, 0xa1, 0x0e,
	0x9b, 0x2f, 0xf4, 0x57, 0x07, 0xfe, 0x80, 0x72, 0x2a, 0xbe, 0x95, 0xa0, 0xcf, 0x00, 0xa6, 0x5f,
	0x3c, 0x50, 0x68, 0x96, 0x8a, 0x7e, 0x1d, 0x51, 0x9b, 0x09, 0xbb, 0xf2, 0x08, 0x19, 0xf4, 0x12,
	0x2a, 0x81, 0xcf, 0x1f, 0x68, 0x96, 0x3f, 0xf8, 0xa9, 0x43, 0xbd, 0x9b, 0xb4, 0xed, 0xe3, 0x7d,
	0x26, 0xfb, 0xaa, 0x80, 0xdb, 0x8e, 0x4e, 0x12, 0x21, 0xb4, 0x66, 0xc2, 0xae, 0x0f, 0xf6, 0x25,
	0xd4, 0xc3, 0x9f, 0x39, 0x50, 0x6b, 0xc6, 0x80, 0xc8, 0xa7, 0x09, 0x75, 0x37, 0x85, 0xc3, 0x07,
	0xfe, 0x1c, 0x6a, 0xa1, 0x17, 0xf9, 0x68, 0x27, 0xf9, 0x15, 0xbf, 0x80, 0x6d, 0xcd, 0xfb, 0x06,
	0x10, 0x38, 0xbb, 0x18, 0x7e, 0xb7, 0xd3, 0x5e, 0x5c, 0xa9, 0xe9, 0x4f, 0xa4, 0x5a, 0x06, 0xfd,
	0x14, 0xd6, 0xa2, 0xcf, 0xc2, 0x73, 0x20, 0xf7, 0x66, 0xc7, 0xb6, 0x99, 0xc7, 0x68, 0x2d, 0xd3,
	0xce, 0x7e, 0x90, 0x65, 0x51, 0x0f, 0x3c, 0x6d, 0xa2, 0x66, 0xc2, 0x53, 0x68, 0x4c, 0xd4, 0x63,
	0x1e, 0x6a, 0xb5, 0x0c, 0xfa, 0xc9, 0xec, 0x88, 0xbe, 0x9b, 0x32, 0x44, 0x4a, 0x5c, 0x2d, 0x8d,
	0x25, 0x88, 0x1d, 0x99, 0xeb, 0x42, 0xd8, 0xf1, 0x13, 0x68, 0x08, 0x3b, 0x61, 0x2c, 0xd4, 0x32,
	0x68, 0x00, 0x1b, 0x31, 0x53, 0x13, 0xba, 0x1f, 0x2b, 0x1c, 0x9d, 0xdd, 0xd4, 0x77, 0xe6, 0xb1,
	0xf9, 0x7a, 0x86, 0x70, 0x2b, 0x76, 0x2c, 0x42, 0x0f, 0x92, 0x21, 0x42, 0x03, 0x9a, 0xda, 0x9e,
	0xcf, 0xe8, 0x6b, 0x73, 0xa1, 0x91, 0x34, 0x0e, 0xa0, 0x77, 0x17, 0x9a, 0x19, 0x84, 0xce, 0xf7,
	0x96, 0x98, 0x2f, 0xb4, 0x0c, 0xfa, 0x25, 0xa8, 0xc9, 0x9d, 0x1c, 0x3d, 0x4c, 0x05, 0x8b, 0x5c,
	0xf6, 0xea, 0xfb, 0x0b, 0x72, 0xfb, 0xca, 0x7f, 0x93, 0x85, 0xed, 0xb4, 0x36, 0x8f, 0x3a, 0xa9,
	0x88, 0x33, 0x17, 0xb7, 0xfa, 0x68, 0x61, 0x7e, 0xdf, 0x06, 0x1b, 0xb6, 0xe2, 0xfb, 0x1f, 0x6a,
	0x2f, 0xd0, 0x22, 0x85, 0xda, 0x6f, 0x2e, 0xdc, 0x4c, 0x45, 0xa0, 0x93, 0xba, 0x54, 0x28, 0xd0,
	0x73, 0x5a, 0x6b, 0x28, 0xd0, 0x73, 0xdb, 0x1e, 0x0f, 0x74, 0x72, 0xff, 0x0a, 0x05, 0x7a, 0x6e,
	0x7f, 0x0c, 0x05, 0x7a, 0x7e, 0x53, 0xd4, 0x32, 0x67, 0x45, 0xfe, 0x5f, 0x80, 0xc7, 0xff, 0x0d,
	0x00, 0x00, 0xff, 0xff, 0xaa, 0xbc, 0xe8, 0x67, 0x40, 0x20, 0x00, 0x00,
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// GRPCInferenceServiceClient is the client API for GRPCInferenceService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type GRPCInferenceServiceClient interface {
	//@@  .. cpp:var:: rpc ServerLive(ServerLiveRequest) returns
	//@@       (ServerLiveResponse)
	//@@
	//@@     Check liveness of the inference server.
	//@@
	ServerLive(ctx context.Context, in *ServerLiveRequest, opts ...grpc.CallOption) (*ServerLiveResponse, error)
	//@@  .. cpp:var:: rpc ServerReady(ServerReadyRequest) returns
	//@@       (ServerReadyResponse)
	//@@
	//@@     Check readiness of the inference server.
	//@@
	ServerReady(ctx context.Context, in *ServerReadyRequest, opts ...grpc.CallOption) (*ServerReadyResponse, error)
	//@@  .. cpp:var:: rpc ModelReady(ModelReadyRequest) returns
	//@@       (ModelReadyResponse)
	//@@
	//@@     Check readiness of a model in the inference server.
	//@@
	ModelReady(ctx context.Context, in *ModelReadyRequest, opts ...grpc.CallOption) (*ModelReadyResponse, error)
	//@@  .. cpp:var:: rpc ServerMetadata(ServerMetadataRequest) returns
	//@@       (ServerMetadataResponse)
	//@@
	//@@     Get server metadata.
	//@@
	ServerMetadata(ctx context.Context, in *ServerMetadataRequest, opts ...grpc.CallOption) (*ServerMetadataResponse, error)
	//@@  .. cpp:var:: rpc ModelMetadata(ModelMetadataRequest) returns
	//@@       (ModelMetadataResponse)
	//@@
	//@@     Get model metadata.
	//@@
	ModelMetadata(ctx context.Context, in *ModelMetadataRequest, opts ...grpc.CallOption) (*ModelMetadataResponse, error)
	//@@  .. cpp:var:: rpc ModelInfer(ModelInferRequest) returns
	//@@       (ModelInferResponse)
	//@@
	//@@     Perform inference using a specific model.
	//@@
	ModelInfer(ctx context.Context, in *ModelInferRequest, opts ...grpc.CallOption) (*ModelInferResponse, error)
	//@@  .. cpp:var:: rpc ModelStreamInfer(stream ModelInferRequest) returns
	//@@       (stream ModelStreamInferResponse)
	//@@
	//@@     Perform streaming inference.
	//@@
	ModelStreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCInferenceService_ModelStreamInferClient, error)
	//@@  .. cpp:var:: rpc ModelConfig(ModelConfigRequest) returns
	//@@       (ModelConfigResponse)
	//@@
	//@@     Get model configuration.
	//@@
	ModelConfig(ctx context.Context, in *ModelConfigRequest, opts ...grpc.CallOption) (*ModelConfigResponse, error)
	//@@  .. cpp:var:: rpc ModelStatistics(
	//@@                     ModelStatisticsRequest)
	//@@                   returns (ModelStatisticsResponse)
	//@@
	//@@     Get the cumulative inference statistics for a model.
	//@@
	ModelStatistics(ctx context.Context, in *ModelStatisticsRequest, opts ...grpc.CallOption) (*ModelStatisticsResponse, error)
	//@@  .. cpp:var:: rpc RepositoryIndex(RepositoryIndexRequest) returns
	//@@       (RepositoryIndexResponse)
	//@@
	//@@     Get the index of model repository contents.
	//@@
	RepositoryIndex(ctx context.Context, in *RepositoryIndexRequest, opts ...grpc.CallOption) (*RepositoryIndexResponse, error)
	//@@  .. cpp:var:: rpc RepositoryModelLoad(RepositoryModelLoadRequest) returns
	//@@       (RepositoryModelLoadResponse)
	//@@
	//@@     Load or reload a model from a repository.
	//@@
	RepositoryModelLoad(ctx context.Context, in *RepositoryModelLoadRequest, opts ...grpc.CallOption) (*RepositoryModelLoadResponse, error)
	//@@  .. cpp:var:: rpc RepositoryModelUnload(RepositoryModelUnloadRequest)
	//@@       returns (RepositoryModelUnloadResponse)
	//@@
	//@@     Unload a model.
	//@@
	RepositoryModelUnload(ctx context.Context, in *RepositoryModelUnloadRequest, opts ...grpc.CallOption) (*RepositoryModelUnloadResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryStatus(
	//@@                     SystemSharedMemoryStatusRequest)
	//@@                   returns (SystemSharedMemoryStatusRespose)
	//@@
	//@@     Get the status of all registered system-shared-memory regions.
	//@@
	SystemSharedMemoryStatus(ctx context.Context, in *SystemSharedMemoryStatusRequest, opts ...grpc.CallOption) (*SystemSharedMemoryStatusResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryRegister(
	//@@                     SystemSharedMemoryRegisterRequest)
	//@@                   returns (SystemSharedMemoryRegisterResponse)
	//@@
	//@@     Register a system-shared-memory region.
	//@@
	SystemSharedMemoryRegister(ctx context.Context, in *SystemSharedMemoryRegisterRequest, opts ...grpc.CallOption) (*SystemSharedMemoryRegisterResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryUnregister(
	//@@                     SystemSharedMemoryUnregisterRequest)
	//@@                   returns (SystemSharedMemoryUnregisterResponse)
	//@@
	//@@     Unregister a system-shared-memory region.
	//@@
	SystemSharedMemoryUnregister(ctx context.Context, in *SystemSharedMemoryUnregisterRequest, opts ...grpc.CallOption) (*SystemSharedMemoryUnregisterResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryStatus(
	//@@                     CudaSharedMemoryStatusRequest)
	//@@                   returns (CudaSharedMemoryStatusRespose)
	//@@
	//@@     Get the status of all registered CUDA-shared-memory regions.
	//@@
	CudaSharedMemoryStatus(ctx context.Context, in *CudaSharedMemoryStatusRequest, opts ...grpc.CallOption) (*CudaSharedMemoryStatusResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryRegister(
	//@@                     CudaSharedMemoryRegisterRequest)
	//@@                   returns (CudaSharedMemoryRegisterResponse)
	//@@
	//@@     Register a CUDA-shared-memory region.
	//@@
	CudaSharedMemoryRegister(ctx context.Context, in *CudaSharedMemoryRegisterRequest, opts ...grpc.CallOption) (*CudaSharedMemoryRegisterResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryUnregister(
	//@@                     CudaSharedMemoryUnregisterRequest)
	//@@                   returns (CudaSharedMemoryUnregisterResponse)
	//@@
	//@@     Unregister a CUDA-shared-memory region.
	//@@
	CudaSharedMemoryUnregister(ctx context.Context, in *CudaSharedMemoryUnregisterRequest, opts ...grpc.CallOption) (*CudaSharedMemoryUnregisterResponse, error)
}

type gRPCInferenceServiceClient struct {
	cc *grpc.ClientConn
}

func NewGRPCInferenceServiceClient(cc *grpc.ClientConn) GRPCInferenceServiceClient {
	return &gRPCInferenceServiceClient{cc}
}

func (c *gRPCInferenceServiceClient) ServerLive(ctx context.Context, in *ServerLiveRequest, opts ...grpc.CallOption) (*ServerLiveResponse, error) {
	out := new(ServerLiveResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerLive", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ServerReady(ctx context.Context, in *ServerReadyRequest, opts ...grpc.CallOption) (*ServerReadyResponse, error) {
	out := new(ServerReadyResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerReady", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelReady(ctx context.Context, in *ModelReadyRequest, opts ...grpc.CallOption) (*ModelReadyResponse, error) {
	out := new(ModelReadyResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelReady", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ServerMetadata(ctx context.Context, in *ServerMetadataRequest, opts ...grpc.CallOption) (*ServerMetadataResponse, error) {
	out := new(ServerMetadataResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ServerMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelMetadata(ctx context.Context, in *ModelMetadataRequest, opts ...grpc.CallOption) (*ModelMetadataResponse, error) {
	out := new(ModelMetadataResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelInfer(ctx context.Context, in *ModelInferRequest, opts ...grpc.CallOption) (*ModelInferResponse, error) {
	out := new(ModelInferResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelInfer", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelStreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCInferenceService_ModelStreamInferClient, error) {
	stream, err := c.cc.NewStream(ctx, &_GRPCInferenceService_serviceDesc.Streams[0], "/inference.GRPCInferenceService/ModelStreamInfer", opts...)
	if err != nil {
		return nil, err
	}
	x := &gRPCInferenceServiceModelStreamInferClient{stream}
	return x, nil
}

type GRPCInferenceService_ModelStreamInferClient interface {
	Send(*ModelInferRequest) error
	Recv() (*ModelStreamInferResponse, error)
	grpc.ClientStream
}

type gRPCInferenceServiceModelStreamInferClient struct {
	grpc.ClientStream
}

func (x *gRPCInferenceServiceModelStreamInferClient) Send(m *ModelInferRequest) error {
	return x.ClientStream.SendMsg(m)
}

func (x *gRPCInferenceServiceModelStreamInferClient) Recv() (*ModelStreamInferResponse, error) {
	m := new(ModelStreamInferResponse)
	if err := x.ClientStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

func (c *gRPCInferenceServiceClient) ModelConfig(ctx context.Context, in *ModelConfigRequest, opts ...grpc.CallOption) (*ModelConfigResponse, error) {
	out := new(ModelConfigResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelConfig", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) ModelStatistics(ctx context.Context, in *ModelStatisticsRequest, opts ...grpc.CallOption) (*ModelStatisticsResponse, error) {
	out := new(ModelStatisticsResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/ModelStatistics", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) RepositoryIndex(ctx context.Context, in *RepositoryIndexRequest, opts ...grpc.CallOption) (*RepositoryIndexResponse, error) {
	out := new(RepositoryIndexResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/RepositoryIndex", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) RepositoryModelLoad(ctx context.Context, in *RepositoryModelLoadRequest, opts ...grpc.CallOption) (*RepositoryModelLoadResponse, error) {
	out := new(RepositoryModelLoadResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/RepositoryModelLoad", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) RepositoryModelUnload(ctx context.Context, in *RepositoryModelUnloadRequest, opts ...grpc.CallOption) (*RepositoryModelUnloadResponse, error) {
	out := new(RepositoryModelUnloadResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/RepositoryModelUnload", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) SystemSharedMemoryStatus(ctx context.Context, in *SystemSharedMemoryStatusRequest, opts ...grpc.CallOption) (*SystemSharedMemoryStatusResponse, error) {
	out := new(SystemSharedMemoryStatusResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/SystemSharedMemoryStatus", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) SystemSharedMemoryRegister(ctx context.Context, in *SystemSharedMemoryRegisterRequest, opts ...grpc.CallOption) (*SystemSharedMemoryRegisterResponse, error) {
	out := new(SystemSharedMemoryRegisterResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/SystemSharedMemoryRegister", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) SystemSharedMemoryUnregister(ctx context.Context, in *SystemSharedMemoryUnregisterRequest, opts ...grpc.CallOption) (*SystemSharedMemoryUnregisterResponse, error) {
	out := new(SystemSharedMemoryUnregisterResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/SystemSharedMemoryUnregister", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) CudaSharedMemoryStatus(ctx context.Context, in *CudaSharedMemoryStatusRequest, opts ...grpc.CallOption) (*CudaSharedMemoryStatusResponse, error) {
	out := new(CudaSharedMemoryStatusResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/CudaSharedMemoryStatus", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) CudaSharedMemoryRegister(ctx context.Context, in *CudaSharedMemoryRegisterRequest, opts ...grpc.CallOption) (*CudaSharedMemoryRegisterResponse, error) {
	out := new(CudaSharedMemoryRegisterResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/CudaSharedMemoryRegister", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCInferenceServiceClient) CudaSharedMemoryUnregister(ctx context.Context, in *CudaSharedMemoryUnregisterRequest, opts ...grpc.CallOption) (*CudaSharedMemoryUnregisterResponse, error) {
	out := new(CudaSharedMemoryUnregisterResponse)
	err := c.cc.Invoke(ctx, "/inference.GRPCInferenceService/CudaSharedMemoryUnregister", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// GRPCInferenceServiceServer is the server API for GRPCInferenceService service.
type GRPCInferenceServiceServer interface {
	//@@  .. cpp:var:: rpc ServerLive(ServerLiveRequest) returns
	//@@       (ServerLiveResponse)
	//@@
	//@@     Check liveness of the inference server.
	//@@
	ServerLive(context.Context, *ServerLiveRequest) (*ServerLiveResponse, error)
	//@@  .. cpp:var:: rpc ServerReady(ServerReadyRequest) returns
	//@@       (ServerReadyResponse)
	//@@
	//@@     Check readiness of the inference server.
	//@@
	ServerReady(context.Context, *ServerReadyRequest) (*ServerReadyResponse, error)
	//@@  .. cpp:var:: rpc ModelReady(ModelReadyRequest) returns
	//@@       (ModelReadyResponse)
	//@@
	//@@     Check readiness of a model in the inference server.
	//@@
	ModelReady(context.Context, *ModelReadyRequest) (*ModelReadyResponse, error)
	//@@  .. cpp:var:: rpc ServerMetadata(ServerMetadataRequest) returns
	//@@       (ServerMetadataResponse)
	//@@
	//@@     Get server metadata.
	//@@
	ServerMetadata(context.Context, *ServerMetadataRequest) (*ServerMetadataResponse, error)
	//@@  .. cpp:var:: rpc ModelMetadata(ModelMetadataRequest) returns
	//@@       (ModelMetadataResponse)
	//@@
	//@@     Get model metadata.
	//@@
	ModelMetadata(context.Context, *ModelMetadataRequest) (*ModelMetadataResponse, error)
	//@@  .. cpp:var:: rpc ModelInfer(ModelInferRequest) returns
	//@@       (ModelInferResponse)
	//@@
	//@@     Perform inference using a specific model.
	//@@
	ModelInfer(context.Context, *ModelInferRequest) (*ModelInferResponse, error)
	//@@  .. cpp:var:: rpc ModelStreamInfer(stream ModelInferRequest) returns
	//@@       (stream ModelStreamInferResponse)
	//@@
	//@@     Perform streaming inference.
	//@@
	ModelStreamInfer(GRPCInferenceService_ModelStreamInferServer) error
	//@@  .. cpp:var:: rpc ModelConfig(ModelConfigRequest) returns
	//@@       (ModelConfigResponse)
	//@@
	//@@     Get model configuration.
	//@@
	ModelConfig(context.Context, *ModelConfigRequest) (*ModelConfigResponse, error)
	//@@  .. cpp:var:: rpc ModelStatistics(
	//@@                     ModelStatisticsRequest)
	//@@                   returns (ModelStatisticsResponse)
	//@@
	//@@     Get the cumulative inference statistics for a model.
	//@@
	ModelStatistics(context.Context, *ModelStatisticsRequest) (*ModelStatisticsResponse, error)
	//@@  .. cpp:var:: rpc RepositoryIndex(RepositoryIndexRequest) returns
	//@@       (RepositoryIndexResponse)
	//@@
	//@@     Get the index of model repository contents.
	//@@
	RepositoryIndex(context.Context, *RepositoryIndexRequest) (*RepositoryIndexResponse, error)
	//@@  .. cpp:var:: rpc RepositoryModelLoad(RepositoryModelLoadRequest) returns
	//@@       (RepositoryModelLoadResponse)
	//@@
	//@@     Load or reload a model from a repository.
	//@@
	RepositoryModelLoad(context.Context, *RepositoryModelLoadRequest) (*RepositoryModelLoadResponse, error)
	//@@  .. cpp:var:: rpc RepositoryModelUnload(RepositoryModelUnloadRequest)
	//@@       returns (RepositoryModelUnloadResponse)
	//@@
	//@@     Unload a model.
	//@@
	RepositoryModelUnload(context.Context, *RepositoryModelUnloadRequest) (*RepositoryModelUnloadResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryStatus(
	//@@                     SystemSharedMemoryStatusRequest)
	//@@                   returns (SystemSharedMemoryStatusRespose)
	//@@
	//@@     Get the status of all registered system-shared-memory regions.
	//@@
	SystemSharedMemoryStatus(context.Context, *SystemSharedMemoryStatusRequest) (*SystemSharedMemoryStatusResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryRegister(
	//@@                     SystemSharedMemoryRegisterRequest)
	//@@                   returns (SystemSharedMemoryRegisterResponse)
	//@@
	//@@     Register a system-shared-memory region.
	//@@
	SystemSharedMemoryRegister(context.Context, *SystemSharedMemoryRegisterRequest) (*SystemSharedMemoryRegisterResponse, error)
	//@@  .. cpp:var:: rpc SystemSharedMemoryUnregister(
	//@@                     SystemSharedMemoryUnregisterRequest)
	//@@                   returns (SystemSharedMemoryUnregisterResponse)
	//@@
	//@@     Unregister a system-shared-memory region.
	//@@
	SystemSharedMemoryUnregister(context.Context, *SystemSharedMemoryUnregisterRequest) (*SystemSharedMemoryUnregisterResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryStatus(
	//@@                     CudaSharedMemoryStatusRequest)
	//@@                   returns (CudaSharedMemoryStatusRespose)
	//@@
	//@@     Get the status of all registered CUDA-shared-memory regions.
	//@@
	CudaSharedMemoryStatus(context.Context, *CudaSharedMemoryStatusRequest) (*CudaSharedMemoryStatusResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryRegister(
	//@@                     CudaSharedMemoryRegisterRequest)
	//@@                   returns (CudaSharedMemoryRegisterResponse)
	//@@
	//@@     Register a CUDA-shared-memory region.
	//@@
	CudaSharedMemoryRegister(context.Context, *CudaSharedMemoryRegisterRequest) (*CudaSharedMemoryRegisterResponse, error)
	//@@  .. cpp:var:: rpc CudaSharedMemoryUnregister(
	//@@                     CudaSharedMemoryUnregisterRequest)
	//@@                   returns (CudaSharedMemoryUnregisterResponse)
	//@@
	//@@     Unregister a CUDA-shared-memory region.
	//@@
	CudaSharedMemoryUnregister(context.Context, *CudaSharedMemoryUnregisterRequest) (*CudaSharedMemoryUnregisterResponse, error)
}

// UnimplementedGRPCInferenceServiceServer can be embedded to have forward compatible implementations.
type UnimplementedGRPCInferenceServiceServer struct {
}

func (*UnimplementedGRPCInferenceServiceServer) ServerLive(ctx context.Context, req *ServerLiveRequest) (*ServerLiveResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ServerLive not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ServerReady(ctx context.Context, req *ServerReadyRequest) (*ServerReadyResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ServerReady not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelReady(ctx context.Context, req *ModelReadyRequest) (*ModelReadyResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelReady not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ServerMetadata(ctx context.Context, req *ServerMetadataRequest) (*ServerMetadataResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ServerMetadata not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelMetadata(ctx context.Context, req *ModelMetadataRequest) (*ModelMetadataResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelMetadata not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelInfer(ctx context.Context, req *ModelInferRequest) (*ModelInferResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelInfer not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelStreamInfer(srv GRPCInferenceService_ModelStreamInferServer) error {
	return status.Errorf(codes.Unimplemented, "method ModelStreamInfer not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelConfig(ctx context.Context, req *ModelConfigRequest) (*ModelConfigResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelConfig not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) ModelStatistics(ctx context.Context, req *ModelStatisticsRequest) (*ModelStatisticsResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method ModelStatistics not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) RepositoryIndex(ctx context.Context, req *RepositoryIndexRequest) (*RepositoryIndexResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method RepositoryIndex not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) RepositoryModelLoad(ctx context.Context, req *RepositoryModelLoadRequest) (*RepositoryModelLoadResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method RepositoryModelLoad not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) RepositoryModelUnload(ctx context.Context, req *RepositoryModelUnloadRequest) (*RepositoryModelUnloadResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method RepositoryModelUnload not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) SystemSharedMemoryStatus(ctx context.Context, req *SystemSharedMemoryStatusRequest) (*SystemSharedMemoryStatusResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SystemSharedMemoryStatus not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) SystemSharedMemoryRegister(ctx context.Context, req *SystemSharedMemoryRegisterRequest) (*SystemSharedMemoryRegisterResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SystemSharedMemoryRegister not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) SystemSharedMemoryUnregister(ctx context.Context, req *SystemSharedMemoryUnregisterRequest) (*SystemSharedMemoryUnregisterResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SystemSharedMemoryUnregister not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) CudaSharedMemoryStatus(ctx context.Context, req *CudaSharedMemoryStatusRequest) (*CudaSharedMemoryStatusResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method CudaSharedMemoryStatus not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) CudaSharedMemoryRegister(ctx context.Context, req *CudaSharedMemoryRegisterRequest) (*CudaSharedMemoryRegisterResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method CudaSharedMemoryRegister not implemented")
}
func (*UnimplementedGRPCInferenceServiceServer) CudaSharedMemoryUnregister(ctx context.Context, req *CudaSharedMemoryUnregisterRequest) (*CudaSharedMemoryUnregisterResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method CudaSharedMemoryUnregister not implemented")
}

func RegisterGRPCInferenceServiceServer(s *grpc.Server, srv GRPCInferenceServiceServer) {
	s.RegisterService(&_GRPCInferenceService_serviceDesc, srv)
}

func _GRPCInferenceService_ServerLive_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerLiveRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerLive(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerLive",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerLive(ctx, req.(*ServerLiveRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ServerReady_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerReadyRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerReady(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerReady",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerReady(ctx, req.(*ServerReadyRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelReady_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelReadyRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelReady(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelReady",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelReady(ctx, req.(*ModelReadyRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ServerMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ServerMetadataRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ServerMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ServerMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ServerMetadata(ctx, req.(*ServerMetadataRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelMetadataRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelMetadata(ctx, req.(*ModelMetadataRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelInfer_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelInferRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelInfer(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelInfer",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelInfer(ctx, req.(*ModelInferRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelStreamInfer_Handler(srv interface{}, stream grpc.ServerStream) error {
	return srv.(GRPCInferenceServiceServer).ModelStreamInfer(&gRPCInferenceServiceModelStreamInferServer{stream})
}

type GRPCInferenceService_ModelStreamInferServer interface {
	Send(*ModelStreamInferResponse) error
	Recv() (*ModelInferRequest, error)
	grpc.ServerStream
}

type gRPCInferenceServiceModelStreamInferServer struct {
	grpc.ServerStream
}

func (x *gRPCInferenceServiceModelStreamInferServer) Send(m *ModelStreamInferResponse) error {
	return x.ServerStream.SendMsg(m)
}

func (x *gRPCInferenceServiceModelStreamInferServer) Recv() (*ModelInferRequest, error) {
	m := new(ModelInferRequest)
	if err := x.ServerStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

func _GRPCInferenceService_ModelConfig_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelConfigRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelConfig(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelConfig",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelConfig(ctx, req.(*ModelConfigRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_ModelStatistics_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ModelStatisticsRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).ModelStatistics(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/ModelStatistics",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).ModelStatistics(ctx, req.(*ModelStatisticsRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_RepositoryIndex_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RepositoryIndexRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).RepositoryIndex(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/RepositoryIndex",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).RepositoryIndex(ctx, req.(*RepositoryIndexRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_RepositoryModelLoad_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RepositoryModelLoadRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).RepositoryModelLoad(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/RepositoryModelLoad",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).RepositoryModelLoad(ctx, req.(*RepositoryModelLoadRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_RepositoryModelUnload_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(RepositoryModelUnloadRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).RepositoryModelUnload(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/RepositoryModelUnload",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).RepositoryModelUnload(ctx, req.(*RepositoryModelUnloadRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_SystemSharedMemoryStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SystemSharedMemoryStatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryStatus(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/SystemSharedMemoryStatus",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryStatus(ctx, req.(*SystemSharedMemoryStatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_SystemSharedMemoryRegister_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SystemSharedMemoryRegisterRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryRegister(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/SystemSharedMemoryRegister",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryRegister(ctx, req.(*SystemSharedMemoryRegisterRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_SystemSharedMemoryUnregister_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SystemSharedMemoryUnregisterRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryUnregister(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/SystemSharedMemoryUnregister",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).SystemSharedMemoryUnregister(ctx, req.(*SystemSharedMemoryUnregisterRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_CudaSharedMemoryStatus_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(CudaSharedMemoryStatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryStatus(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/CudaSharedMemoryStatus",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryStatus(ctx, req.(*CudaSharedMemoryStatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_CudaSharedMemoryRegister_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(CudaSharedMemoryRegisterRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryRegister(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/CudaSharedMemoryRegister",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryRegister(ctx, req.(*CudaSharedMemoryRegisterRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCInferenceService_CudaSharedMemoryUnregister_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(CudaSharedMemoryUnregisterRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryUnregister(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/inference.GRPCInferenceService/CudaSharedMemoryUnregister",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCInferenceServiceServer).CudaSharedMemoryUnregister(ctx, req.(*CudaSharedMemoryUnregisterRequest))
	}
	return interceptor(ctx, in, info, handler)
}

var _GRPCInferenceService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "inference.GRPCInferenceService",
	HandlerType: (*GRPCInferenceServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "ServerLive",
			Handler:    _GRPCInferenceService_ServerLive_Handler,
		},
		{
			MethodName: "ServerReady",
			Handler:    _GRPCInferenceService_ServerReady_Handler,
		},
		{
			MethodName: "ModelReady",
			Handler:    _GRPCInferenceService_ModelReady_Handler,
		},
		{
			MethodName: "ServerMetadata",
			Handler:    _GRPCInferenceService_ServerMetadata_Handler,
		},
		{
			MethodName: "ModelMetadata",
			Handler:    _GRPCInferenceService_ModelMetadata_Handler,
		},
		{
			MethodName: "ModelInfer",
			Handler:    _GRPCInferenceService_ModelInfer_Handler,
		},
		{
			MethodName: "ModelConfig",
			Handler:    _GRPCInferenceService_ModelConfig_Handler,
		},
		{
			MethodName: "ModelStatistics",
			Handler:    _GRPCInferenceService_ModelStatistics_Handler,
		},
		{
			MethodName: "RepositoryIndex",
			Handler:    _GRPCInferenceService_RepositoryIndex_Handler,
		},
		{
			MethodName: "RepositoryModelLoad",
			Handler:    _GRPCInferenceService_RepositoryModelLoad_Handler,
		},
		{
			MethodName: "RepositoryModelUnload",
			Handler:    _GRPCInferenceService_RepositoryModelUnload_Handler,
		},
		{
			MethodName: "SystemSharedMemoryStatus",
			Handler:    _GRPCInferenceService_SystemSharedMemoryStatus_Handler,
		},
		{
			MethodName: "SystemSharedMemoryRegister",
			Handler:    _GRPCInferenceService_SystemSharedMemoryRegister_Handler,
		},
		{
			MethodName: "SystemSharedMemoryUnregister",
			Handler:    _GRPCInferenceService_SystemSharedMemoryUnregister_Handler,
		},
		{
			MethodName: "CudaSharedMemoryStatus",
			Handler:    _GRPCInferenceService_CudaSharedMemoryStatus_Handler,
		},
		{
			MethodName: "CudaSharedMemoryRegister",
			Handler:    _GRPCInferenceService_CudaSharedMemoryRegister_Handler,
		},
		{
			MethodName: "CudaSharedMemoryUnregister",
			Handler:    _GRPCInferenceService_CudaSharedMemoryUnregister_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "ModelStreamInfer",
			Handler:       _GRPCInferenceService_ModelStreamInfer_Handler,
			ServerStreams: true,
			ClientStreams: true,
		},
	},
	Metadata: "grpc_service.proto",
}
