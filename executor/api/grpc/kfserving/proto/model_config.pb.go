// Code generated by protoc-gen-go. DO NOT EDIT.
// source: model_config.proto

package proto

import (
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

//@@
//@@.. cpp:enum:: DataType
//@@
//@@   Data types supported for input and output tensors.
//@@
type DataType int32

const (
	//@@  .. cpp:enumerator:: DataType::INVALID = 0
	DataType_TYPE_INVALID DataType = 0
	//@@  .. cpp:enumerator:: DataType::BOOL = 1
	DataType_TYPE_BOOL DataType = 1
	//@@  .. cpp:enumerator:: DataType::UINT8 = 2
	DataType_TYPE_UINT8 DataType = 2
	//@@  .. cpp:enumerator:: DataType::UINT16 = 3
	DataType_TYPE_UINT16 DataType = 3
	//@@  .. cpp:enumerator:: DataType::UINT32 = 4
	DataType_TYPE_UINT32 DataType = 4
	//@@  .. cpp:enumerator:: DataType::UINT64 = 5
	DataType_TYPE_UINT64 DataType = 5
	//@@  .. cpp:enumerator:: DataType::INT8 = 6
	DataType_TYPE_INT8 DataType = 6
	//@@  .. cpp:enumerator:: DataType::INT16 = 7
	DataType_TYPE_INT16 DataType = 7
	//@@  .. cpp:enumerator:: DataType::INT32 = 8
	DataType_TYPE_INT32 DataType = 8
	//@@  .. cpp:enumerator:: DataType::INT64 = 9
	DataType_TYPE_INT64 DataType = 9
	//@@  .. cpp:enumerator:: DataType::FP16 = 10
	DataType_TYPE_FP16 DataType = 10
	//@@  .. cpp:enumerator:: DataType::FP32 = 11
	DataType_TYPE_FP32 DataType = 11
	//@@  .. cpp:enumerator:: DataType::FP64 = 12
	DataType_TYPE_FP64 DataType = 12
	//@@  .. cpp:enumerator:: DataType::STRING = 13
	DataType_TYPE_STRING DataType = 13
)

var DataType_name = map[int32]string{
	0:  "TYPE_INVALID",
	1:  "TYPE_BOOL",
	2:  "TYPE_UINT8",
	3:  "TYPE_UINT16",
	4:  "TYPE_UINT32",
	5:  "TYPE_UINT64",
	6:  "TYPE_INT8",
	7:  "TYPE_INT16",
	8:  "TYPE_INT32",
	9:  "TYPE_INT64",
	10: "TYPE_FP16",
	11: "TYPE_FP32",
	12: "TYPE_FP64",
	13: "TYPE_STRING",
}

var DataType_value = map[string]int32{
	"TYPE_INVALID": 0,
	"TYPE_BOOL":    1,
	"TYPE_UINT8":   2,
	"TYPE_UINT16":  3,
	"TYPE_UINT32":  4,
	"TYPE_UINT64":  5,
	"TYPE_INT8":    6,
	"TYPE_INT16":   7,
	"TYPE_INT32":   8,
	"TYPE_INT64":   9,
	"TYPE_FP16":    10,
	"TYPE_FP32":    11,
	"TYPE_FP64":    12,
	"TYPE_STRING":  13,
}

func (x DataType) String() string {
	return proto.EnumName(DataType_name, int32(x))
}

func (DataType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0}
}

//@@
//@@  .. cpp:enum:: Kind
//@@
//@@     Kind of this instance group.
//@@
type ModelInstanceGroup_Kind int32

const (
	//@@    .. cpp:enumerator:: Kind::KIND_AUTO = 0
	//@@
	//@@       This instance group represents instances that can run on either
	//@@       CPU or GPU. If all GPUs listed in 'gpus' are available then
	//@@       instances will be created on GPU(s), otherwise instances will
	//@@       be created on CPU.
	//@@
	ModelInstanceGroup_KIND_AUTO ModelInstanceGroup_Kind = 0
	//@@    .. cpp:enumerator:: Kind::KIND_GPU = 1
	//@@
	//@@       This instance group represents instances that must run on the
	//@@       GPU.
	//@@
	ModelInstanceGroup_KIND_GPU ModelInstanceGroup_Kind = 1
	//@@    .. cpp:enumerator:: Kind::KIND_CPU = 2
	//@@
	//@@       This instance group represents instances that must run on the
	//@@       CPU.
	//@@
	ModelInstanceGroup_KIND_CPU ModelInstanceGroup_Kind = 2
	//@@    .. cpp:enumerator:: Kind::KIND_MODEL = 3
	//@@
	//@@       This instance group represents instances that should run on the
	//@@       CPU and/or GPU(s) as specified by the model or backend itself.
	//@@       The inference server will not override the model/backend
	//@@       settings.
	//@@       Currently, this option is supported only for Tensorflow models.
	//@@
	ModelInstanceGroup_KIND_MODEL ModelInstanceGroup_Kind = 3
)

var ModelInstanceGroup_Kind_name = map[int32]string{
	0: "KIND_AUTO",
	1: "KIND_GPU",
	2: "KIND_CPU",
	3: "KIND_MODEL",
}

var ModelInstanceGroup_Kind_value = map[string]int32{
	"KIND_AUTO":  0,
	"KIND_GPU":   1,
	"KIND_CPU":   2,
	"KIND_MODEL": 3,
}

func (x ModelInstanceGroup_Kind) String() string {
	return proto.EnumName(ModelInstanceGroup_Kind_name, int32(x))
}

func (ModelInstanceGroup_Kind) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0, 0}
}

//@@
//@@  .. cpp:enum:: Format
//@@
//@@     The format for the input.
//@@
type ModelInput_Format int32

const (
	//@@    .. cpp:enumerator:: Format::FORMAT_NONE = 0
	//@@
	//@@       The input has no specific format. This is the default.
	//@@
	ModelInput_FORMAT_NONE ModelInput_Format = 0
	//@@    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
	//@@
	//@@       HWC image format. Tensors with this format require 3 dimensions
	//@@       if the model does not support batching (max_batch_size = 0) or 4
	//@@       dimensions if the model does support batching (max_batch_size
	//@@       >= 1). In either case the 'dims' below should only specify the
	//@@       3 non-batch dimensions (i.e. HWC or CHW).
	//@@
	ModelInput_FORMAT_NHWC ModelInput_Format = 1
	//@@    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
	//@@
	//@@       CHW image format. Tensors with this format require 3 dimensions
	//@@       if the model does not support batching (max_batch_size = 0) or 4
	//@@       dimensions if the model does support batching (max_batch_size
	//@@       >= 1). In either case the 'dims' below should only specify the
	//@@       3 non-batch dimensions (i.e. HWC or CHW).
	//@@
	ModelInput_FORMAT_NCHW ModelInput_Format = 2
)

var ModelInput_Format_name = map[int32]string{
	0: "FORMAT_NONE",
	1: "FORMAT_NHWC",
	2: "FORMAT_NCHW",
}

var ModelInput_Format_value = map[string]int32{
	"FORMAT_NONE": 0,
	"FORMAT_NHWC": 1,
	"FORMAT_NCHW": 2,
}

func (x ModelInput_Format) String() string {
	return proto.EnumName(ModelInput_Format_name, int32(x))
}

func (ModelInput_Format) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{2, 0}
}

//@@
//@@  .. cpp:enum:: ModelPriority
//@@
//@@     Model priorities. A model will be given scheduling and execution
//@@     preference over models at lower priorities. Current model
//@@     priorities only work for TensorRT models.
//@@
type ModelOptimizationPolicy_ModelPriority int32

const (
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
	//@@
	//@@       The default model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_DEFAULT ModelOptimizationPolicy_ModelPriority = 0
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
	//@@
	//@@       The maximum model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_MAX ModelOptimizationPolicy_ModelPriority = 1
	//@@    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
	//@@
	//@@       The minimum model priority.
	//@@
	ModelOptimizationPolicy_PRIORITY_MIN ModelOptimizationPolicy_ModelPriority = 2
)

var ModelOptimizationPolicy_ModelPriority_name = map[int32]string{
	0: "PRIORITY_DEFAULT",
	1: "PRIORITY_MAX",
	2: "PRIORITY_MIN",
}

var ModelOptimizationPolicy_ModelPriority_value = map[string]int32{
	"PRIORITY_DEFAULT": 0,
	"PRIORITY_MAX":     1,
	"PRIORITY_MIN":     2,
}

func (x ModelOptimizationPolicy_ModelPriority) String() string {
	return proto.EnumName(ModelOptimizationPolicy_ModelPriority_name, int32(x))
}

func (ModelOptimizationPolicy_ModelPriority) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 0}
}

//@@
//@@  .. cpp:enum:: TimeoutAction
//@@
//@@     The action applied to timed-out requests.
//@@
type ModelQueuePolicy_TimeoutAction int32

const (
	//@@    .. cpp:enumerator:: Action::REJECT = 0
	//@@
	//@@       Reject the request and return error message accordingly.
	//@@
	ModelQueuePolicy_REJECT ModelQueuePolicy_TimeoutAction = 0
	//@@    .. cpp:enumerator:: Action::DELAY = 1
	//@@
	//@@       Delay the request until all other requests at the same
	//@@       (or higher) priority levels that have not reached their timeouts
	//@@       are processed. A delayed request will eventually be processed,
	//@@       but may be delayed indefinitely due to newly arriving requests.
	//@@
	ModelQueuePolicy_DELAY ModelQueuePolicy_TimeoutAction = 1
)

var ModelQueuePolicy_TimeoutAction_name = map[int32]string{
	0: "REJECT",
	1: "DELAY",
}

var ModelQueuePolicy_TimeoutAction_value = map[string]int32{
	"REJECT": 0,
	"DELAY":  1,
}

func (x ModelQueuePolicy_TimeoutAction) String() string {
	return proto.EnumName(ModelQueuePolicy_TimeoutAction_name, int32(x))
}

func (ModelQueuePolicy_TimeoutAction) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{6, 0}
}

//@@
//@@    .. cpp:enum:: Kind
//@@
//@@       The kind of the control.
//@@
type ModelSequenceBatching_Control_Kind int32

const (
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
	//@@
	//@@         A new sequence is/is-not starting. If true a sequence is
	//@@         starting, if false a sequence is continuing. Must
	//@@         specify either int32_false_true or fp32_false_true for
	//@@         this control. This control is optional.
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_START ModelSequenceBatching_Control_Kind = 0
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
	//@@
	//@@         A sequence is/is-not ready for inference. If true the
	//@@         input tensor data is valid and should be used. If false
	//@@         the input tensor data is invalid and inferencing should
	//@@         be "skipped".  Must specify either int32_false_true or
	//@@         fp32_false_true for this control. This control is optional.
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_READY ModelSequenceBatching_Control_Kind = 1
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
	//@@
	//@@         A sequence is/is-not ending. If true a sequence is
	//@@         ending, if false a sequence is continuing. Must
	//@@         specify either int32_false_true or fp32_false_true for
	//@@         this control. This control is optional.
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_END ModelSequenceBatching_Control_Kind = 2
	//@@      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
	//@@
	//@@         The correlation ID of the sequence. The correlation ID
	//@@         is an uint64_t value that is communicated in whole or
	//@@         in part by the tensor. The tensor's datatype must be
	//@@         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
	//@@         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
	//@@         the correlation ID will be truncated to the low-order 32
	//@@         bits. This control is optional.
	//@@
	ModelSequenceBatching_Control_CONTROL_SEQUENCE_CORRID ModelSequenceBatching_Control_Kind = 3
)

var ModelSequenceBatching_Control_Kind_name = map[int32]string{
	0: "CONTROL_SEQUENCE_START",
	1: "CONTROL_SEQUENCE_READY",
	2: "CONTROL_SEQUENCE_END",
	3: "CONTROL_SEQUENCE_CORRID",
}

var ModelSequenceBatching_Control_Kind_value = map[string]int32{
	"CONTROL_SEQUENCE_START":  0,
	"CONTROL_SEQUENCE_READY":  1,
	"CONTROL_SEQUENCE_END":    2,
	"CONTROL_SEQUENCE_CORRID": 3,
}

func (x ModelSequenceBatching_Control_Kind) String() string {
	return proto.EnumName(ModelSequenceBatching_Control_Kind_name, int32(x))
}

func (ModelSequenceBatching_Control_Kind) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 0, 0}
}

//@@
//@@.. cpp:var:: message ModelInstanceGroup
//@@
//@@   A group of one or more instances of a model and resources made
//@@   available for those instances.
//@@
type ModelInstanceGroup struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     Optional name of this group of instances. If not specified the
	//@@     name will be formed as <model name>_<group number>. The name of
	//@@     individual instances will be further formed by a unique instance
	//@@     number and GPU index:
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: Kind kind
	//@@
	//@@     The kind of this instance group. Default is KIND_AUTO. If
	//@@     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
	//@@     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
	//@@     and 'gpu' cannot be specified.
	//@@
	Kind ModelInstanceGroup_Kind `protobuf:"varint,4,opt,name=kind,proto3,enum=nvidia.inferenceserver.ModelInstanceGroup_Kind" json:"kind,omitempty"`
	//@@  .. cpp:var:: int32 count
	//@@
	//@@     For a group assigned to GPU, the number of instances created for
	//@@     each GPU listed in 'gpus'. For a group assigned to CPU the number
	//@@     of instances created. Default is 1.
	Count int32 `protobuf:"varint,2,opt,name=count,proto3" json:"count,omitempty"`
	//@@  .. cpp:var:: int32 gpus (repeated)
	//@@
	//@@     GPU(s) where instances should be available. For each GPU listed,
	//@@     'count' instances of the model will be available. Setting 'gpus'
	//@@     to empty (or not specifying at all) is eqivalent to listing all
	//@@     available GPUs.
	//@@
	Gpus []int32 `protobuf:"varint,3,rep,packed,name=gpus,proto3" json:"gpus,omitempty"`
	//@@  .. cpp:var:: string profile (repeated)
	//@@
	//@@     For TensorRT models, using inputs with dynamic shape, this
	//@@     parameter specifies a set of optimization profiles available to this
	//@@     instance group. The inference server will choose the optimal profile
	//@@     based on the shapes of the input tensors. This field should lie
	//@@     between 0 and <TotalNumberOfOptimizationProfilesInPlanModel> - 1
	//@@     and be specified only for TensorRT backend, otherwise an error will
	//@@     be generated.
	//@@
	Profile              []string `protobuf:"bytes,5,rep,name=profile,proto3" json:"profile,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInstanceGroup) Reset()         { *m = ModelInstanceGroup{} }
func (m *ModelInstanceGroup) String() string { return proto.CompactTextString(m) }
func (*ModelInstanceGroup) ProtoMessage()    {}
func (*ModelInstanceGroup) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{0}
}

func (m *ModelInstanceGroup) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInstanceGroup.Unmarshal(m, b)
}
func (m *ModelInstanceGroup) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInstanceGroup.Marshal(b, m, deterministic)
}
func (m *ModelInstanceGroup) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInstanceGroup.Merge(m, src)
}
func (m *ModelInstanceGroup) XXX_Size() int {
	return xxx_messageInfo_ModelInstanceGroup.Size(m)
}
func (m *ModelInstanceGroup) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInstanceGroup.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInstanceGroup proto.InternalMessageInfo

func (m *ModelInstanceGroup) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInstanceGroup) GetKind() ModelInstanceGroup_Kind {
	if m != nil {
		return m.Kind
	}
	return ModelInstanceGroup_KIND_AUTO
}

func (m *ModelInstanceGroup) GetCount() int32 {
	if m != nil {
		return m.Count
	}
	return 0
}

func (m *ModelInstanceGroup) GetGpus() []int32 {
	if m != nil {
		return m.Gpus
	}
	return nil
}

func (m *ModelInstanceGroup) GetProfile() []string {
	if m != nil {
		return m.Profile
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelTensorReshape
//@@
//@@   Reshape specification for input and output tensors.
//@@
type ModelTensorReshape struct {
	//@@  .. cpp:var:: int64 shape (repeated)
	//@@
	//@@     The shape to use for reshaping.
	//@@
	Shape                []int64  `protobuf:"varint,1,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelTensorReshape) Reset()         { *m = ModelTensorReshape{} }
func (m *ModelTensorReshape) String() string { return proto.CompactTextString(m) }
func (*ModelTensorReshape) ProtoMessage()    {}
func (*ModelTensorReshape) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{1}
}

func (m *ModelTensorReshape) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelTensorReshape.Unmarshal(m, b)
}
func (m *ModelTensorReshape) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelTensorReshape.Marshal(b, m, deterministic)
}
func (m *ModelTensorReshape) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelTensorReshape.Merge(m, src)
}
func (m *ModelTensorReshape) XXX_Size() int {
	return xxx_messageInfo_ModelTensorReshape.Size(m)
}
func (m *ModelTensorReshape) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelTensorReshape.DiscardUnknown(m)
}

var xxx_messageInfo_ModelTensorReshape proto.InternalMessageInfo

func (m *ModelTensorReshape) GetShape() []int64 {
	if m != nil {
		return m.Shape
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelInput
//@@
//@@   An input required by the model.
//@@
type ModelInput struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the input.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: DataType data_type
	//@@
	//@@     The data-type of the input.
	//@@
	DataType DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	//@@  .. cpp:var:: Format format
	//@@
	//@@     The format of the input. Optional.
	//@@
	Format ModelInput_Format `protobuf:"varint,3,opt,name=format,proto3,enum=nvidia.inferenceserver.ModelInput_Format" json:"format,omitempty"`
	//@@  .. cpp:var:: int64 dims (repeated)
	//@@
	//@@     The dimensions/shape of the input tensor that must be provided
	//@@     when invoking the inference API for this model.
	//@@
	Dims []int64 `protobuf:"varint,4,rep,packed,name=dims,proto3" json:"dims,omitempty"`
	//@@  .. cpp:var:: ModelTensorReshape reshape
	//@@
	//@@     The shape expected for this input by the backend. The input will
	//@@     be reshaped to this before being presented to the backend. The
	//@@     reshape must have the same number of elements as the input shape
	//@@     specified by 'dims'. Optional.
	//@@
	Reshape *ModelTensorReshape `protobuf:"bytes,5,opt,name=reshape,proto3" json:"reshape,omitempty"`
	//@@  .. cpp:var:: bool is_shape_tensor
	//@@
	//@@     Whether or not the input is a shape tensor to the model. This field
	//@@     is currently supported only for the TensorRT model. An error will be
	//@@     generated if this specification does not comply with underlying
	//@@     model.
	//@@
	IsShapeTensor bool `protobuf:"varint,6,opt,name=is_shape_tensor,json=isShapeTensor,proto3" json:"is_shape_tensor,omitempty"`
	//@@  .. cpp:var:: bool allow_ragged_batch
	//@@
	//@@     Whether or not the input is allowed to be "ragged" in a dynamically
	//@@     created batch. Default is false indicating that two requests will
	//@@     only be batched if this tensor has the same shape in both requests.
	//@@     True indicates that two requests can be batched even if this tensor
	//@@     has a different shape in each request. A true value is currently
	//@@     supported only for custom models.
	//@@
	AllowRaggedBatch     bool     `protobuf:"varint,7,opt,name=allow_ragged_batch,json=allowRaggedBatch,proto3" json:"allow_ragged_batch,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelInput) Reset()         { *m = ModelInput{} }
func (m *ModelInput) String() string { return proto.CompactTextString(m) }
func (*ModelInput) ProtoMessage()    {}
func (*ModelInput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{2}
}

func (m *ModelInput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelInput.Unmarshal(m, b)
}
func (m *ModelInput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelInput.Marshal(b, m, deterministic)
}
func (m *ModelInput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelInput.Merge(m, src)
}
func (m *ModelInput) XXX_Size() int {
	return xxx_messageInfo_ModelInput.Size(m)
}
func (m *ModelInput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelInput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelInput proto.InternalMessageInfo

func (m *ModelInput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelInput) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

func (m *ModelInput) GetFormat() ModelInput_Format {
	if m != nil {
		return m.Format
	}
	return ModelInput_FORMAT_NONE
}

func (m *ModelInput) GetDims() []int64 {
	if m != nil {
		return m.Dims
	}
	return nil
}

func (m *ModelInput) GetReshape() *ModelTensorReshape {
	if m != nil {
		return m.Reshape
	}
	return nil
}

func (m *ModelInput) GetIsShapeTensor() bool {
	if m != nil {
		return m.IsShapeTensor
	}
	return false
}

func (m *ModelInput) GetAllowRaggedBatch() bool {
	if m != nil {
		return m.AllowRaggedBatch
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelOutput
//@@
//@@   An output produced by the model.
//@@
type ModelOutput struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the output.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: DataType data_type
	//@@
	//@@     The data-type of the output.
	//@@
	DataType DataType `protobuf:"varint,2,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	//@@  .. cpp:var:: int64 dims (repeated)
	//@@
	//@@     The dimensions/shape of the output tensor.
	//@@
	Dims []int64 `protobuf:"varint,3,rep,packed,name=dims,proto3" json:"dims,omitempty"`
	//@@  .. cpp:var:: ModelTensorReshape reshape
	//@@
	//@@     The shape produced for this output by the backend. The output will
	//@@     be reshaped from this to the shape specifed in 'dims' before being
	//@@     returned in the inference response. The reshape must have the same
	//@@     number of elements as the output shape specified by 'dims'. Optional.
	//@@
	Reshape *ModelTensorReshape `protobuf:"bytes,5,opt,name=reshape,proto3" json:"reshape,omitempty"`
	//@@  .. cpp:var:: string label_filename
	//@@
	//@@     The label file associated with this output. Should be specified only
	//@@     for outputs that represent classifications. Optional.
	//@@
	LabelFilename string `protobuf:"bytes,4,opt,name=label_filename,json=labelFilename,proto3" json:"label_filename,omitempty"`
	//@@  .. cpp:var:: bool is_shape_tensor
	//@@
	//@@     Whether or not the output is a shape tensor to the model. This field
	//@@     is currently supported only for the TensorRT model. An error will be
	//@@     generated if this specification does not comply with underlying
	//@@     model.
	//@@
	IsShapeTensor        bool     `protobuf:"varint,6,opt,name=is_shape_tensor,json=isShapeTensor,proto3" json:"is_shape_tensor,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOutput) Reset()         { *m = ModelOutput{} }
func (m *ModelOutput) String() string { return proto.CompactTextString(m) }
func (*ModelOutput) ProtoMessage()    {}
func (*ModelOutput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{3}
}

func (m *ModelOutput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOutput.Unmarshal(m, b)
}
func (m *ModelOutput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOutput.Marshal(b, m, deterministic)
}
func (m *ModelOutput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOutput.Merge(m, src)
}
func (m *ModelOutput) XXX_Size() int {
	return xxx_messageInfo_ModelOutput.Size(m)
}
func (m *ModelOutput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOutput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOutput proto.InternalMessageInfo

func (m *ModelOutput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelOutput) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

func (m *ModelOutput) GetDims() []int64 {
	if m != nil {
		return m.Dims
	}
	return nil
}

func (m *ModelOutput) GetReshape() *ModelTensorReshape {
	if m != nil {
		return m.Reshape
	}
	return nil
}

func (m *ModelOutput) GetLabelFilename() string {
	if m != nil {
		return m.LabelFilename
	}
	return ""
}

func (m *ModelOutput) GetIsShapeTensor() bool {
	if m != nil {
		return m.IsShapeTensor
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelVersionPolicy
//@@
//@@   Policy indicating which versions of a model should be made
//@@   available by the inference server.
//@@
type ModelVersionPolicy struct {
	//@@  .. cpp:var:: oneof policy_choice
	//@@
	//@@     Each model must implement only a single version policy. The
	//@@     default policy is 'Latest'.
	//@@
	//
	// Types that are valid to be assigned to PolicyChoice:
	//	*ModelVersionPolicy_Latest_
	//	*ModelVersionPolicy_All_
	//	*ModelVersionPolicy_Specific_
	PolicyChoice         isModelVersionPolicy_PolicyChoice `protobuf_oneof:"policy_choice"`
	XXX_NoUnkeyedLiteral struct{}                          `json:"-"`
	XXX_unrecognized     []byte                            `json:"-"`
	XXX_sizecache        int32                             `json:"-"`
}

func (m *ModelVersionPolicy) Reset()         { *m = ModelVersionPolicy{} }
func (m *ModelVersionPolicy) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy) ProtoMessage()    {}
func (*ModelVersionPolicy) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4}
}

func (m *ModelVersionPolicy) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy.Unmarshal(m, b)
}
func (m *ModelVersionPolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy.Merge(m, src)
}
func (m *ModelVersionPolicy) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy.Size(m)
}
func (m *ModelVersionPolicy) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy proto.InternalMessageInfo

type isModelVersionPolicy_PolicyChoice interface {
	isModelVersionPolicy_PolicyChoice()
}

type ModelVersionPolicy_Latest_ struct {
	Latest *ModelVersionPolicy_Latest `protobuf:"bytes,1,opt,name=latest,proto3,oneof"`
}

type ModelVersionPolicy_All_ struct {
	All *ModelVersionPolicy_All `protobuf:"bytes,2,opt,name=all,proto3,oneof"`
}

type ModelVersionPolicy_Specific_ struct {
	Specific *ModelVersionPolicy_Specific `protobuf:"bytes,3,opt,name=specific,proto3,oneof"`
}

func (*ModelVersionPolicy_Latest_) isModelVersionPolicy_PolicyChoice() {}

func (*ModelVersionPolicy_All_) isModelVersionPolicy_PolicyChoice() {}

func (*ModelVersionPolicy_Specific_) isModelVersionPolicy_PolicyChoice() {}

func (m *ModelVersionPolicy) GetPolicyChoice() isModelVersionPolicy_PolicyChoice {
	if m != nil {
		return m.PolicyChoice
	}
	return nil
}

func (m *ModelVersionPolicy) GetLatest() *ModelVersionPolicy_Latest {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_Latest_); ok {
		return x.Latest
	}
	return nil
}

func (m *ModelVersionPolicy) GetAll() *ModelVersionPolicy_All {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_All_); ok {
		return x.All
	}
	return nil
}

func (m *ModelVersionPolicy) GetSpecific() *ModelVersionPolicy_Specific {
	if x, ok := m.GetPolicyChoice().(*ModelVersionPolicy_Specific_); ok {
		return x.Specific
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelVersionPolicy) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelVersionPolicy_Latest_)(nil),
		(*ModelVersionPolicy_All_)(nil),
		(*ModelVersionPolicy_Specific_)(nil),
	}
}

//@@  .. cpp:var:: message Latest
//@@
//@@     Serve only the latest version(s) of a model. This is
//@@     the default policy.
//@@
type ModelVersionPolicy_Latest struct {
	//@@    .. cpp:var:: uint32 num_versions
	//@@
	//@@       Serve only the 'num_versions' highest-numbered versions. T
	//@@       The default value of 'num_versions' is 1, indicating that by
	//@@       default only the single highest-number version of a
	//@@       model will be served.
	//@@
	NumVersions          uint32   `protobuf:"varint,1,opt,name=num_versions,json=numVersions,proto3" json:"num_versions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_Latest) Reset()         { *m = ModelVersionPolicy_Latest{} }
func (m *ModelVersionPolicy_Latest) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_Latest) ProtoMessage()    {}
func (*ModelVersionPolicy_Latest) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 0}
}

func (m *ModelVersionPolicy_Latest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_Latest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_Latest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_Latest.Merge(m, src)
}
func (m *ModelVersionPolicy_Latest) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_Latest.Size(m)
}
func (m *ModelVersionPolicy_Latest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_Latest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_Latest proto.InternalMessageInfo

func (m *ModelVersionPolicy_Latest) GetNumVersions() uint32 {
	if m != nil {
		return m.NumVersions
	}
	return 0
}

//@@  .. cpp:var:: message All
//@@
//@@     Serve all versions of the model.
//@@
type ModelVersionPolicy_All struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_All) Reset()         { *m = ModelVersionPolicy_All{} }
func (m *ModelVersionPolicy_All) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_All) ProtoMessage()    {}
func (*ModelVersionPolicy_All) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 1}
}

func (m *ModelVersionPolicy_All) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_All.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_All) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_All.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_All) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_All.Merge(m, src)
}
func (m *ModelVersionPolicy_All) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_All.Size(m)
}
func (m *ModelVersionPolicy_All) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_All.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_All proto.InternalMessageInfo

//@@  .. cpp:var:: message Specific
//@@
//@@     Serve only specific versions of the model.
//@@
type ModelVersionPolicy_Specific struct {
	//@@    .. cpp:var:: int64 versions (repeated)
	//@@
	//@@       The specific versions of the model that will be served.
	//@@
	Versions             []int64  `protobuf:"varint,1,rep,packed,name=versions,proto3" json:"versions,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelVersionPolicy_Specific) Reset()         { *m = ModelVersionPolicy_Specific{} }
func (m *ModelVersionPolicy_Specific) String() string { return proto.CompactTextString(m) }
func (*ModelVersionPolicy_Specific) ProtoMessage()    {}
func (*ModelVersionPolicy_Specific) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{4, 2}
}

func (m *ModelVersionPolicy_Specific) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Unmarshal(m, b)
}
func (m *ModelVersionPolicy_Specific) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Marshal(b, m, deterministic)
}
func (m *ModelVersionPolicy_Specific) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelVersionPolicy_Specific.Merge(m, src)
}
func (m *ModelVersionPolicy_Specific) XXX_Size() int {
	return xxx_messageInfo_ModelVersionPolicy_Specific.Size(m)
}
func (m *ModelVersionPolicy_Specific) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelVersionPolicy_Specific.DiscardUnknown(m)
}

var xxx_messageInfo_ModelVersionPolicy_Specific proto.InternalMessageInfo

func (m *ModelVersionPolicy_Specific) GetVersions() []int64 {
	if m != nil {
		return m.Versions
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelOptimizationPolicy
//@@
//@@   Optimization settings for a model. These settings control if/how a
//@@   model is optimized and prioritized by the backend framework when
//@@   it is loaded.
//@@
type ModelOptimizationPolicy struct {
	//@@  .. cpp:var:: Graph graph
	//@@
	//@@     The graph optimization setting for the model. Optional.
	//@@
	Graph *ModelOptimizationPolicy_Graph `protobuf:"bytes,1,opt,name=graph,proto3" json:"graph,omitempty"`
	//@@  .. cpp:var:: ModelPriority priority
	//@@
	//@@     The priority setting for the model. Optional.
	//@@
	Priority ModelOptimizationPolicy_ModelPriority `protobuf:"varint,2,opt,name=priority,proto3,enum=nvidia.inferenceserver.ModelOptimizationPolicy_ModelPriority" json:"priority,omitempty"`
	//@@  .. cpp:var:: Cuda cuda
	//@@
	//@@     CUDA-specific optimization settings. Optional.
	//@@
	Cuda *ModelOptimizationPolicy_Cuda `protobuf:"bytes,3,opt,name=cuda,proto3" json:"cuda,omitempty"`
	//@@  .. cpp:var:: ExecutionAccelerators execution_accelerators
	//@@
	//@@     The accelerators used for the model. Optional.
	//@@
	ExecutionAccelerators *ModelOptimizationPolicy_ExecutionAccelerators `protobuf:"bytes,4,opt,name=execution_accelerators,json=executionAccelerators,proto3" json:"execution_accelerators,omitempty"`
	//@@  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
	//@@
	//@@     Use pinned memory buffer when the data transfer for inputs
	//@@     is between GPU memory and non-pinned system memory.
	//@@     Default is true.
	//@@
	InputPinnedMemory *ModelOptimizationPolicy_PinnedMemoryBuffer `protobuf:"bytes,5,opt,name=input_pinned_memory,json=inputPinnedMemory,proto3" json:"input_pinned_memory,omitempty"`
	//@@  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
	//@@
	//@@     Use pinned memory buffer when the data transfer for outputs
	//@@     is between GPU memory and non-pinned system memory.
	//@@     Default is true.
	//@@
	OutputPinnedMemory   *ModelOptimizationPolicy_PinnedMemoryBuffer `protobuf:"bytes,6,opt,name=output_pinned_memory,json=outputPinnedMemory,proto3" json:"output_pinned_memory,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                                    `json:"-"`
	XXX_unrecognized     []byte                                      `json:"-"`
	XXX_sizecache        int32                                       `json:"-"`
}

func (m *ModelOptimizationPolicy) Reset()         { *m = ModelOptimizationPolicy{} }
func (m *ModelOptimizationPolicy) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy) ProtoMessage()    {}
func (*ModelOptimizationPolicy) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5}
}

func (m *ModelOptimizationPolicy) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy.Merge(m, src)
}
func (m *ModelOptimizationPolicy) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy.Size(m)
}
func (m *ModelOptimizationPolicy) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy proto.InternalMessageInfo

func (m *ModelOptimizationPolicy) GetGraph() *ModelOptimizationPolicy_Graph {
	if m != nil {
		return m.Graph
	}
	return nil
}

func (m *ModelOptimizationPolicy) GetPriority() ModelOptimizationPolicy_ModelPriority {
	if m != nil {
		return m.Priority
	}
	return ModelOptimizationPolicy_PRIORITY_DEFAULT
}

func (m *ModelOptimizationPolicy) GetCuda() *ModelOptimizationPolicy_Cuda {
	if m != nil {
		return m.Cuda
	}
	return nil
}

func (m *ModelOptimizationPolicy) GetExecutionAccelerators() *ModelOptimizationPolicy_ExecutionAccelerators {
	if m != nil {
		return m.ExecutionAccelerators
	}
	return nil
}

func (m *ModelOptimizationPolicy) GetInputPinnedMemory() *ModelOptimizationPolicy_PinnedMemoryBuffer {
	if m != nil {
		return m.InputPinnedMemory
	}
	return nil
}

func (m *ModelOptimizationPolicy) GetOutputPinnedMemory() *ModelOptimizationPolicy_PinnedMemoryBuffer {
	if m != nil {
		return m.OutputPinnedMemory
	}
	return nil
}

//@@
//@@  .. cpp:var:: message Graph
//@@
//@@     Enable generic graph optimization of the model. If not specified
//@@     the framework's default level of optimization is used. Supports
//@@     TensorFlow graphdef and savedmodel and Onnx models. For TensorFlow
//@@     causes XLA to be enabled/disabled for the model. For Onnx defaults
//@@     to enabling all optimizations, -1 enables only basic optimizations,
//@@     +1 enables only basic and extended optimizations.
//@@
type ModelOptimizationPolicy_Graph struct {
	//@@    .. cpp:var:: int32 level
	//@@
	//@@       The optimization level. Defaults to 0 (zero) if not specified.
	//@@
	//@@         - -1: Disabled
	//@@         -  0: Framework default
	//@@         -  1+: Enable optimization level (greater values indicate
	//@@            higher optimization levels)
	//@@
	Level                int32    `protobuf:"varint,1,opt,name=level,proto3" json:"level,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOptimizationPolicy_Graph) Reset()         { *m = ModelOptimizationPolicy_Graph{} }
func (m *ModelOptimizationPolicy_Graph) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy_Graph) ProtoMessage()    {}
func (*ModelOptimizationPolicy_Graph) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 0}
}

func (m *ModelOptimizationPolicy_Graph) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_Graph.Merge(m, src)
}
func (m *ModelOptimizationPolicy_Graph) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_Graph.Size(m)
}
func (m *ModelOptimizationPolicy_Graph) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_Graph.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_Graph proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_Graph) GetLevel() int32 {
	if m != nil {
		return m.Level
	}
	return 0
}

//@@
//@@  .. cpp:var:: message Cuda
//@@
//@@     CUDA-specific optimization settings.
//@@
type ModelOptimizationPolicy_Cuda struct {
	//@@    .. cpp:var:: bool graphs
	//@@
	//@@       Use CUDA graphs API to capture model operations and execute
	//@@       them more efficiently. Currently only recognized by TensorRT
	//@@       backend.
	//@@
	Graphs               bool     `protobuf:"varint,1,opt,name=graphs,proto3" json:"graphs,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOptimizationPolicy_Cuda) Reset()         { *m = ModelOptimizationPolicy_Cuda{} }
func (m *ModelOptimizationPolicy_Cuda) String() string { return proto.CompactTextString(m) }
func (*ModelOptimizationPolicy_Cuda) ProtoMessage()    {}
func (*ModelOptimizationPolicy_Cuda) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 1}
}

func (m *ModelOptimizationPolicy_Cuda) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_Cuda.Merge(m, src)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_Cuda.Size(m)
}
func (m *ModelOptimizationPolicy_Cuda) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_Cuda.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_Cuda proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_Cuda) GetGraphs() bool {
	if m != nil {
		return m.Graphs
	}
	return false
}

//@@
//@@  .. cpp:var:: message ExecutionAccelerators
//@@
//@@     Specify the preferred execution accelerators to be used to execute
//@@     the model. Currently only recognized by ONNX Runtime backend and
//@@     TensorFlow backend.
//@@
//@@     For ONNX Runtime backend, it will deploy the model with the execution
//@@     accelerators by priority, the priority is determined based on the
//@@     order that they are set, i.e. the provider at the front has highest
//@@     priority. Overall, the priority will be in the following order:
//@@         <gpu_execution_accelerator> (if instance is on GPU)
//@@         CUDA Execution Provider     (if instance is on GPU)
//@@         <cpu_execution_accelerator>
//@@         Default CPU Execution Provider
//@@
type ModelOptimizationPolicy_ExecutionAccelerators struct {
	//@@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
	//@@
	//@@       The preferred execution provider to be used if the model instance
	//@@       is deployed on GPU.
	//@@
	//@@       For ONNX Runtime backend, possible value is "tensorrt" as name,
	//@@       and no parameters are required.
	//@@
	//@@       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
	//@@
	//@@       For "tensorrt", the following parameters can be specified:
	//@@         "precision_mode": The precision used for optimization.
	//@@         Allowed values are "FP32" and "FP16". Default value is "FP32".
	//@@
	//@@         "max_cached_engines": The maximum number of cached TensorRT
	//@@         engines in dynamic TensorRT ops. Default value is 100.
	//@@
	//@@         "minimum_segment_size": The smallest model subgraph that will
	//@@         be considered for optimization by TensorRT. Default value is 3.
	//@@
	//@@         "max_workspace_size_bytes": The maximum GPU memory the model
	//@@         can use temporarily during execution. Default value is 1GB.
	//@@
	//@@       For "gpu_io", no parameters are required. If set, the model will
	//@@       be executed using TensorFlow Callable API to set input and output
	//@@       tensors in GPU memory if possible, which can reduce data transfer
	//@@       overhead if the model is used in ensemble. However, the Callable
	//@@       object will be created on model creation and it will request all
	//@@       outputs for every model execution, which may impact the
	//@@       performance if a request does not require all outputs. This
	//@@       optimization will only take affect if the model instance is
	//@@       created with KIND_GPU.
	//@@
	GpuExecutionAccelerator []*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator `protobuf:"bytes,1,rep,name=gpu_execution_accelerator,json=gpuExecutionAccelerator,proto3" json:"gpu_execution_accelerator,omitempty"`
	//@@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
	//@@
	//@@       The preferred execution provider to be used if the model instance
	//@@       is deployed on CPU.
	//@@
	//@@       For ONNX Runtime backend, possible value is "openvino" as name,
	//@@       and no parameters are required.
	//@@
	CpuExecutionAccelerator []*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator `protobuf:"bytes,2,rep,name=cpu_execution_accelerator,json=cpuExecutionAccelerator,proto3" json:"cpu_execution_accelerator,omitempty"`
	XXX_NoUnkeyedLiteral    struct{}                                                     `json:"-"`
	XXX_unrecognized        []byte                                                       `json:"-"`
	XXX_sizecache           int32                                                        `json:"-"`
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators) Reset() {
	*m = ModelOptimizationPolicy_ExecutionAccelerators{}
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators) String() string {
	return proto.CompactTextString(m)
}
func (*ModelOptimizationPolicy_ExecutionAccelerators) ProtoMessage() {}
func (*ModelOptimizationPolicy_ExecutionAccelerators) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 2}
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators.Merge(m, src)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators.Size(m)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_ExecutionAccelerators) GetGpuExecutionAccelerator() []*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator {
	if m != nil {
		return m.GpuExecutionAccelerator
	}
	return nil
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators) GetCpuExecutionAccelerator() []*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator {
	if m != nil {
		return m.CpuExecutionAccelerator
	}
	return nil
}

//@@
//@@  .. cpp:var:: message Accelerator
//@@
//@@     Specify the accelerator to be used to execute the model.
//@@     Accelerator with the same name may accept different parameters
//@@     depending on the backends.
//@@
type ModelOptimizationPolicy_ExecutionAccelerators_Accelerator struct {
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name of the execution accelerator.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: map<string, string> parameters
	//@@
	//@@       Additional paremeters used to configure the accelerator.
	//@@
	Parameters           map[string]string `protobuf:"bytes,2,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
	XXX_unrecognized     []byte            `json:"-"`
	XXX_sizecache        int32             `json:"-"`
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) Reset() {
	*m = ModelOptimizationPolicy_ExecutionAccelerators_Accelerator{}
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) String() string {
	return proto.CompactTextString(m)
}
func (*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) ProtoMessage() {}
func (*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 2, 0}
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator.Merge(m, src)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator.Size(m)
}
func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelOptimizationPolicy_ExecutionAccelerators_Accelerator) GetParameters() map[string]string {
	if m != nil {
		return m.Parameters
	}
	return nil
}

//@@
//@@  .. cpp:var:: message PinnedMemoryBuffer
//@@
//@@     Specify whether to use a pinned memory buffer when transferring data
//@@     between non-pinned system memory and GPU memory. Using a pinned
//@@     memory buffer for system from/to GPU transfers will typically provide
//@@     increased performance. For example, in the common use case where the
//@@     request provides inputs and delivers outputs via non-pinned system
//@@     memory, if the model instance accepts GPU IOs, the inputs will be
//@@     processed by two copies: from non-pinned system memory to pinned
//@@     memory, and from pinned memory to GPU memory. Similarly, pinned
//@@     memory will be used for delivering the outputs.
//@@
type ModelOptimizationPolicy_PinnedMemoryBuffer struct {
	//@@    .. cpp:var:: bool enable
	//@@
	//@@       Use pinned memory buffer. Default is true.
	//@@
	Enable               bool     `protobuf:"varint,1,opt,name=enable,proto3" json:"enable,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) Reset() {
	*m = ModelOptimizationPolicy_PinnedMemoryBuffer{}
}
func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) String() string {
	return proto.CompactTextString(m)
}
func (*ModelOptimizationPolicy_PinnedMemoryBuffer) ProtoMessage() {}
func (*ModelOptimizationPolicy_PinnedMemoryBuffer) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{5, 3}
}

func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer.Unmarshal(m, b)
}
func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer.Marshal(b, m, deterministic)
}
func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer.Merge(m, src)
}
func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) XXX_Size() int {
	return xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer.Size(m)
}
func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer.DiscardUnknown(m)
}

var xxx_messageInfo_ModelOptimizationPolicy_PinnedMemoryBuffer proto.InternalMessageInfo

func (m *ModelOptimizationPolicy_PinnedMemoryBuffer) GetEnable() bool {
	if m != nil {
		return m.Enable
	}
	return false
}

//@@
//@@.. cpp:var:: message ModelQueuePolicy
//@@
//@@   Queue policy for inference requests.
//@@
type ModelQueuePolicy struct {
	//@@
	//@@  .. cpp:var:: TimeoutAction timeout_action
	//@@
	//@@     The action applied to timed-out request.
	//@@     The default action is REJECT.
	//@@
	TimeoutAction ModelQueuePolicy_TimeoutAction `protobuf:"varint,1,opt,name=timeout_action,json=timeoutAction,proto3,enum=nvidia.inferenceserver.ModelQueuePolicy_TimeoutAction" json:"timeout_action,omitempty"`
	//@@
	//@@  .. cpp:var:: uint64 default_timeout_microseconds
	//@@
	//@@     The default timeout for every request, in microseconds.
	//@@     The default value is 0 which indicates that no timeout is set.
	//@@
	DefaultTimeoutMicroseconds uint64 `protobuf:"varint,2,opt,name=default_timeout_microseconds,json=defaultTimeoutMicroseconds,proto3" json:"default_timeout_microseconds,omitempty"`
	//@@
	//@@  .. cpp:var:: bool allow_timeout_override
	//@@
	//@@     Whether individual request can override the default timeout value.
	//@@     When true, individual requests can set a timeout that is less than
	//@@     the default timeout value but may not increase the timeout.
	//@@     The default value is false.
	//@@
	AllowTimeoutOverride bool `protobuf:"varint,3,opt,name=allow_timeout_override,json=allowTimeoutOverride,proto3" json:"allow_timeout_override,omitempty"`
	//@@
	//@@  .. cpp:var:: uint32 max_queue_size
	//@@
	//@@     The maximum queue size for holding requests. A request will be
	//@@     rejected immediately if it can't be enqueued because the queue is
	//@@     full. The default value is 0 which indicates that no maximum
	//@@     queue size is enforced.
	//@@
	MaxQueueSize         uint32   `protobuf:"varint,4,opt,name=max_queue_size,json=maxQueueSize,proto3" json:"max_queue_size,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelQueuePolicy) Reset()         { *m = ModelQueuePolicy{} }
func (m *ModelQueuePolicy) String() string { return proto.CompactTextString(m) }
func (*ModelQueuePolicy) ProtoMessage()    {}
func (*ModelQueuePolicy) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{6}
}

func (m *ModelQueuePolicy) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelQueuePolicy.Unmarshal(m, b)
}
func (m *ModelQueuePolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelQueuePolicy.Marshal(b, m, deterministic)
}
func (m *ModelQueuePolicy) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelQueuePolicy.Merge(m, src)
}
func (m *ModelQueuePolicy) XXX_Size() int {
	return xxx_messageInfo_ModelQueuePolicy.Size(m)
}
func (m *ModelQueuePolicy) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelQueuePolicy.DiscardUnknown(m)
}

var xxx_messageInfo_ModelQueuePolicy proto.InternalMessageInfo

func (m *ModelQueuePolicy) GetTimeoutAction() ModelQueuePolicy_TimeoutAction {
	if m != nil {
		return m.TimeoutAction
	}
	return ModelQueuePolicy_REJECT
}

func (m *ModelQueuePolicy) GetDefaultTimeoutMicroseconds() uint64 {
	if m != nil {
		return m.DefaultTimeoutMicroseconds
	}
	return 0
}

func (m *ModelQueuePolicy) GetAllowTimeoutOverride() bool {
	if m != nil {
		return m.AllowTimeoutOverride
	}
	return false
}

func (m *ModelQueuePolicy) GetMaxQueueSize() uint32 {
	if m != nil {
		return m.MaxQueueSize
	}
	return 0
}

//@@
//@@.. cpp:var:: message ModelDynamicBatching
//@@
//@@   Dynamic batching configuration. These settings control how dynamic
//@@   batching operates for the model.
//@@
type ModelDynamicBatching struct {
	//@@  .. cpp:var:: int32 preferred_batch_size (repeated)
	//@@
	//@@     Preferred batch sizes for dynamic batching. If a batch of one of
	//@@     these sizes can be formed it will be executed immediately.  If
	//@@     not specified a preferred batch size will be chosen automatically
	//@@     based on model and GPU characteristics.
	//@@
	PreferredBatchSize []int32 `protobuf:"varint,1,rep,packed,name=preferred_batch_size,json=preferredBatchSize,proto3" json:"preferred_batch_size,omitempty"`
	//@@  .. cpp:var:: uint64 max_queue_delay_microseconds
	//@@
	//@@     The maximum time, in microseconds, a request will be delayed in
	//@@     the scheduling queue to wait for additional requests for
	//@@     batching. Default is 0.
	//@@
	MaxQueueDelayMicroseconds uint64 `protobuf:"varint,2,opt,name=max_queue_delay_microseconds,json=maxQueueDelayMicroseconds,proto3" json:"max_queue_delay_microseconds,omitempty"`
	//@@  .. cpp:var:: bool preserve_ordering
	//@@
	//@@     Should the dynamic batcher preserve the ordering of responses to
	//@@     match the order of requests received by the scheduler. Default is
	//@@     false. If true, the responses will be returned in the same order as
	//@@     the order of requests sent to the scheduler. If false, the responses
	//@@     may be returned in arbitrary order. This option is specifically
	//@@     needed when a sequence of related inference requests (i.e. inference
	//@@     requests with the same correlation ID) are sent to the dynamic
	//@@     batcher to ensure that the sequence responses are in the correct
	//@@     order.
	//@@
	PreserveOrdering bool `protobuf:"varint,3,opt,name=preserve_ordering,json=preserveOrdering,proto3" json:"preserve_ordering,omitempty"`
	//@@  .. cpp:var:: uint32 priority_levels
	//@@
	//@@     The number of priority levels to be enabled for the model,
	//@@     the priority level starts from 1 and 1 is the highest priority.
	//@@     Requests are handled in priority order with all priority 1 requests
	//@@     processed before priority 2, all priority 2 requests processed before
	//@@     priority 3, etc. Requests with the same priority level will be
	//@@     handled in the order that they are received.
	//@@
	PriorityLevels uint32 `protobuf:"varint,4,opt,name=priority_levels,json=priorityLevels,proto3" json:"priority_levels,omitempty"`
	//@@  .. cpp:var:: uint32 default_priority_level
	//@@
	//@@     The priority level used for requests that don't specify their
	//@@     priority. The value must be in the range [ 1, 'priority_levels' ].
	//@@
	DefaultPriorityLevel uint32 `protobuf:"varint,5,opt,name=default_priority_level,json=defaultPriorityLevel,proto3" json:"default_priority_level,omitempty"`
	//@@  .. cpp:var:: ModelQueuePolicy default_queue_policy
	//@@
	//@@     The default queue policy used for requests that don't require
	//@@     priority handling and requests that specify priority levels where
	//@@     there is no specific policy given. If not specified, a policy with
	//@@     default field values will be used.
	//@@
	DefaultQueuePolicy *ModelQueuePolicy `protobuf:"bytes,6,opt,name=default_queue_policy,json=defaultQueuePolicy,proto3" json:"default_queue_policy,omitempty"`
	//@@  .. cpp:var:: map<uint32, ModelQueuePolicy> priority_queue_policy
	//@@
	//@@     Specify the queue policy for the priority level. The default queue
	//@@     policy will be used if a priority level doesn't specify a queue
	//@@     policy.
	//@@
	PriorityQueuePolicy  map[uint32]*ModelQueuePolicy `protobuf:"bytes,7,rep,name=priority_queue_policy,json=priorityQueuePolicy,proto3" json:"priority_queue_policy,omitempty" protobuf_key:"varint,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                     `json:"-"`
	XXX_unrecognized     []byte                       `json:"-"`
	XXX_sizecache        int32                        `json:"-"`
}

func (m *ModelDynamicBatching) Reset()         { *m = ModelDynamicBatching{} }
func (m *ModelDynamicBatching) String() string { return proto.CompactTextString(m) }
func (*ModelDynamicBatching) ProtoMessage()    {}
func (*ModelDynamicBatching) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{7}
}

func (m *ModelDynamicBatching) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelDynamicBatching.Unmarshal(m, b)
}
func (m *ModelDynamicBatching) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelDynamicBatching.Marshal(b, m, deterministic)
}
func (m *ModelDynamicBatching) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelDynamicBatching.Merge(m, src)
}
func (m *ModelDynamicBatching) XXX_Size() int {
	return xxx_messageInfo_ModelDynamicBatching.Size(m)
}
func (m *ModelDynamicBatching) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelDynamicBatching.DiscardUnknown(m)
}

var xxx_messageInfo_ModelDynamicBatching proto.InternalMessageInfo

func (m *ModelDynamicBatching) GetPreferredBatchSize() []int32 {
	if m != nil {
		return m.PreferredBatchSize
	}
	return nil
}

func (m *ModelDynamicBatching) GetMaxQueueDelayMicroseconds() uint64 {
	if m != nil {
		return m.MaxQueueDelayMicroseconds
	}
	return 0
}

func (m *ModelDynamicBatching) GetPreserveOrdering() bool {
	if m != nil {
		return m.PreserveOrdering
	}
	return false
}

func (m *ModelDynamicBatching) GetPriorityLevels() uint32 {
	if m != nil {
		return m.PriorityLevels
	}
	return 0
}

func (m *ModelDynamicBatching) GetDefaultPriorityLevel() uint32 {
	if m != nil {
		return m.DefaultPriorityLevel
	}
	return 0
}

func (m *ModelDynamicBatching) GetDefaultQueuePolicy() *ModelQueuePolicy {
	if m != nil {
		return m.DefaultQueuePolicy
	}
	return nil
}

func (m *ModelDynamicBatching) GetPriorityQueuePolicy() map[uint32]*ModelQueuePolicy {
	if m != nil {
		return m.PriorityQueuePolicy
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelSequenceBatching
//@@
//@@   Sequence batching configuration. These settings control how sequence
//@@   batching operates for the model.
//@@
type ModelSequenceBatching struct {
	//@@  .. cpp:var:: oneof strategy_choice
	//@@
	//@@     The strategy used by the sequence batcher. Default strategy
	//@@     is 'direct'.
	//@@
	//
	// Types that are valid to be assigned to StrategyChoice:
	//	*ModelSequenceBatching_Direct
	//	*ModelSequenceBatching_Oldest
	StrategyChoice isModelSequenceBatching_StrategyChoice `protobuf_oneof:"strategy_choice"`
	//@@  .. cpp:var:: uint64 max_sequence_idle_microseconds
	//@@
	//@@     The maximum time, in microseconds, that a sequence is allowed to
	//@@     be idle before it is aborted. The inference server considers a
	//@@     sequence idle when it does not have any inference request queued
	//@@     for the sequence. If this limit is exceeded, the inference server
	//@@     will free the sequence slot allocated by the sequence and make it
	//@@     available for another sequence. If not specified (or specified as
	//@@     zero) a default value of 1000000 (1 second) is used.
	//@@
	MaxSequenceIdleMicroseconds uint64 `protobuf:"varint,1,opt,name=max_sequence_idle_microseconds,json=maxSequenceIdleMicroseconds,proto3" json:"max_sequence_idle_microseconds,omitempty"`
	//@@  .. cpp:var:: ControlInput control_input (repeated)
	//@@
	//@@     The model input(s) that the server should use to communicate
	//@@     sequence start, stop, ready and similar control values to the
	//@@     model.
	//@@
	ControlInput         []*ModelSequenceBatching_ControlInput `protobuf:"bytes,2,rep,name=control_input,json=controlInput,proto3" json:"control_input,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                              `json:"-"`
	XXX_unrecognized     []byte                                `json:"-"`
	XXX_sizecache        int32                                 `json:"-"`
}

func (m *ModelSequenceBatching) Reset()         { *m = ModelSequenceBatching{} }
func (m *ModelSequenceBatching) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching) ProtoMessage()    {}
func (*ModelSequenceBatching) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8}
}

func (m *ModelSequenceBatching) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching.Unmarshal(m, b)
}
func (m *ModelSequenceBatching) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching.Merge(m, src)
}
func (m *ModelSequenceBatching) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching.Size(m)
}
func (m *ModelSequenceBatching) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching proto.InternalMessageInfo

type isModelSequenceBatching_StrategyChoice interface {
	isModelSequenceBatching_StrategyChoice()
}

type ModelSequenceBatching_Direct struct {
	Direct *ModelSequenceBatching_StrategyDirect `protobuf:"bytes,3,opt,name=direct,proto3,oneof"`
}

type ModelSequenceBatching_Oldest struct {
	Oldest *ModelSequenceBatching_StrategyOldest `protobuf:"bytes,4,opt,name=oldest,proto3,oneof"`
}

func (*ModelSequenceBatching_Direct) isModelSequenceBatching_StrategyChoice() {}

func (*ModelSequenceBatching_Oldest) isModelSequenceBatching_StrategyChoice() {}

func (m *ModelSequenceBatching) GetStrategyChoice() isModelSequenceBatching_StrategyChoice {
	if m != nil {
		return m.StrategyChoice
	}
	return nil
}

func (m *ModelSequenceBatching) GetDirect() *ModelSequenceBatching_StrategyDirect {
	if x, ok := m.GetStrategyChoice().(*ModelSequenceBatching_Direct); ok {
		return x.Direct
	}
	return nil
}

func (m *ModelSequenceBatching) GetOldest() *ModelSequenceBatching_StrategyOldest {
	if x, ok := m.GetStrategyChoice().(*ModelSequenceBatching_Oldest); ok {
		return x.Oldest
	}
	return nil
}

func (m *ModelSequenceBatching) GetMaxSequenceIdleMicroseconds() uint64 {
	if m != nil {
		return m.MaxSequenceIdleMicroseconds
	}
	return 0
}

func (m *ModelSequenceBatching) GetControlInput() []*ModelSequenceBatching_ControlInput {
	if m != nil {
		return m.ControlInput
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelSequenceBatching) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelSequenceBatching_Direct)(nil),
		(*ModelSequenceBatching_Oldest)(nil),
	}
}

//@@  .. cpp:var:: message Control
//@@
//@@     A control is a signal that the sequence batcher uses to
//@@     communicate with a backend.
//@@
type ModelSequenceBatching_Control struct {
	//@@    .. cpp:var:: Kind kind
	//@@
	//@@       The kind of this control.
	//@@
	Kind ModelSequenceBatching_Control_Kind `protobuf:"varint,1,opt,name=kind,proto3,enum=nvidia.inferenceserver.ModelSequenceBatching_Control_Kind" json:"kind,omitempty"`
	//@@    .. cpp:var:: int32 int32_false_true (repeated)
	//@@
	//@@       The control's true and false setting is indicated by setting
	//@@       a value in an int32 tensor. The tensor must be a
	//@@       1-dimensional tensor with size equal to the batch size of
	//@@       the request. 'int32_false_true' must have two entries: the
	//@@       first the false value and the second the true value.
	//@@
	Int32FalseTrue []int32 `protobuf:"varint,2,rep,packed,name=int32_false_true,json=int32FalseTrue,proto3" json:"int32_false_true,omitempty"`
	//@@    .. cpp:var:: float fp32_false_true (repeated)
	//@@
	//@@       The control's true and false setting is indicated by setting
	//@@       a value in a fp32 tensor. The tensor must be a
	//@@       1-dimensional tensor with size equal to the batch size of
	//@@       the request. 'fp32_false_true' must have two entries: the
	//@@       first the false value and the second the true value.
	//@@
	Fp32FalseTrue []float32 `protobuf:"fixed32,3,rep,packed,name=fp32_false_true,json=fp32FalseTrue,proto3" json:"fp32_false_true,omitempty"`
	//@@    .. cpp:var:: DataType data_type
	//@@
	//@@       The control's datatype.
	//@@
	DataType             DataType `protobuf:"varint,4,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelSequenceBatching_Control) Reset()         { *m = ModelSequenceBatching_Control{} }
func (m *ModelSequenceBatching_Control) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_Control) ProtoMessage()    {}
func (*ModelSequenceBatching_Control) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 0}
}

func (m *ModelSequenceBatching_Control) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_Control.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_Control) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_Control.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_Control) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_Control.Merge(m, src)
}
func (m *ModelSequenceBatching_Control) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_Control.Size(m)
}
func (m *ModelSequenceBatching_Control) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_Control.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_Control proto.InternalMessageInfo

func (m *ModelSequenceBatching_Control) GetKind() ModelSequenceBatching_Control_Kind {
	if m != nil {
		return m.Kind
	}
	return ModelSequenceBatching_Control_CONTROL_SEQUENCE_START
}

func (m *ModelSequenceBatching_Control) GetInt32FalseTrue() []int32 {
	if m != nil {
		return m.Int32FalseTrue
	}
	return nil
}

func (m *ModelSequenceBatching_Control) GetFp32FalseTrue() []float32 {
	if m != nil {
		return m.Fp32FalseTrue
	}
	return nil
}

func (m *ModelSequenceBatching_Control) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

//@@  .. cpp:var:: message ControlInput
//@@
//@@     The sequence control values to communicate by a model input.
//@@
type ModelSequenceBatching_ControlInput struct {
	//@@    .. cpp:var:: string name
	//@@
	//@@       The name of the model input.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@    .. cpp:var:: Control control (repeated)
	//@@
	//@@       The control value(s) that should be communicated to the
	//@@       model using this model input.
	//@@
	Control              []*ModelSequenceBatching_Control `protobuf:"bytes,2,rep,name=control,proto3" json:"control,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                         `json:"-"`
	XXX_unrecognized     []byte                           `json:"-"`
	XXX_sizecache        int32                            `json:"-"`
}

func (m *ModelSequenceBatching_ControlInput) Reset()         { *m = ModelSequenceBatching_ControlInput{} }
func (m *ModelSequenceBatching_ControlInput) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_ControlInput) ProtoMessage()    {}
func (*ModelSequenceBatching_ControlInput) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 1}
}

func (m *ModelSequenceBatching_ControlInput) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_ControlInput.Merge(m, src)
}
func (m *ModelSequenceBatching_ControlInput) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_ControlInput.Size(m)
}
func (m *ModelSequenceBatching_ControlInput) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_ControlInput.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_ControlInput proto.InternalMessageInfo

func (m *ModelSequenceBatching_ControlInput) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelSequenceBatching_ControlInput) GetControl() []*ModelSequenceBatching_Control {
	if m != nil {
		return m.Control
	}
	return nil
}

//@@  .. cpp:var:: message StrategyDirect
//@@
//@@     The sequence batcher uses a specific, unique batch
//@@     slot for each sequence. All inference requests in a
//@@     sequence are directed to the same batch slot in the same
//@@     model instance over the lifetime of the sequence. This
//@@     is the default strategy.
//@@
type ModelSequenceBatching_StrategyDirect struct {
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelSequenceBatching_StrategyDirect) Reset()         { *m = ModelSequenceBatching_StrategyDirect{} }
func (m *ModelSequenceBatching_StrategyDirect) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_StrategyDirect) ProtoMessage()    {}
func (*ModelSequenceBatching_StrategyDirect) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 2}
}

func (m *ModelSequenceBatching_StrategyDirect) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_StrategyDirect.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_StrategyDirect) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_StrategyDirect.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_StrategyDirect) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_StrategyDirect.Merge(m, src)
}
func (m *ModelSequenceBatching_StrategyDirect) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_StrategyDirect.Size(m)
}
func (m *ModelSequenceBatching_StrategyDirect) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_StrategyDirect.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_StrategyDirect proto.InternalMessageInfo

//@@  .. cpp:var:: message StrategyOldest
//@@
//@@     The sequence batcher maintains up to 'max_candidate_sequences'
//@@     candidate sequences. 'max_candidate_sequences' can be greater
//@@     than the model's 'max_batch_size'. For inferencing the batcher
//@@     chooses from the candidate sequences up to 'max_batch_size'
//@@     inference requests. Requests are chosen in an oldest-first
//@@     manner across all candidate sequences. A given sequence is
//@@     not guaranteed to be assigned to the same batch slot for
//@@     all inference requests of that sequence.
//@@
type ModelSequenceBatching_StrategyOldest struct {
	//@@    .. cpp:var:: int32 max_candidate_sequences
	//@@
	//@@       Maximum number of candidate sequences that the batcher
	//@@       maintains. Excess seqences are kept in an ordered backlog
	//@@       and become candidates when existing candidate sequences
	//@@       complete.
	//@@
	MaxCandidateSequences int32 `protobuf:"varint,1,opt,name=max_candidate_sequences,json=maxCandidateSequences,proto3" json:"max_candidate_sequences,omitempty"`
	//@@    .. cpp:var:: int32 preferred_batch_size (repeated)
	//@@
	//@@       Preferred batch sizes for dynamic batching of candidate
	//@@       sequences. If a batch of one of these sizes can be formed
	//@@       it will be executed immediately.  If not specified a
	//@@       preferred batch size will be chosen automatically
	//@@       based on model and GPU characteristics.
	//@@
	PreferredBatchSize []int32 `protobuf:"varint,2,rep,packed,name=preferred_batch_size,json=preferredBatchSize,proto3" json:"preferred_batch_size,omitempty"`
	//@@    .. cpp:var:: uint64 max_queue_delay_microseconds
	//@@
	//@@       The maximum time, in microseconds, a candidate request
	//@@       will be delayed in the dynamic batch scheduling queue to
	//@@       wait for additional requests for batching. Default is 0.
	//@@
	MaxQueueDelayMicroseconds uint64   `protobuf:"varint,3,opt,name=max_queue_delay_microseconds,json=maxQueueDelayMicroseconds,proto3" json:"max_queue_delay_microseconds,omitempty"`
	XXX_NoUnkeyedLiteral      struct{} `json:"-"`
	XXX_unrecognized          []byte   `json:"-"`
	XXX_sizecache             int32    `json:"-"`
}

func (m *ModelSequenceBatching_StrategyOldest) Reset()         { *m = ModelSequenceBatching_StrategyOldest{} }
func (m *ModelSequenceBatching_StrategyOldest) String() string { return proto.CompactTextString(m) }
func (*ModelSequenceBatching_StrategyOldest) ProtoMessage()    {}
func (*ModelSequenceBatching_StrategyOldest) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{8, 3}
}

func (m *ModelSequenceBatching_StrategyOldest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelSequenceBatching_StrategyOldest.Unmarshal(m, b)
}
func (m *ModelSequenceBatching_StrategyOldest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelSequenceBatching_StrategyOldest.Marshal(b, m, deterministic)
}
func (m *ModelSequenceBatching_StrategyOldest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelSequenceBatching_StrategyOldest.Merge(m, src)
}
func (m *ModelSequenceBatching_StrategyOldest) XXX_Size() int {
	return xxx_messageInfo_ModelSequenceBatching_StrategyOldest.Size(m)
}
func (m *ModelSequenceBatching_StrategyOldest) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelSequenceBatching_StrategyOldest.DiscardUnknown(m)
}

var xxx_messageInfo_ModelSequenceBatching_StrategyOldest proto.InternalMessageInfo

func (m *ModelSequenceBatching_StrategyOldest) GetMaxCandidateSequences() int32 {
	if m != nil {
		return m.MaxCandidateSequences
	}
	return 0
}

func (m *ModelSequenceBatching_StrategyOldest) GetPreferredBatchSize() []int32 {
	if m != nil {
		return m.PreferredBatchSize
	}
	return nil
}

func (m *ModelSequenceBatching_StrategyOldest) GetMaxQueueDelayMicroseconds() uint64 {
	if m != nil {
		return m.MaxQueueDelayMicroseconds
	}
	return 0
}

//@@
//@@.. cpp:var:: message ModelEnsembling
//@@
//@@   Model ensembling configuration. These settings specify the models that
//@@   compose the ensemble and how data flows between the models.
//@@
type ModelEnsembling struct {
	//@@  .. cpp:var:: Step step (repeated)
	//@@
	//@@     The models and the input / output mappings used within the ensemble.
	//@@
	Step                 []*ModelEnsembling_Step `protobuf:"bytes,1,rep,name=step,proto3" json:"step,omitempty"`
	XXX_NoUnkeyedLiteral struct{}                `json:"-"`
	XXX_unrecognized     []byte                  `json:"-"`
	XXX_sizecache        int32                   `json:"-"`
}

func (m *ModelEnsembling) Reset()         { *m = ModelEnsembling{} }
func (m *ModelEnsembling) String() string { return proto.CompactTextString(m) }
func (*ModelEnsembling) ProtoMessage()    {}
func (*ModelEnsembling) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{9}
}

func (m *ModelEnsembling) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelEnsembling.Unmarshal(m, b)
}
func (m *ModelEnsembling) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelEnsembling.Marshal(b, m, deterministic)
}
func (m *ModelEnsembling) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelEnsembling.Merge(m, src)
}
func (m *ModelEnsembling) XXX_Size() int {
	return xxx_messageInfo_ModelEnsembling.Size(m)
}
func (m *ModelEnsembling) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelEnsembling.DiscardUnknown(m)
}

var xxx_messageInfo_ModelEnsembling proto.InternalMessageInfo

func (m *ModelEnsembling) GetStep() []*ModelEnsembling_Step {
	if m != nil {
		return m.Step
	}
	return nil
}

//@@  .. cpp:var:: message Step
//@@
//@@     Each step specifies a model included in the ensemble,
//@@     maps ensemble tensor names to the model input tensors,
//@@     and maps model output tensors to ensemble tensor names
//@@
type ModelEnsembling_Step struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model to execute for this step of the ensemble.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: int64 model_version
	//@@
	//@@     The version of the model to use for inference. If -1
	//@@     the latest/most-recent version of the model is used.
	//@@
	ModelVersion int64 `protobuf:"varint,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: map<string,string> input_map
	//@@
	//@@     Map from name of an input tensor on this step's model to ensemble
	//@@     tensor name. The ensemble tensor must have the same data type and
	//@@     shape as the model input. Each model input must be assigned to
	//@@     one ensemble tensor, but the same ensemble tensor can be assigned
	//@@     to multiple model inputs.
	//@@
	InputMap map[string]string `protobuf:"bytes,3,rep,name=input_map,json=inputMap,proto3" json:"input_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,string> output_map
	//@@
	//@@     Map from name of an output tensor on this step's model to ensemble
	//@@     tensor name. The data type and shape of the ensemble tensor will
	//@@     be inferred from the model output. It is optional to assign all
	//@@     model outputs to ensemble tensors. One ensemble tensor name
	//@@     can appear in an output map only once.
	//@@
	OutputMap            map[string]string `protobuf:"bytes,4,rep,name=output_map,json=outputMap,proto3" json:"output_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
	XXX_unrecognized     []byte            `json:"-"`
	XXX_sizecache        int32             `json:"-"`
}

func (m *ModelEnsembling_Step) Reset()         { *m = ModelEnsembling_Step{} }
func (m *ModelEnsembling_Step) String() string { return proto.CompactTextString(m) }
func (*ModelEnsembling_Step) ProtoMessage()    {}
func (*ModelEnsembling_Step) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{9, 0}
}

func (m *ModelEnsembling_Step) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelEnsembling_Step.Unmarshal(m, b)
}
func (m *ModelEnsembling_Step) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelEnsembling_Step.Marshal(b, m, deterministic)
}
func (m *ModelEnsembling_Step) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelEnsembling_Step.Merge(m, src)
}
func (m *ModelEnsembling_Step) XXX_Size() int {
	return xxx_messageInfo_ModelEnsembling_Step.Size(m)
}
func (m *ModelEnsembling_Step) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelEnsembling_Step.DiscardUnknown(m)
}

var xxx_messageInfo_ModelEnsembling_Step proto.InternalMessageInfo

func (m *ModelEnsembling_Step) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *ModelEnsembling_Step) GetModelVersion() int64 {
	if m != nil {
		return m.ModelVersion
	}
	return 0
}

func (m *ModelEnsembling_Step) GetInputMap() map[string]string {
	if m != nil {
		return m.InputMap
	}
	return nil
}

func (m *ModelEnsembling_Step) GetOutputMap() map[string]string {
	if m != nil {
		return m.OutputMap
	}
	return nil
}

//@@
//@@.. cpp:var:: message ModelParameter
//@@
//@@   A model parameter.
//@@
type ModelParameter struct {
	//@@  .. cpp:var:: string string_value
	//@@
	//@@     The string value of the parameter.
	//@@
	StringValue          string   `protobuf:"bytes,1,opt,name=string_value,json=stringValue,proto3" json:"string_value,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ModelParameter) Reset()         { *m = ModelParameter{} }
func (m *ModelParameter) String() string { return proto.CompactTextString(m) }
func (*ModelParameter) ProtoMessage()    {}
func (*ModelParameter) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{10}
}

func (m *ModelParameter) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelParameter.Unmarshal(m, b)
}
func (m *ModelParameter) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelParameter.Marshal(b, m, deterministic)
}
func (m *ModelParameter) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelParameter.Merge(m, src)
}
func (m *ModelParameter) XXX_Size() int {
	return xxx_messageInfo_ModelParameter.Size(m)
}
func (m *ModelParameter) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelParameter.DiscardUnknown(m)
}

var xxx_messageInfo_ModelParameter proto.InternalMessageInfo

func (m *ModelParameter) GetStringValue() string {
	if m != nil {
		return m.StringValue
	}
	return ""
}

//@@
//@@.. cpp:var:: message ModelWarmup
//@@
//@@   Settings used to construct the request sample for model warmup.
//@@
type ModelWarmup struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the request sample.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: uint32 batch_size
	//@@
	//@@     The batch size of the inference request. This must be >= 1. For
	//@@     models that don't support batching, batch_size must be 1. If
	//@@     batch_size > 1, the 'inputs' specified below will be duplicated to
	//@@     match the batch size requested.
	//@@
	BatchSize uint32 `protobuf:"varint,2,opt,name=batch_size,json=batchSize,proto3" json:"batch_size,omitempty"`
	//@@  .. cpp:var:: map<string, Input> inputs
	//@@
	//@@     The warmup meta data associated with every model input, including
	//@@     control tensors.
	//@@
	Inputs               map[string]*ModelWarmup_Input `protobuf:"bytes,3,rep,name=inputs,proto3" json:"inputs,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	XXX_NoUnkeyedLiteral struct{}                      `json:"-"`
	XXX_unrecognized     []byte                        `json:"-"`
	XXX_sizecache        int32                         `json:"-"`
}

func (m *ModelWarmup) Reset()         { *m = ModelWarmup{} }
func (m *ModelWarmup) String() string { return proto.CompactTextString(m) }
func (*ModelWarmup) ProtoMessage()    {}
func (*ModelWarmup) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{11}
}

func (m *ModelWarmup) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelWarmup.Unmarshal(m, b)
}
func (m *ModelWarmup) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelWarmup.Marshal(b, m, deterministic)
}
func (m *ModelWarmup) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelWarmup.Merge(m, src)
}
func (m *ModelWarmup) XXX_Size() int {
	return xxx_messageInfo_ModelWarmup.Size(m)
}
func (m *ModelWarmup) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelWarmup.DiscardUnknown(m)
}

var xxx_messageInfo_ModelWarmup proto.InternalMessageInfo

func (m *ModelWarmup) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelWarmup) GetBatchSize() uint32 {
	if m != nil {
		return m.BatchSize
	}
	return 0
}

func (m *ModelWarmup) GetInputs() map[string]*ModelWarmup_Input {
	if m != nil {
		return m.Inputs
	}
	return nil
}

//@@
//@@  .. cpp:var:: message Input
//@@
//@@     Meta data associated with an input.
//@@
type ModelWarmup_Input struct {
	//@@    .. cpp:var:: DataType data_type
	//@@
	//@@       The data-type of the input.
	//@@
	DataType DataType `protobuf:"varint,1,opt,name=data_type,json=dataType,proto3,enum=nvidia.inferenceserver.DataType" json:"data_type,omitempty"`
	//@@    .. cpp:var:: int64 dims (repeated)
	//@@
	//@@       The shape of the input tensor, not including the batch dimension.
	//@@
	Dims []int64 `protobuf:"varint,2,rep,packed,name=dims,proto3" json:"dims,omitempty"`
	//@@    .. cpp:var:: oneof input_data_type
	//@@
	//@@       Specify how the input data is generated. If the input has STRING
	//@@       data type and 'random_data' is set, the data generation will fall
	//@@       back to 'zero_data'.
	//@@
	//
	// Types that are valid to be assigned to InputDataType:
	//	*ModelWarmup_Input_ZeroData
	//	*ModelWarmup_Input_RandomData
	//	*ModelWarmup_Input_InputDataFile
	InputDataType        isModelWarmup_Input_InputDataType `protobuf_oneof:"input_data_type"`
	XXX_NoUnkeyedLiteral struct{}                          `json:"-"`
	XXX_unrecognized     []byte                            `json:"-"`
	XXX_sizecache        int32                             `json:"-"`
}

func (m *ModelWarmup_Input) Reset()         { *m = ModelWarmup_Input{} }
func (m *ModelWarmup_Input) String() string { return proto.CompactTextString(m) }
func (*ModelWarmup_Input) ProtoMessage()    {}
func (*ModelWarmup_Input) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{11, 0}
}

func (m *ModelWarmup_Input) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelWarmup_Input.Unmarshal(m, b)
}
func (m *ModelWarmup_Input) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelWarmup_Input.Marshal(b, m, deterministic)
}
func (m *ModelWarmup_Input) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelWarmup_Input.Merge(m, src)
}
func (m *ModelWarmup_Input) XXX_Size() int {
	return xxx_messageInfo_ModelWarmup_Input.Size(m)
}
func (m *ModelWarmup_Input) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelWarmup_Input.DiscardUnknown(m)
}

var xxx_messageInfo_ModelWarmup_Input proto.InternalMessageInfo

func (m *ModelWarmup_Input) GetDataType() DataType {
	if m != nil {
		return m.DataType
	}
	return DataType_TYPE_INVALID
}

func (m *ModelWarmup_Input) GetDims() []int64 {
	if m != nil {
		return m.Dims
	}
	return nil
}

type isModelWarmup_Input_InputDataType interface {
	isModelWarmup_Input_InputDataType()
}

type ModelWarmup_Input_ZeroData struct {
	ZeroData bool `protobuf:"varint,3,opt,name=zero_data,json=zeroData,proto3,oneof"`
}

type ModelWarmup_Input_RandomData struct {
	RandomData bool `protobuf:"varint,4,opt,name=random_data,json=randomData,proto3,oneof"`
}

type ModelWarmup_Input_InputDataFile struct {
	InputDataFile string `protobuf:"bytes,5,opt,name=input_data_file,json=inputDataFile,proto3,oneof"`
}

func (*ModelWarmup_Input_ZeroData) isModelWarmup_Input_InputDataType() {}

func (*ModelWarmup_Input_RandomData) isModelWarmup_Input_InputDataType() {}

func (*ModelWarmup_Input_InputDataFile) isModelWarmup_Input_InputDataType() {}

func (m *ModelWarmup_Input) GetInputDataType() isModelWarmup_Input_InputDataType {
	if m != nil {
		return m.InputDataType
	}
	return nil
}

func (m *ModelWarmup_Input) GetZeroData() bool {
	if x, ok := m.GetInputDataType().(*ModelWarmup_Input_ZeroData); ok {
		return x.ZeroData
	}
	return false
}

func (m *ModelWarmup_Input) GetRandomData() bool {
	if x, ok := m.GetInputDataType().(*ModelWarmup_Input_RandomData); ok {
		return x.RandomData
	}
	return false
}

func (m *ModelWarmup_Input) GetInputDataFile() string {
	if x, ok := m.GetInputDataType().(*ModelWarmup_Input_InputDataFile); ok {
		return x.InputDataFile
	}
	return ""
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelWarmup_Input) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelWarmup_Input_ZeroData)(nil),
		(*ModelWarmup_Input_RandomData)(nil),
		(*ModelWarmup_Input_InputDataFile)(nil),
	}
}

//@@
//@@.. cpp:var:: message ModelConfig
//@@
//@@   A model configuration.
//@@
type ModelConfig struct {
	//@@  .. cpp:var:: string name
	//@@
	//@@     The name of the model.
	//@@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	//@@  .. cpp:var:: string platform
	//@@
	//@@     The framework for the model. Possible values are
	//@@     "tensorrt_plan", "tensorflow_graphdef",
	//@@     "tensorflow_savedmodel", "caffe2_netdef",
	//@@     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
	//@@
	Platform string `protobuf:"bytes,2,opt,name=platform,proto3" json:"platform,omitempty"`
	//@@  .. cpp:var:: ModelVersionPolicy version_policy
	//@@
	//@@     Policy indicating which version(s) of the model will be served.
	//@@
	VersionPolicy *ModelVersionPolicy `protobuf:"bytes,3,opt,name=version_policy,json=versionPolicy,proto3" json:"version_policy,omitempty"`
	//@@  .. cpp:var:: int32 max_batch_size
	//@@
	//@@     Maximum batch size allowed for inference. This can only decrease
	//@@     what is allowed by the model itself. A max_batch_size value of 0
	//@@     indicates that batching is not allowed for the model and the
	//@@     dimension/shape of the input and output tensors must exactly
	//@@     match what is specified in the input and output configuration. A
	//@@     max_batch_size value > 0 indicates that batching is allowed and
	//@@     so the model expects the input tensors to have an additional
	//@@     initial dimension for the batching that is not specified in the
	//@@     input (for example, if the model supports batched inputs of
	//@@     2-dimensional tensors then the model configuration will specify
	//@@     the input shape as [ X, Y ] but the model will expect the actual
	//@@     input tensors to have shape [ N, X, Y ]). For max_batch_size > 0
	//@@     returned outputs will also have an additional initial dimension
	//@@     for the batch.
	//@@
	MaxBatchSize int32 `protobuf:"varint,4,opt,name=max_batch_size,json=maxBatchSize,proto3" json:"max_batch_size,omitempty"`
	//@@  .. cpp:var:: ModelInput input (repeated)
	//@@
	//@@     The inputs request by the model.
	//@@
	Input []*ModelInput `protobuf:"bytes,5,rep,name=input,proto3" json:"input,omitempty"`
	//@@  .. cpp:var:: ModelOutput output (repeated)
	//@@
	//@@     The outputs produced by the model.
	//@@
	Output []*ModelOutput `protobuf:"bytes,6,rep,name=output,proto3" json:"output,omitempty"`
	//@@  .. cpp:var:: ModelOptimizationPolicy optimization
	//@@
	//@@     Optimization configuration for the model. If not specified
	//@@     then default optimization policy is used.
	//@@
	Optimization *ModelOptimizationPolicy `protobuf:"bytes,12,opt,name=optimization,proto3" json:"optimization,omitempty"`
	//@@  .. cpp:var:: oneof scheduling_choice
	//@@
	//@@     The scheduling policy for the model. If not specified the
	//@@     default scheduling policy is used for the model. The default
	//@@     policy is to execute each inference request independently.
	//@@
	//
	// Types that are valid to be assigned to SchedulingChoice:
	//	*ModelConfig_DynamicBatching
	//	*ModelConfig_SequenceBatching
	//	*ModelConfig_EnsembleScheduling
	SchedulingChoice isModelConfig_SchedulingChoice `protobuf_oneof:"scheduling_choice"`
	//@@  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
	//@@
	//@@     Instances of this model. If not specified, one instance
	//@@     of the model will be instantiated on each available GPU.
	//@@
	InstanceGroup []*ModelInstanceGroup `protobuf:"bytes,7,rep,name=instance_group,json=instanceGroup,proto3" json:"instance_group,omitempty"`
	//@@  .. cpp:var:: string default_model_filename
	//@@
	//@@     Optional filename of the model file to use if a
	//@@     compute-capability specific model is not specified in
	//@@     :cpp:var:`cc_model_filenames`. If not specified the default name
	//@@     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
	//@@     'model.netdef' depending on the model type.
	//@@
	DefaultModelFilename string `protobuf:"bytes,8,opt,name=default_model_filename,json=defaultModelFilename,proto3" json:"default_model_filename,omitempty"`
	//@@  .. cpp:var:: map<string,string> cc_model_filenames
	//@@
	//@@     Optional map from CUDA compute capability to the filename of
	//@@     the model that supports that compute capability. The filename
	//@@     refers to a file within the model version directory.
	//@@
	CcModelFilenames map[string]string `protobuf:"bytes,9,rep,name=cc_model_filenames,json=ccModelFilenames,proto3" json:"cc_model_filenames,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,string> metric_tags
	//@@
	//@@     Optional metric tags. User-specific key-value pairs for metrics
	//@@     reported for this model. These tags are applied to the metrics
	//@@     reported on the HTTP metrics port.
	//@@
	MetricTags map[string]string `protobuf:"bytes,10,rep,name=metric_tags,json=metricTags,proto3" json:"metric_tags,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: map<string,ModelParameter> parameters
	//@@
	//@@     Optional model parameters. User-specified parameter values that
	//@@     are made available to custom backends.
	//@@
	Parameters map[string]*ModelParameter `protobuf:"bytes,14,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
	//@@  .. cpp:var:: ModelWarmup model_warmup (repeated)
	//@@
	//@@     Warmup setting of this model. If specified, all instances
	//@@     will be run with the request samples in sequence before
	//@@     serving the model.
	//@@     This field can only be specified if the model is not an ensemble
	//@@     model.
	//@@
	ModelWarmup          []*ModelWarmup `protobuf:"bytes,16,rep,name=model_warmup,json=modelWarmup,proto3" json:"model_warmup,omitempty"`
	XXX_NoUnkeyedLiteral struct{}       `json:"-"`
	XXX_unrecognized     []byte         `json:"-"`
	XXX_sizecache        int32          `json:"-"`
}

func (m *ModelConfig) Reset()         { *m = ModelConfig{} }
func (m *ModelConfig) String() string { return proto.CompactTextString(m) }
func (*ModelConfig) ProtoMessage()    {}
func (*ModelConfig) Descriptor() ([]byte, []int) {
	return fileDescriptor_5214c5af697e4203, []int{12}
}

func (m *ModelConfig) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ModelConfig.Unmarshal(m, b)
}
func (m *ModelConfig) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ModelConfig.Marshal(b, m, deterministic)
}
func (m *ModelConfig) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ModelConfig.Merge(m, src)
}
func (m *ModelConfig) XXX_Size() int {
	return xxx_messageInfo_ModelConfig.Size(m)
}
func (m *ModelConfig) XXX_DiscardUnknown() {
	xxx_messageInfo_ModelConfig.DiscardUnknown(m)
}

var xxx_messageInfo_ModelConfig proto.InternalMessageInfo

func (m *ModelConfig) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *ModelConfig) GetPlatform() string {
	if m != nil {
		return m.Platform
	}
	return ""
}

func (m *ModelConfig) GetVersionPolicy() *ModelVersionPolicy {
	if m != nil {
		return m.VersionPolicy
	}
	return nil
}

func (m *ModelConfig) GetMaxBatchSize() int32 {
	if m != nil {
		return m.MaxBatchSize
	}
	return 0
}

func (m *ModelConfig) GetInput() []*ModelInput {
	if m != nil {
		return m.Input
	}
	return nil
}

func (m *ModelConfig) GetOutput() []*ModelOutput {
	if m != nil {
		return m.Output
	}
	return nil
}

func (m *ModelConfig) GetOptimization() *ModelOptimizationPolicy {
	if m != nil {
		return m.Optimization
	}
	return nil
}

type isModelConfig_SchedulingChoice interface {
	isModelConfig_SchedulingChoice()
}

type ModelConfig_DynamicBatching struct {
	DynamicBatching *ModelDynamicBatching `protobuf:"bytes,11,opt,name=dynamic_batching,json=dynamicBatching,proto3,oneof"`
}

type ModelConfig_SequenceBatching struct {
	SequenceBatching *ModelSequenceBatching `protobuf:"bytes,13,opt,name=sequence_batching,json=sequenceBatching,proto3,oneof"`
}

type ModelConfig_EnsembleScheduling struct {
	EnsembleScheduling *ModelEnsembling `protobuf:"bytes,15,opt,name=ensemble_scheduling,json=ensembleScheduling,proto3,oneof"`
}

func (*ModelConfig_DynamicBatching) isModelConfig_SchedulingChoice() {}

func (*ModelConfig_SequenceBatching) isModelConfig_SchedulingChoice() {}

func (*ModelConfig_EnsembleScheduling) isModelConfig_SchedulingChoice() {}

func (m *ModelConfig) GetSchedulingChoice() isModelConfig_SchedulingChoice {
	if m != nil {
		return m.SchedulingChoice
	}
	return nil
}

func (m *ModelConfig) GetDynamicBatching() *ModelDynamicBatching {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_DynamicBatching); ok {
		return x.DynamicBatching
	}
	return nil
}

func (m *ModelConfig) GetSequenceBatching() *ModelSequenceBatching {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_SequenceBatching); ok {
		return x.SequenceBatching
	}
	return nil
}

func (m *ModelConfig) GetEnsembleScheduling() *ModelEnsembling {
	if x, ok := m.GetSchedulingChoice().(*ModelConfig_EnsembleScheduling); ok {
		return x.EnsembleScheduling
	}
	return nil
}

func (m *ModelConfig) GetInstanceGroup() []*ModelInstanceGroup {
	if m != nil {
		return m.InstanceGroup
	}
	return nil
}

func (m *ModelConfig) GetDefaultModelFilename() string {
	if m != nil {
		return m.DefaultModelFilename
	}
	return ""
}

func (m *ModelConfig) GetCcModelFilenames() map[string]string {
	if m != nil {
		return m.CcModelFilenames
	}
	return nil
}

func (m *ModelConfig) GetMetricTags() map[string]string {
	if m != nil {
		return m.MetricTags
	}
	return nil
}

func (m *ModelConfig) GetParameters() map[string]*ModelParameter {
	if m != nil {
		return m.Parameters
	}
	return nil
}

func (m *ModelConfig) GetModelWarmup() []*ModelWarmup {
	if m != nil {
		return m.ModelWarmup
	}
	return nil
}

// XXX_OneofWrappers is for the internal use of the proto package.
func (*ModelConfig) XXX_OneofWrappers() []interface{} {
	return []interface{}{
		(*ModelConfig_DynamicBatching)(nil),
		(*ModelConfig_SequenceBatching)(nil),
		(*ModelConfig_EnsembleScheduling)(nil),
	}
}

func init() {
	proto.RegisterEnum("nvidia.inferenceserver.DataType", DataType_name, DataType_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelInstanceGroup_Kind", ModelInstanceGroup_Kind_name, ModelInstanceGroup_Kind_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelInput_Format", ModelInput_Format_name, ModelInput_Format_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelOptimizationPolicy_ModelPriority", ModelOptimizationPolicy_ModelPriority_name, ModelOptimizationPolicy_ModelPriority_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelQueuePolicy_TimeoutAction", ModelQueuePolicy_TimeoutAction_name, ModelQueuePolicy_TimeoutAction_value)
	proto.RegisterEnum("nvidia.inferenceserver.ModelSequenceBatching_Control_Kind", ModelSequenceBatching_Control_Kind_name, ModelSequenceBatching_Control_Kind_value)
	proto.RegisterType((*ModelInstanceGroup)(nil), "nvidia.inferenceserver.ModelInstanceGroup")
	proto.RegisterType((*ModelTensorReshape)(nil), "nvidia.inferenceserver.ModelTensorReshape")
	proto.RegisterType((*ModelInput)(nil), "nvidia.inferenceserver.ModelInput")
	proto.RegisterType((*ModelOutput)(nil), "nvidia.inferenceserver.ModelOutput")
	proto.RegisterType((*ModelVersionPolicy)(nil), "nvidia.inferenceserver.ModelVersionPolicy")
	proto.RegisterType((*ModelVersionPolicy_Latest)(nil), "nvidia.inferenceserver.ModelVersionPolicy.Latest")
	proto.RegisterType((*ModelVersionPolicy_All)(nil), "nvidia.inferenceserver.ModelVersionPolicy.All")
	proto.RegisterType((*ModelVersionPolicy_Specific)(nil), "nvidia.inferenceserver.ModelVersionPolicy.Specific")
	proto.RegisterType((*ModelOptimizationPolicy)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy")
	proto.RegisterType((*ModelOptimizationPolicy_Graph)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.Graph")
	proto.RegisterType((*ModelOptimizationPolicy_Cuda)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.Cuda")
	proto.RegisterType((*ModelOptimizationPolicy_ExecutionAccelerators)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators")
	proto.RegisterType((*ModelOptimizationPolicy_ExecutionAccelerators_Accelerator)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry")
	proto.RegisterType((*ModelOptimizationPolicy_PinnedMemoryBuffer)(nil), "nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer")
	proto.RegisterType((*ModelQueuePolicy)(nil), "nvidia.inferenceserver.ModelQueuePolicy")
	proto.RegisterType((*ModelDynamicBatching)(nil), "nvidia.inferenceserver.ModelDynamicBatching")
	proto.RegisterMapType((map[uint32]*ModelQueuePolicy)(nil), "nvidia.inferenceserver.ModelDynamicBatching.PriorityQueuePolicyEntry")
	proto.RegisterType((*ModelSequenceBatching)(nil), "nvidia.inferenceserver.ModelSequenceBatching")
	proto.RegisterType((*ModelSequenceBatching_Control)(nil), "nvidia.inferenceserver.ModelSequenceBatching.Control")
	proto.RegisterType((*ModelSequenceBatching_ControlInput)(nil), "nvidia.inferenceserver.ModelSequenceBatching.ControlInput")
	proto.RegisterType((*ModelSequenceBatching_StrategyDirect)(nil), "nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect")
	proto.RegisterType((*ModelSequenceBatching_StrategyOldest)(nil), "nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest")
	proto.RegisterType((*ModelEnsembling)(nil), "nvidia.inferenceserver.ModelEnsembling")
	proto.RegisterType((*ModelEnsembling_Step)(nil), "nvidia.inferenceserver.ModelEnsembling.Step")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry")
	proto.RegisterType((*ModelParameter)(nil), "nvidia.inferenceserver.ModelParameter")
	proto.RegisterType((*ModelWarmup)(nil), "nvidia.inferenceserver.ModelWarmup")
	proto.RegisterMapType((map[string]*ModelWarmup_Input)(nil), "nvidia.inferenceserver.ModelWarmup.InputsEntry")
	proto.RegisterType((*ModelWarmup_Input)(nil), "nvidia.inferenceserver.ModelWarmup.Input")
	proto.RegisterType((*ModelConfig)(nil), "nvidia.inferenceserver.ModelConfig")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry")
	proto.RegisterMapType((map[string]string)(nil), "nvidia.inferenceserver.ModelConfig.MetricTagsEntry")
	proto.RegisterMapType((map[string]*ModelParameter)(nil), "nvidia.inferenceserver.ModelConfig.ParametersEntry")
}

func init() { proto.RegisterFile("model_config.proto", fileDescriptor_5214c5af697e4203) }

var fileDescriptor_5214c5af697e4203 = []byte{
	// 2417 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x59, 0x4b, 0x73, 0x1b, 0xc7,
	0xf1, 0xe7, 0xe2, 0x45, 0xa0, 0x41, 0x00, 0xab, 0x11, 0x25, 0xc1, 0xb0, 0xe5, 0xa2, 0xe1, 0xff,
	0x5f, 0x86, 0x65, 0x05, 0x8e, 0x41, 0x99, 0xe5, 0x58, 0x56, 0x6c, 0x10, 0x00, 0x45, 0xc4, 0x24,
	0x40, 0x0d, 0x20, 0x29, 0x52, 0x25, 0xb5, 0xb5, 0xda, 0x1d, 0x40, 0x5b, 0xde, 0x97, 0xf7, 0x41,
	0x8b, 0xaa, 0x5c, 0x72, 0xcc, 0x21, 0x95, 0x6f, 0x90, 0x7b, 0x3e, 0x41, 0x0e, 0xf9, 0x08, 0xc9,
	0x21, 0xd7, 0x5c, 0x52, 0x95, 0xaa, 0xe4, 0x94, 0x6b, 0x4e, 0xa9, 0x1c, 0x52, 0xf3, 0xd8, 0xe5,
	0x2e, 0x09, 0x41, 0x84, 0x22, 0xdf, 0xb6, 0x7b, 0xa6, 0x7f, 0xdd, 0xd3, 0xd3, 0xd3, 0xd3, 0x3d,
	0x0b, 0xc8, 0x72, 0x74, 0x62, 0x2a, 0x9a, 0x63, 0xcf, 0x8c, 0x79, 0xdb, 0xf5, 0x9c, 0xc0, 0x41,
	0x57, 0xed, 0x63, 0x43, 0x37, 0xd4, 0xb6, 0x61, 0xcf, 0x88, 0x47, 0x6c, 0x8d, 0xf8, 0xc4, 0x3b,
	0x26, 0x5e, 0xf3, 0x5f, 0x12, 0xa0, 0x43, 0x3a, 0x7d, 0x68, 0xfb, 0x81, 0x6a, 0x6b, 0xe4, 0x9e,
	0xe7, 0x84, 0x2e, 0x42, 0x90, 0xb3, 0x55, 0x8b, 0xd4, 0xa5, 0x2d, 0xa9, 0x55, 0xc2, 0xec, 0x1b,
	0xf5, 0x20, 0xf7, 0x8d, 0x61, 0xeb, 0xf5, 0xdc, 0x96, 0xd4, 0xaa, 0x76, 0x3e, 0x6e, 0x2f, 0x46,
	0x6c, 0x9f, 0x47, 0x6b, 0x7f, 0x6d, 0xd8, 0x3a, 0x66, 0xc2, 0x68, 0x13, 0xf2, 0x9a, 0x13, 0xda,
	0x41, 0x3d, 0xb3, 0x25, 0xb5, 0xf2, 0x98, 0x13, 0x54, 0xdd, 0xdc, 0x0d, 0xfd, 0x7a, 0x76, 0x2b,
	0xdb, 0xca, 0x63, 0xf6, 0x8d, 0xea, 0xb0, 0xee, 0x7a, 0xce, 0xcc, 0x30, 0x49, 0x3d, 0xbf, 0x95,
	0x6d, 0x95, 0x70, 0x44, 0x36, 0xbb, 0x90, 0xa3, 0x88, 0xa8, 0x02, 0xa5, 0xaf, 0x87, 0xa3, 0xbe,
	0xd2, 0x7d, 0x30, 0x1d, 0xcb, 0x6b, 0x68, 0x03, 0x8a, 0x8c, 0xbc, 0x77, 0xf4, 0x40, 0x96, 0x62,
	0xaa, 0x77, 0xf4, 0x40, 0xce, 0xa0, 0x2a, 0x00, 0xa3, 0x0e, 0xc7, 0xfd, 0xc1, 0x81, 0x9c, 0x6d,
	0xde, 0x14, 0xab, 0x9e, 0x12, 0xdb, 0x77, 0x3c, 0x4c, 0xfc, 0x67, 0xaa, 0x4b, 0xa8, 0x71, 0xec,
	0xa3, 0x2e, 0x6d, 0x65, 0x5b, 0x59, 0xcc, 0x89, 0xe6, 0x6f, 0xb2, 0x00, 0x62, 0x51, 0x6e, 0x18,
	0x2c, 0x74, 0xcd, 0x5d, 0x28, 0xe9, 0x6a, 0xa0, 0x2a, 0xc1, 0x89, 0x4b, 0xd8, 0xca, 0xaa, 0x9d,
	0xad, 0x97, 0xf9, 0xa7, 0xaf, 0x06, 0xea, 0xf4, 0xc4, 0x25, 0xb8, 0xa8, 0x8b, 0x2f, 0xd4, 0x85,
	0xc2, 0xcc, 0xf1, 0x2c, 0x35, 0xa8, 0x67, 0x99, 0xec, 0x87, 0xaf, 0xf0, 0xad, 0x1b, 0x06, 0xed,
	0x3d, 0x26, 0x80, 0x85, 0x20, 0xb5, 0x4a, 0x37, 0x2c, 0xbf, 0x9e, 0x63, 0x96, 0xb3, 0x6f, 0xd4,
	0x87, 0x75, 0x8f, 0xaf, 0xac, 0x9e, 0xdf, 0x92, 0x5a, 0xe5, 0xce, 0xcd, 0xa5, 0xb8, 0x29, 0x5f,
	0xe0, 0x48, 0x14, 0xdd, 0x80, 0x9a, 0xe1, 0x2b, 0xec, 0x5b, 0x09, 0xd8, 0x94, 0x7a, 0x61, 0x4b,
	0x6a, 0x15, 0x71, 0xc5, 0xf0, 0x27, 0x94, 0xcb, 0xe5, 0xd0, 0x2d, 0x40, 0xaa, 0x69, 0x3a, 0xdf,
	0x29, 0x9e, 0x3a, 0x9f, 0x13, 0x5d, 0x79, 0xaa, 0x06, 0xda, 0xb3, 0xfa, 0x3a, 0x9b, 0x2a, 0xb3,
	0x11, 0xcc, 0x06, 0x76, 0x29, 0xbf, 0x79, 0x07, 0x0a, 0x7c, 0x05, 0xa8, 0x06, 0xe5, 0xbd, 0x31,
	0x3e, 0xec, 0x4e, 0x95, 0xd1, 0x78, 0x34, 0x90, 0xd7, 0x92, 0x8c, 0xfd, 0x47, 0x3d, 0x59, 0x4a,
	0x32, 0x7a, 0xfb, 0x8f, 0xe4, 0x4c, 0xf3, 0x57, 0x19, 0x28, 0x33, 0x93, 0xc7, 0x61, 0xf0, 0x3d,
	0x6d, 0x49, 0xe4, 0xcf, 0xec, 0x1b, 0xf7, 0xe7, 0xff, 0x43, 0xd5, 0x54, 0x9f, 0x12, 0x53, 0xa1,
	0xb1, 0xcc, 0xcc, 0xce, 0x31, 0xb3, 0x2b, 0x8c, 0xbb, 0x27, 0x98, 0x17, 0x75, 0x7b, 0xf3, 0x1f,
	0x19, 0x11, 0xca, 0x0f, 0x89, 0xe7, 0x1b, 0x8e, 0x7d, 0xe4, 0x98, 0x86, 0x76, 0x82, 0xbe, 0x86,
	0x82, 0xa9, 0x06, 0xc4, 0x0f, 0x98, 0x53, 0xca, 0x9d, 0x4f, 0x96, 0x9a, 0x9a, 0x92, 0x6d, 0x1f,
	0x30, 0xc1, 0xfd, 0x35, 0x2c, 0x20, 0xd0, 0x2e, 0x64, 0x55, 0xd3, 0x64, 0x5e, 0x2c, 0x77, 0xda,
	0x2b, 0x20, 0x75, 0x4d, 0x73, 0x7f, 0x0d, 0x53, 0x61, 0x74, 0x1f, 0x8a, 0xbe, 0x4b, 0x34, 0x63,
	0x66, 0x68, 0x2c, 0xca, 0xcb, 0x9d, 0xed, 0x15, 0x80, 0x26, 0x42, 0x74, 0x7f, 0x0d, 0xc7, 0x30,
	0x8d, 0x8f, 0xa0, 0xc0, 0x4d, 0x45, 0xef, 0xc1, 0x86, 0x1d, 0x5a, 0xca, 0x31, 0x97, 0xf1, 0xd9,
	0x9a, 0x2b, 0xb8, 0x6c, 0x87, 0x96, 0x80, 0xf1, 0x1b, 0x79, 0xc8, 0x76, 0x4d, 0xb3, 0x71, 0x03,
	0x8a, 0x11, 0x16, 0x6a, 0x40, 0x31, 0x21, 0x41, 0xf7, 0x39, 0xa6, 0x77, 0x6b, 0x50, 0x71, 0x99,
	0x6a, 0x45, 0x7b, 0xe6, 0x18, 0x1a, 0x69, 0xfe, 0x09, 0xe0, 0x1a, 0x8f, 0x39, 0x37, 0x30, 0x2c,
	0xe3, 0x85, 0x1a, 0x24, 0x9d, 0x9d, 0x9f, 0x7b, 0xaa, 0xfb, 0x4c, 0xf8, 0xfa, 0xd3, 0xa5, 0x0b,
	0x3b, 0x2f, 0xdf, 0xbe, 0x47, 0x85, 0x31, 0xc7, 0x40, 0x8f, 0xa1, 0xe8, 0x7a, 0x86, 0xe3, 0x19,
	0xc1, 0x89, 0x88, 0xdb, 0xbb, 0xab, 0xe2, 0x31, 0xfe, 0x91, 0x00, 0xc1, 0x31, 0x1c, 0xda, 0x87,
	0x9c, 0x16, 0xea, 0xaa, 0xf0, 0xff, 0xed, 0x55, 0x61, 0x7b, 0xa1, 0xae, 0x62, 0x86, 0x80, 0x7e,
	0x01, 0x57, 0xc9, 0x73, 0xa2, 0x85, 0x74, 0x54, 0x51, 0x35, 0x8d, 0x98, 0xc4, 0x53, 0x03, 0xc7,
	0xf3, 0x59, 0x30, 0x97, 0x3b, 0x83, 0x55, 0xb1, 0x07, 0x11, 0x5a, 0x37, 0x01, 0x86, 0xaf, 0x90,
	0x45, 0x6c, 0xe4, 0xc1, 0x65, 0x83, 0x26, 0x41, 0xc5, 0x35, 0x6c, 0x9b, 0xe8, 0x8a, 0x45, 0x2c,
	0xc7, 0x3b, 0x11, 0x87, 0x72, 0x77, 0x55, 0xd5, 0x47, 0x0c, 0xe4, 0x90, 0x61, 0xec, 0x86, 0xb3,
	0x19, 0xf1, 0xf0, 0x25, 0x06, 0x9f, 0x1c, 0x40, 0x01, 0x6c, 0x3a, 0x2c, 0xdb, 0x9c, 0x51, 0x5a,
	0x78, 0x63, 0x4a, 0x11, 0xc7, 0x4f, 0x8e, 0x34, 0xae, 0x43, 0x9e, 0x05, 0x07, 0xbd, 0x9a, 0x4c,
	0x72, 0x4c, 0x4c, 0x16, 0x62, 0x79, 0xcc, 0x89, 0xc6, 0xbb, 0x90, 0xa3, 0x9b, 0x82, 0xae, 0x42,
	0x81, 0x05, 0x0f, 0x8f, 0xfc, 0x22, 0x16, 0x54, 0xe3, 0xb7, 0x39, 0xb8, 0xb2, 0xd0, 0xb3, 0xe8,
	0xd7, 0x12, 0xbc, 0x35, 0x77, 0x43, 0x65, 0xe1, 0x2e, 0xb2, 0xd3, 0x50, 0xee, 0xdc, 0x7f, 0x23,
	0x9b, 0xd8, 0x4e, 0x10, 0xf8, 0xda, 0xdc, 0x0d, 0x17, 0xcd, 0x62, 0xf6, 0x68, 0x2f, 0xb5, 0x27,
	0xf3, 0xbd, 0xd9, 0xa3, 0x2d, 0xb6, 0xa7, 0xf1, 0x77, 0x09, 0xca, 0x49, 0xfb, 0x16, 0x5d, 0x31,
	0xbf, 0x94, 0x00, 0x5c, 0xd5, 0x53, 0x2d, 0x12, 0x10, 0xcf, 0x17, 0x46, 0xaa, 0x6f, 0xdc, 0xc8,
	0xf6, 0x51, 0xac, 0x63, 0x60, 0x07, 0xde, 0x09, 0x4e, 0x28, 0x6d, 0xdc, 0x85, 0xda, 0x99, 0x61,
	0x24, 0x43, 0xf6, 0x1b, 0x72, 0x22, 0x2c, 0xa5, 0x9f, 0x34, 0x78, 0x8e, 0x55, 0x33, 0xe4, 0xf7,
	0x60, 0x09, 0x73, 0xe2, 0xf3, 0xcc, 0x67, 0x52, 0xe3, 0x16, 0xa0, 0xf3, 0x91, 0x48, 0xc3, 0x89,
	0xd8, 0xea, 0x53, 0x93, 0x44, 0xe1, 0xc4, 0xa9, 0xe6, 0x10, 0x2a, 0xa9, 0xd4, 0x82, 0x36, 0x41,
	0x3e, 0xc2, 0xc3, 0x31, 0x1e, 0x4e, 0x1f, 0x2b, 0xfd, 0xc1, 0x5e, 0xf7, 0xc1, 0xc1, 0x54, 0x5e,
	0x43, 0x32, 0x6c, 0xc4, 0xdc, 0xc3, 0xee, 0x4f, 0x65, 0x29, 0xcd, 0x19, 0x8e, 0xe4, 0x4c, 0xf3,
	0xf7, 0x19, 0x90, 0x19, 0xd6, 0xfd, 0x90, 0x84, 0x44, 0xe4, 0xd1, 0x9f, 0x43, 0x35, 0x30, 0x2c,
	0xe2, 0x84, 0x81, 0xa2, 0x6a, 0xd4, 0x25, 0x4c, 0x7f, 0xb5, 0xb3, 0xb3, 0xd4, 0xa7, 0x09, 0x84,
	0xf6, 0x94, 0x8b, 0x77, 0x99, 0x34, 0xae, 0x04, 0x49, 0x12, 0x7d, 0x05, 0xef, 0xe8, 0x64, 0xa6,
	0x86, 0x66, 0xa0, 0x44, 0x6a, 0x2c, 0x43, 0xf3, 0x1c, 0x9f, 0x68, 0x8e, 0xad, 0xfb, 0xcc, 0x3b,
	0x39, 0xdc, 0x10, 0x73, 0x04, 0xd4, 0x61, 0x62, 0x06, 0xba, 0x0d, 0x57, 0x79, 0x8d, 0x13, 0xc9,
	0x3b, 0xc7, 0xc4, 0xf3, 0x0c, 0x9d, 0xb0, 0x94, 0x5a, 0xc4, 0x9b, 0x6c, 0x54, 0x48, 0x8e, 0xc5,
	0x18, 0xfa, 0x3f, 0xa8, 0x5a, 0xea, 0x73, 0xe5, 0x5b, 0x6a, 0xa7, 0xe2, 0x1b, 0x2f, 0xf8, 0x8d,
	0x5f, 0xc1, 0x1b, 0x96, 0xfa, 0x9c, 0x19, 0x3f, 0x31, 0x5e, 0x90, 0xe6, 0x0d, 0xa8, 0xa4, 0xac,
	0x47, 0x00, 0x05, 0x3c, 0xf8, 0xc9, 0xa0, 0x47, 0x5d, 0x5a, 0x82, 0x7c, 0x7f, 0x70, 0xd0, 0x7d,
	0x2c, 0x4b, 0xcd, 0xbf, 0xe6, 0x60, 0x93, 0xad, 0xbb, 0x7f, 0x62, 0xab, 0x96, 0xa1, 0xb1, 0x7a,
	0xca, 0xb0, 0xe7, 0xe8, 0x87, 0xb0, 0xe9, 0x7a, 0x64, 0x46, 0x3c, 0x2f, 0xaa, 0xbe, 0xb8, 0x32,
	0x89, 0x15, 0xd5, 0x28, 0x1e, 0x63, 0x02, 0x54, 0x25, 0xfa, 0x12, 0xde, 0x39, 0x35, 0x4c, 0x27,
	0xa6, 0x7a, 0xb2, 0xc8, 0x21, 0x6f, 0x45, 0x66, 0xf6, 0xe9, 0x8c, 0x94, 0x3f, 0x3e, 0x82, 0x4b,
	0xae, 0xc7, 0x37, 0x43, 0x71, 0x3c, 0x9d, 0x78, 0x86, 0x3d, 0x17, 0xae, 0x90, 0xa3, 0x81, 0xb1,
	0xe0, 0xa3, 0x0f, 0xa0, 0x16, 0xdd, 0x44, 0x0a, 0x4b, 0x5f, 0xbe, 0xf0, 0x43, 0x35, 0x62, 0x1f,
	0x30, 0x2e, 0xf5, 0x72, 0xb4, 0x4f, 0x69, 0x01, 0x96, 0xe1, 0x2b, 0x78, 0x53, 0x8c, 0x1e, 0x25,
	0xc5, 0xd0, 0x13, 0x88, 0xf8, 0x62, 0x41, 0xfc, 0xfe, 0x16, 0x09, 0xba, 0x75, 0xd1, 0x10, 0xc2,
	0x48, 0xa0, 0x24, 0x03, 0xf3, 0x04, 0xae, 0xc4, 0x96, 0xa4, 0xc0, 0xd7, 0xd9, 0x99, 0x5f, 0x7e,
	0xdb, 0x9d, 0xd9, 0xa7, 0x76, 0x64, 0x76, 0x42, 0x01, 0x3f, 0xd7, 0x97, 0xdd, 0xf3, 0x23, 0x0d,
	0x17, 0xea, 0x2f, 0x13, 0x48, 0x9e, 0xf4, 0x0a, 0x3f, 0xe9, 0x3f, 0x4e, 0x9e, 0xf4, 0x55, 0x56,
	0x7d, 0x9a, 0x13, 0x9a, 0xbf, 0x2b, 0xc2, 0x15, 0x36, 0x3e, 0x21, 0xdf, 0x86, 0x54, 0x26, 0x8e,
	0xb0, 0x87, 0x50, 0xd0, 0x0d, 0x8f, 0x68, 0x81, 0xa8, 0x20, 0xbe, 0x58, 0x0a, 0x7f, 0x56, 0xbc,
	0x3d, 0x09, 0x3c, 0x35, 0x20, 0xf3, 0x93, 0x3e, 0xc3, 0xa0, 0xf5, 0x25, 0x47, 0xa3, 0xb8, 0x8e,
	0xa9, 0xd3, 0x62, 0x35, 0xf7, 0xbf, 0xe0, 0x8e, 0x19, 0x06, 0xc5, 0xe5, 0x68, 0xa8, 0x07, 0xef,
	0xd2, 0xf8, 0xf6, 0x85, 0x80, 0x62, 0xe8, 0x26, 0x49, 0x47, 0xb8, 0xc4, 0x22, 0xfc, 0x6d, 0x4b,
	0x7d, 0x1e, 0xa1, 0x0e, 0x75, 0x93, 0xa4, 0x62, 0x5c, 0x81, 0x8a, 0xe6, 0xd8, 0x81, 0xe7, 0x98,
	0x0a, 0xab, 0x0a, 0x44, 0x9e, 0xff, 0x7c, 0x35, 0x1b, 0x7b, 0x1c, 0x82, 0xf5, 0x6e, 0x78, 0x43,
	0x4b, 0x50, 0x8d, 0xbf, 0x65, 0x60, 0x5d, 0x0c, 0xa3, 0x91, 0xe8, 0xb1, 0x79, 0xde, 0x7b, 0x3d,
	0x1d, 0xc9, 0x76, 0xbb, 0x05, 0xb2, 0x61, 0x07, 0xdb, 0x1d, 0x65, 0xa6, 0x9a, 0x3e, 0x51, 0x02,
	0x8f, 0x85, 0x06, 0xcd, 0x07, 0x55, 0xc6, 0xdf, 0xa3, 0xec, 0xa9, 0x17, 0xb2, 0x7e, 0x63, 0xe6,
	0xa6, 0x27, 0xd2, 0xde, 0x27, 0x83, 0x2b, 0x94, 0x7d, 0x3a, 0x2f, 0xd5, 0x57, 0xe5, 0x56, 0xed,
	0xab, 0x9a, 0xa1, 0xe8, 0xdd, 0x1b, 0x70, 0xb5, 0x37, 0x1e, 0x4d, 0xf1, 0xf8, 0x40, 0x99, 0x0c,
	0xee, 0x3f, 0x18, 0x8c, 0x7a, 0x03, 0x65, 0x32, 0xed, 0x62, 0x9a, 0xec, 0x16, 0x8d, 0xe1, 0x41,
	0xb7, 0xff, 0x58, 0x96, 0x50, 0x1d, 0x36, 0xcf, 0x8d, 0x0d, 0x46, 0x7d, 0x39, 0x83, 0xde, 0x86,
	0x6b, 0xe7, 0x46, 0x7a, 0x63, 0x8c, 0x87, 0x7d, 0x39, 0xdb, 0xf0, 0x61, 0x23, 0xb9, 0x03, 0x0b,
	0xaf, 0xf3, 0x31, 0xac, 0x8b, 0x7d, 0x11, 0x5b, 0xfc, 0xe9, 0x6b, 0xb9, 0x1f, 0x47, 0x28, 0x0d,
	0x19, 0xaa, 0xe9, 0x90, 0x6f, 0xfc, 0x41, 0x3a, 0x65, 0xf1, 0x68, 0x45, 0x3b, 0x70, 0x8d, 0xc6,
	0xa8, 0xa6, 0xda, 0xba, 0xa1, 0xab, 0x01, 0x89, 0xa3, 0xd5, 0x17, 0xa5, 0xde, 0x15, 0x4b, 0x7d,
	0xde, 0x8b, 0x46, 0x23, 0xa5, 0xfe, 0x4b, 0xb3, 0x7d, 0xe6, 0xb5, 0xb3, 0x7d, 0xf6, 0x15, 0xd9,
	0x7e, 0xf7, 0x12, 0xd4, 0x7c, 0x61, 0x7c, 0xd4, 0x15, 0xfd, 0x39, 0x0b, 0x35, 0xe6, 0x8d, 0x81,
	0xed, 0x13, 0xeb, 0xa9, 0x49, 0xb3, 0xc4, 0x57, 0x90, 0xf3, 0x03, 0xe2, 0x8a, 0x22, 0xf2, 0xd6,
	0x52, 0x27, 0x9e, 0x8a, 0xb5, 0x27, 0x01, 0x71, 0x31, 0x93, 0x6c, 0xfc, 0x3b, 0x03, 0x39, 0x4a,
	0xa2, 0xeb, 0x00, 0xfc, 0x2d, 0x2b, 0xb1, 0x59, 0x25, 0xc6, 0x19, 0xd1, 0x1d, 0x7b, 0x1f, 0x2a,
	0x7c, 0x58, 0xb4, 0x6d, 0x2c, 0xeb, 0x65, 0xf1, 0x86, 0x95, 0x68, 0x20, 0xd1, 0x23, 0x28, 0xf1,
	0x66, 0xc1, 0x52, 0x5d, 0x16, 0xd2, 0xaf, 0x3a, 0xbb, 0x67, 0x6c, 0x6a, 0xb3, 0x88, 0x39, 0x54,
	0x5d, 0x9e, 0xa4, 0x8b, 0x86, 0x20, 0xd1, 0x13, 0x00, 0xd1, 0x11, 0x50, 0xe4, 0x1c, 0x43, 0xbe,
	0xb3, 0x12, 0x32, 0x7f, 0xbe, 0x88, 0xa1, 0x4b, 0x4e, 0x44, 0x37, 0xee, 0x40, 0x25, 0xa5, 0x76,
	0xa5, 0xa2, 0xee, 0x0b, 0xa8, 0xa6, 0x91, 0x57, 0x91, 0x6e, 0x6e, 0x43, 0x95, 0x17, 0x79, 0x51,
	0x59, 0x49, 0xbb, 0x6b, 0x3f, 0xa0, 0x57, 0xb8, 0xc2, 0x45, 0x38, 0x4c, 0x99, 0xf3, 0x1e, 0x52,
	0x56, 0xf3, 0x8f, 0x59, 0xf1, 0x22, 0xf3, 0x48, 0xf5, 0xac, 0x97, 0xbc, 0x1f, 0x5e, 0x07, 0x48,
	0xc5, 0x29, 0xbd, 0xb4, 0x4a, 0x4f, 0xe3, 0xf0, 0xbc, 0x07, 0x05, 0xe6, 0x5a, 0x5f, 0x6c, 0xd2,
	0xf2, 0x07, 0x46, 0xae, 0x87, 0xef, 0x8d, 0x28, 0x8b, 0x85, 0x78, 0xe3, 0x2f, 0x12, 0xe4, 0xf9,
	0x29, 0x4f, 0xe5, 0x2a, 0xe9, 0xb5, 0xdf, 0x80, 0x32, 0x89, 0x37, 0xa0, 0xeb, 0x50, 0x7a, 0x41,
	0x3c, 0x47, 0xa1, 0x93, 0x78, 0xa5, 0xb3, 0xbf, 0x86, 0x8b, 0x94, 0x45, 0x01, 0xd0, 0x7b, 0x50,
	0xf6, 0x54, 0x5b, 0x77, 0x2c, 0x3e, 0x21, 0x27, 0x26, 0x00, 0x67, 0xb2, 0x29, 0x2d, 0xa8, 0xf1,
	0x78, 0x64, 0xa6, 0x89, 0xf7, 0x4d, 0xa9, 0x55, 0xda, 0x5f, 0xc3, 0x15, 0x36, 0x40, 0x67, 0xed,
	0x19, 0x26, 0xa1, 0xe7, 0x2d, 0x31, 0x93, 0x2e, 0xa2, 0xa1, 0x43, 0x39, 0xb1, 0xe4, 0x05, 0xfb,
	0xfa, 0x65, 0xba, 0x00, 0xf8, 0xf0, 0xc2, 0x4e, 0x4c, 0x86, 0xc0, 0x7f, 0x40, 0xec, 0x66, 0x8f,
	0x3d, 0x21, 0x2f, 0xdc, 0xcd, 0x06, 0x14, 0x5d, 0x53, 0x0d, 0x66, 0x8e, 0x67, 0x89, 0x18, 0x8a,
	0x69, 0x74, 0x1f, 0xaa, 0xe2, 0x44, 0x46, 0x75, 0x52, 0xf6, 0x02, 0xef, 0x65, 0xa9, 0x17, 0x1f,
	0x5c, 0x39, 0x4e, 0xbd, 0x67, 0x89, 0x1a, 0x3a, 0x11, 0x40, 0x39, 0x96, 0x1d, 0x69, 0x0d, 0x7d,
	0x9a, 0xe2, 0x3e, 0x83, 0x3c, 0xbf, 0xa3, 0xf3, 0x2c, 0x84, 0x9a, 0xaf, 0x7e, 0x47, 0xc5, 0x5c,
	0x00, 0xdd, 0x81, 0x02, 0x3f, 0x7d, 0xf5, 0x02, 0x13, 0x7d, 0x7f, 0x79, 0x1b, 0xc7, 0xa6, 0x62,
	0x21, 0x82, 0x26, 0xb0, 0xe1, 0x24, 0x1a, 0xbb, 0xfa, 0x06, 0x5b, 0xed, 0xc7, 0x2b, 0x76, 0x82,
	0x38, 0x05, 0x82, 0x1e, 0x83, 0xac, 0xf3, 0xca, 0x91, 0xaf, 0x9a, 0x96, 0xd6, 0x65, 0x06, 0x7c,
	0x6b, 0x95, 0x72, 0x73, 0x7f, 0x0d, 0xd7, 0xf4, 0x33, 0x9d, 0xc2, 0xcf, 0xe0, 0x52, 0x5c, 0x13,
	0xc5, 0xd8, 0x15, 0x86, 0xfd, 0x83, 0x95, 0xee, 0xbc, 0xfd, 0x35, 0x2c, 0xfb, 0x67, 0xab, 0xc4,
	0x27, 0x70, 0x99, 0xf0, 0x44, 0x47, 0x14, 0x5f, 0x7b, 0x46, 0xf4, 0x90, 0x66, 0xbc, 0x7a, 0x8d,
	0xe1, 0x7f, 0x70, 0xc1, 0x04, 0xb9, 0xbf, 0x86, 0x51, 0x84, 0x32, 0x89, 0x41, 0x68, 0x64, 0x19,
	0xe2, 0xd7, 0x82, 0x32, 0xf7, 0x9c, 0xd0, 0x15, 0x15, 0xf8, 0xcd, 0x8b, 0xff, 0x8d, 0xa0, 0xa7,
	0x2c, 0xf9, 0xab, 0x23, 0xd1, 0x6d, 0xf0, 0xcb, 0x24, 0x7e, 0x97, 0x2d, 0xb2, 0xb0, 0x8e, 0xba,
	0x0a, 0x86, 0x14, 0x3f, 0xcf, 0xce, 0x01, 0x69, 0xda, 0x19, 0x01, 0xbf, 0x5e, 0x62, 0xc6, 0xfc,
	0x68, 0xa9, 0x31, 0xfc, 0x4c, 0xb5, 0x7b, 0x5a, 0x0a, 0x50, 0xe4, 0x30, 0x59, 0x3b, 0xc3, 0x46,
	0x53, 0x28, 0x5b, 0x24, 0xf0, 0x0c, 0x4d, 0x09, 0xd4, 0xb9, 0x5f, 0x07, 0xa6, 0x61, 0xfb, 0x22,
	0x1a, 0x0e, 0x99, 0xd8, 0x54, 0x9d, 0x47, 0xcf, 0x06, 0x56, 0xcc, 0x40, 0x93, 0xd4, 0xcb, 0x45,
	0xf5, 0xe2, 0xa0, 0x4b, 0xde, 0x22, 0xd0, 0x1e, 0xf0, 0x9b, 0x57, 0xf9, 0x8e, 0xe5, 0x95, 0xba,
	0x7c, 0x81, 0x93, 0xc4, 0x53, 0x10, 0x2e, 0x5b, 0xa7, 0x44, 0xa3, 0x07, 0x57, 0x16, 0x7a, 0x67,
	0xa5, 0x4b, 0xf0, 0x2e, 0xd4, 0xce, 0x38, 0x60, 0x25, 0x71, 0x72, 0x91, 0x77, 0x95, 0x2f, 0xd2,
	0xc9, 0xf6, 0xc6, 0xd2, 0x95, 0xc6, 0x70, 0x09, 0x35, 0xbb, 0x97, 0xe1, 0xd2, 0xe9, 0x11, 0x11,
	0x45, 0xd5, 0xcd, 0x7f, 0x4a, 0x50, 0x8c, 0xae, 0x23, 0x24, 0xc3, 0xc6, 0xf4, 0xf1, 0xd1, 0x40,
	0x19, 0x8e, 0x1e, 0x76, 0x0f, 0x86, 0x7d, 0x79, 0x0d, 0x55, 0xa0, 0xc4, 0x38, 0xbb, 0xe3, 0xf1,
	0x81, 0x2c, 0xa1, 0x2a, 0x00, 0x23, 0x1f, 0x0c, 0x47, 0xd3, 0xcf, 0xe4, 0x0c, 0xaa, 0x41, 0x39,
	0xa6, 0x3f, 0xd9, 0x91, 0xb3, 0x29, 0xc6, 0x76, 0x47, 0xce, 0xa5, 0x18, 0x3b, 0xb7, 0xe5, 0x7c,
	0x8c, 0xc8, 0x10, 0x0a, 0x31, 0x22, 0x07, 0x58, 0x4f, 0xd2, 0xdb, 0x1d, 0xb9, 0x98, 0xa4, 0x77,
	0x6e, 0xcb, 0xa5, 0x58, 0x7c, 0xef, 0xe8, 0x93, 0x1d, 0x19, 0x12, 0xe4, 0x76, 0x47, 0x2e, 0x27,
	0xc8, 0x9d, 0xdb, 0xf2, 0x46, 0xac, 0x7c, 0x32, 0xc5, 0xc3, 0xd1, 0x3d, 0xb9, 0xf2, 0xb4, 0xc0,
	0x7e, 0x50, 0x6e, 0xff, 0x37, 0x00, 0x00, 0xff, 0xff, 0xac, 0x72, 0xac, 0x44, 0xb6, 0x1c, 0x00,
	0x00,
}
