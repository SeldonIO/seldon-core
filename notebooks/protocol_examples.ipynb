{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Examples with Different Protocols\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    " * A kubernetes cluster with kubectl configured\n",
    " * curl\n",
    " * grpcurl\n",
    " * pygmentize\n",
    " \n",
    "\n",
    "## Setup Seldon Core\n",
    "\n",
    "Use the setup notebook to [Setup Cluster](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html) to setup Seldon Core with an ingress - either Ambassador or Istio.\n",
    "\n",
    "Then port-forward to that ingress on localhost:8003 in a separate terminal either with:\n",
    "\n",
    " * Ambassador: `kubectl port-forward $(kubectl get pods -n seldon -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') -n seldon 8003:8080`\n",
    " * Istio: `kubectl port-forward $(kubectl get pods -l istio=ingressgateway -n istio-system -o jsonpath='{.items[0].metadata.name}') -n istio-system 8003:80`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create namespace seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl config set-context $(kubectl config current-context) --namespace=seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seldon Protocol REST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy a REST model that uses the SELDON Protocol namely by specifying the attribute `protocol: seldon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_seldon_rest.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: rest-seldon\n",
    "spec:\n",
    "  name: restseldon\n",
    "  protocol: seldon\n",
    "  transport: rest  \n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/mock_classifier_rest:1.3\n",
    "          name: classifier\n",
    "    graph:\n",
    "      name: classifier\n",
    "      type: MODEL\n",
    "    name: model\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_seldon_rest.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=rest-seldon -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep rest-seldon -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send requests using the Seldon protocol format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!curl -s -d '{\"data\": {\"ndarray\":[[1.0, 2.0, 5.0]]}}' \\\n",
    "   -X POST http://localhost:8003/seldon/seldon/rest-seldon/api/v1.0/predictions \\\n",
    "   -H \"Content-Type: application/json\"\n",
    "d=json.loads(X[0])\n",
    "print(d)\n",
    "assert(d[\"data\"][\"ndarray\"][0][0] > 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_seldon_rest.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seldon Protocol GRPC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy a model with a GRPC endpoint that uses the SELDON Protocol namely by specifying the attribute `protocol: seldon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_seldon_grpc.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: grpc-seldon\n",
    "spec:\n",
    "  name: grpcseldon\n",
    "  protocol: seldon\n",
    "  transport: grpc\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - image: seldonio/mock_classifier_grpc:1.3\n",
    "          name: classifier\n",
    "    graph:\n",
    "      name: classifier\n",
    "      type: MODEL\n",
    "      endpoint:\n",
    "        type: GRPC\n",
    "    name: model\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_seldon_grpc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=grpc-seldon \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep grpc-seldon -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send a set of GRPC requests using `grpcurl` and leveraging the Seldon protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!cd ../executor/proto && grpcurl -d '{\"data\":{\"ndarray\":[[1.0,2.0,5.0]]}}' \\\n",
    "         -rpc-header seldon:grpc-seldon -rpc-header namespace:seldon \\\n",
    "         -plaintext \\\n",
    "         -proto ./prediction.proto  0.0.0.0:8003 seldon.protos.Seldon/Predict\n",
    "d=json.loads(\"\".join(X))\n",
    "print(d)\n",
    "assert(d[\"data\"][\"ndarray\"][0][0] > 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_seldon_grpc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Protocol REST Model\n",
    "We will deploy a model with a REST endpoint that uses the TENSORLFOW Protocol namely by specifying the attribute `protocol: tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_tfserving_rest.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: rest-tfserving\n",
    "spec:\n",
    "  name: resttfserving\n",
    "  protocol: tensorflow\n",
    "  transport: rest\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - args: \n",
    "          - --port=8500\n",
    "          - --rest_api_port=8501\n",
    "          - --model_name=halfplustwo\n",
    "          - --model_base_path=gs://seldon-models/tfserving/half_plus_two\n",
    "          image: tensorflow/serving\n",
    "          name: halfplustwo\n",
    "          ports:\n",
    "          - containerPort: 8501\n",
    "            name: http\n",
    "    graph:\n",
    "      name: halfplustwo\n",
    "      type: MODEL\n",
    "      endpoint:\n",
    "        service_port: 8501\n",
    "    name: model\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_tfserving_rest.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=rest-tfserving \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep rest-tfserving -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send requests using the REST tensorflow protocol format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!curl -s -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n",
    "   -X POST http://localhost:8003/seldon/seldon/rest-tfserving/v1/models/halfplustwo/:predict \\\n",
    "   -H \"Content-Type: application/json\"\n",
    "d=json.loads(\"\".join(X))\n",
    "print(d)\n",
    "assert(d[\"predictions\"][0] == 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_tfserving_rest.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Protocol GRPC Model\n",
    "We will deploy a model with a GRPC endpoint that uses the TENSOFRLOW Protocol namely by specifying the attribute `protocol: tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_tfserving_grpc.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: grpc-tfserving\n",
    "spec:\n",
    "  name: grpctfserving\n",
    "  protocol: tensorflow\n",
    "  transport: grpc\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - args: \n",
    "          - --port=8500\n",
    "          - --rest_api_port=8501\n",
    "          - --model_name=halfplustwo\n",
    "          - --model_base_path=gs://seldon-models/tfserving/half_plus_two\n",
    "          image: tensorflow/serving\n",
    "          name: halfplustwo\n",
    "          ports:\n",
    "          - containerPort: 8500\n",
    "            name: grpc\n",
    "    graph:\n",
    "      name: halfplustwo\n",
    "      type: MODEL\n",
    "      endpoint:\n",
    "        service_port: 8500\n",
    "        type: GRPC\n",
    "    name: model\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_tfserving_grpc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=grpc-tfserving \\\n",
    "                                 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep grpc-tfserving -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send requests using the GRPC tensorflow protocol format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!cd ../executor/proto && grpcurl \\\n",
    "   -d '{\"model_spec\":{\"name\":\"halfplustwo\"},\"inputs\":{\"x\":{\"dtype\": 1, \"tensor_shape\": {\"dim\":[{\"size\": 3}]}, \"floatVal\" : [1.0, 2.0, 3.0]}}}' \\\n",
    "   -rpc-header seldon:grpc-tfserving -rpc-header namespace:seldon \\\n",
    "   -plaintext -proto ./prediction_service.proto \\\n",
    "   0.0.0.0:8003 tensorflow.serving.PredictionService/Predict\n",
    "d=json.loads(\"\".join(X))\n",
    "print(d)\n",
    "assert(d[\"outputs\"][\"x\"][\"floatVal\"][0] == 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_tfserving_grpc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 Protocol REST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy a REST model that uses the V2 Protocol namely by specifying the attribute `protocol: kfserving`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_v2_rest.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: triton\n",
    "spec:\n",
    "  protocol: kfserving\n",
    "  predictors:\n",
    "  - graph:\n",
    "      children: []\n",
    "      implementation: TRITON_SERVER\n",
    "      modelUri: gs://seldon-models/trtis/simple-model\n",
    "      name: simple\n",
    "    name: simple\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_v2_rest.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=triton -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep triton -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send requests using the Seldon protocol format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!curl -s -d '{\"inputs\":[{\"name\":\"INPUT0\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\"datatype\":\"INT32\",\"shape\":[1,16]}]}'  \\\n",
    "        -X POST http://0.0.0.0:8003/seldon/seldon/triton/v2/models/simple/infer \\\n",
    "        -H \"Content-Type: application/json\"\n",
    "d=json.loads(X[0])\n",
    "print(d)\n",
    "assert(d[\"outputs\"][0][\"data\"][0]==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_v2_rest.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 Protocol GRPC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy a GRPC model that uses the V2 Protocol namely by specifying the attribute `protocol: kfserving`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/model_v2_grpc.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: triton\n",
    "spec:\n",
    "  protocol: kfserving\n",
    "  transport: grpc\n",
    "  predictors:\n",
    "  - graph:\n",
    "      children: []\n",
    "      implementation: TRITON_SERVER\n",
    "      modelUri: gs://seldon-models/trtis/simple-model\n",
    "      name: simple\n",
    "    name: simple\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f resources/model_v2_grpc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=triton -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    state=!kubectl get sdep triton -o jsonpath='{.status.state}'\n",
    "    state=state[0]\n",
    "    print(state)\n",
    "    if state==\"Available\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "assert(state==\"Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send requests using the Seldon protocol format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=!cd ../executor/api/grpc/kfserving/inference && \\\n",
    "        grpcurl -d '{\"model_name\":\"simple\",\"inputs\":[{\"name\":\"INPUT0\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]},{\"name\":\"INPUT1\",\"contents\":{\"int_contents\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]},\"datatype\":\"INT32\",\"shape\":[1,16]}]}' \\\n",
    "        -plaintext -proto ./grpc_service.proto \\\n",
    "        -rpc-header seldon:triton -rpc-header namespace:seldon \\\n",
    "        0.0.0.0:8003 inference.GRPCInferenceService/ModelInfer\n",
    "X=\"\".join(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f resources/model_v2_grpc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
